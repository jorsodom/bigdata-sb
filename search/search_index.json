{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bienvenidos a Sistemas de Big Data","text":"<p>Los sistemas de Big Data se refieren al conjunto de herramientas, tecnolog\u00edas y m\u00e9todos que permiten la recopilaci\u00f3n, almacenamiento, an\u00e1lisis y visualizaci\u00f3n de grandes vol\u00famenes de datos generados. Estos sistemas recopilan y analizan datos de m\u00faltiples fuentes como plataformas de aprendizaje en l\u00ednea, sistemas de gesti\u00f3n, redes sociales, dispositivos m\u00f3viles y sensores, con el fin de obtener informaci\u00f3n relevante que apoye la toma de decisiones.</p>"},{"location":"#componentes-claves","title":"Componentes claves","text":"<ul> <li> <p>Recopilaci\u00f3n de datos: Los sistemas de Big Data capturan datos de diversas fuentes.</p> </li> <li> <p>Almacenamiento de datos: Utilizan infraestructuras como bases de datos distribuidas y almacenamiento en la nube para gestionar grandes vol\u00famenes de datos de forma segura y escalable.</p> </li> <li> <p>An\u00e1lisis y procesamiento de datos: Emplean algoritmos avanzados y t\u00e9cnicas de an\u00e1lisis de datos como miner\u00eda de datos, an\u00e1lisis predictivo y aprendizaje autom\u00e1tico para transformar datos en informaci\u00f3n \u00fatil.</p> </li> <li> <p>Visualizaci\u00f3n de datos: Herramientas que representan los datos de forma gr\u00e1fica para facilitar la interpretaci\u00f3n y toma de decisiones, ayudando a identificar patrones y tendencias.</p> </li> </ul>"},{"location":"#profesores","title":"Profesores","text":"<ul> <li> <p>Profesor titular: Alberto Aparicio</p> </li> <li> <p>Profesor especialista: Jorge Soro</p> </li> </ul>"},{"location":"#curso","title":"Curso","text":"<p>Curso de Especializaci\u00f3n en Inteligencia Artificial y Big Data (Acceso GS)</p> <p>CE IA Big Data</p>"},{"location":"#instituto","title":"Instituto","text":"<p>IES Dr. Llu\u00eds Simarro</p>"},{"location":"presentacions/","title":"Presentaci\u00f3n","text":""},{"location":"presentacions/#nos-conocemos","title":"\u00bfNos conocemos?","text":""},{"location":"presentacions/#por-que-has-decidido-hacer-esta-especializacion","title":"\u00bfPor qu\u00e9 has decidido hacer esta especializaci\u00f3n?","text":"<p>\"\u00bfQu\u00e9 te motiv\u00f3 principalmente a elegir esta especializaci\u00f3n?\"</p> <ul> <li> Inter\u00e9s personal por el tema.</li> <li> Mejora de oportunidades laborales.</li> <li> Recomendaci\u00f3n de otros.</li> <li> Requisito acad\u00e9mico.</li> </ul>"},{"location":"presentacions/#nuve-de-conceptos-inicial","title":"Nuve de conceptos inicial","text":"<p>Ejercicio mentimeter</p>"},{"location":"presentacions/#reflexion-y-discusion","title":"Reflexi\u00f3n y discusi\u00f3n","text":"<p>A trav\u00e9s de la siguiente infografia, reflexionar sobre la evoluci\u00f3n y hac\u00eda donde creeis que vamos.</p>"},{"location":"azure/AzureIntroduccion/","title":"Microsft Azure","text":""},{"location":"azure/AzureIntroduccion/#que-es-microsoft-azure","title":"\u00bfQu\u00e9 es Microsoft Azure?","text":"<p>Azure es una plataforma en la nube dise\u00f1ada para simplificar el proceso de creaci\u00f3n de aplicaciones modernas. Tanto si decide hospedar completamente las aplicaciones en Azure como ampliar las aplicaciones locales con los servicios de Azure, Azure le ayuda a crear aplicaciones escalables, confiables y f\u00e1ciles de mantener.</p> <ul> <li> <p>Esta compuesta por mas de 200 productos y servicios en la nube dise\u00f1ados para ayudar a crear nuevas aplicaciones o soluciones para resolver las dificultades actuales.</p> </li> <li> <p>Permite cerar, ejecutar y administrar aplicaciones en diferentes nubes, en entornos locales y hibridos con practicamente cualquier herramienta.</p> </li> </ul>"},{"location":"azure/AzureIntroduccion/#caracteristicas-principales","title":"Caracter\u00edsticas principales","text":"<ul> <li> <p>Confianza: Existe seguridad desde el principio, con el respaldo de un equipo de expertos y un cumplimiento normativo proactivo en el que conf\u00edan empresas, administraciones p\u00fablicas y startups.  </p> </li> <li> <p>Entorno h\u00edbrido: Se pueden realizar configuraciones tanto locales como en diferentes nubes y donde lo necesitemos. Integra y administra los entornos con servicios dise\u00f1ados para la nube h\u00edbrida.  </p> </li> <li> <p>Soluciones: Dentro de nuestro compromiso de contribuir al c\u00f3digo abierto y admitir todos los lenguajes y plataformas, ofrece la posibilidad de generar soluciones de la manera que se necesite e implementarlas donde queramos.  </p> </li> <li> <p>Futuro: La innovaci\u00f3n continua de Microsoft sostiene el trabajo de desarrollo que se realiza hoy y sus ideas para productos futuros.  </p> </li> </ul>"},{"location":"azure/AzureIntroduccion/#azure-products","title":"Azure Products","text":"<ul> <li>Directorio productos Azure</li> </ul>"},{"location":"azure/AzurePractica1/","title":"Conectar Raspberry Pi a Azure IoT","text":""},{"location":"azure/AzurePractica1/#objetivo","title":"Objetivo","text":"<p>Vamos a conectar dispositivos a la nube sin ning\u00fan problema con Azure Centro IoT utilizando un simulador de Raspberry Pi.  </p> <p>\u00bfQu\u00e9 vamos a hacer? </p> <ul> <li> Crear un Centro IoT.  </li> <li> Registrar un nuevo dispositivo en el Centro IoT para conectarlo con la Raspberry Pi.  </li> <li> Configurar el simulador Raspberry Pi.  </li> <li> Ejecutar una aplicaci\u00f3n de ejemplo en el simulador para enviar datos de un sensor al Centro IoT.  </li> <li> Recopilar datos del sensor al ejecutar el simulador IoT.  </li> <li> Conectar con una cuenta de almacenamiento (Storage Account), aplicaciones l\u00f3gicas y env\u00edo de correos.  </li> </ul> <p>La pr\u00e1ctica 1* se dividide en 3 partes**:</p> <ul> <li> <p>P1.1. Conectar Raspberry Pi a Azure Centro IoT </p> </li> <li> <p>P1.2. Conectar Raspberry Pi a Azure Centro IoT y Enrutamiento a una Cuenta de Almacenamiento (Storage Account) </p> </li> <li> <p>P1.3. Conectar Raspberry Pi a Azure Centro IoT y Enrutamiento a una Aplicaci\u00f3n L\u00f3gica y env\u00edo de correos </p> </li> </ul>"},{"location":"azure/AzurePractica1/#p11-conectar-raspberry-pi-a-azure-centro-iot","title":"P1.1. Conectar Raspberry Pi a Azure Centro IoT","text":""},{"location":"azure/AzurePractica1/#introduccion","title":"Introducci\u00f3n","text":"<ul> <li> Crearemos un \"Azure Centro IoT\".  </li> <li> Registraremos un dispositivo para Raspberry Pi en Azure Centro IoT.  </li> <li> Configuraremos el simulador Raspberry Pi.  </li> <li> Ejecutaremos una aplicaci\u00f3n de ejemplo en la Raspberry Pi para enviar datos del sensor al Centro IoT Hub.  </li> </ul>"},{"location":"azure/AzurePractica1/#crear-centro-iot","title":"Crear centro IoT","text":"<ul> <li>Creamos un nuevo recurso *Centro de IoT*.</li> </ul> <ul> <li>Como no tenemos un recurso creado, vamos a generarlo para asignar todo a este recurso y as\u00ed despu\u00e9s encontrarlo m\u00e1s f\u00e1cilmente.  </li> </ul> <ul> <li>Rellenamos los campos: Suscripci\u00f3n, Grupo de recursos, nombre de la instancia, regi\u00f3n y Nivel. Despu\u00e9s seleccionamos \u201cSiguiente: Redes\u201d.  </li> </ul> <ul> <li>Seleccionamos \u201cAcceso p\u00fablico\u201d.  </li> </ul> <ul> <li>Seleccionamos \u201cSiguiente: Administraci\u00f3n\u201d. Lo dejamos por defecto. No tocamos ning\u00fan par\u00e1metro.  </li> </ul> <ul> <li>Seleccionamos \u201cSiguiente: Etiquetas\u201d. No introducimos nada. O, si deseas etiquetar el recurso, introduce el Nombre / Valor para luego buscar el recurso a trav\u00e9s de la etiqueta.  </li> <li>Seleccionamos \u201cSiguiente: Revisar y crear\u201d. Revisamos que todo est\u00e9 correcto.  </li> <li>Llegados a este punto, ya podemos crear el recurso Centro de IoT. Hacemos clic en \u201cCreate/Crear\u201d. Tardar\u00e1 un tiempo en implementarse el recurso. Mientras tanto, en el espacio de notificaciones podemos observar el progreso hasta que finalice.  </li> </ul>"},{"location":"azure/AzurePractica1/#nuevo-dispositivo-iot","title":"Nuevo dispositivo IoT","text":"<ul> <li>En el men\u00fa de navegaci\u00f3n del Centro de IoT, seleccionamos \u201cDispositivos\u201d y luego seleccionamos \u201cAgregar dispositivo\u201d.  </li> </ul> <ul> <li>En \u201cCrear un dispositivo\u201d, proporcionamos un nombre para el nuevo dispositivo, como por ejemplo: \u201cRaspberryInput\u201d y no es necesario modificar ning\u00fan otro par\u00e1metro.Dejamos activada la opci\u00f3n \u201cGenerar claves autom\u00e1ticamente\u201d para las claves primarias y secundarias, que se generan de forma autom\u00e1tica. Seleccionamos \u201cGuardar\u201d. Esta acci\u00f3n crea una nueva identidad del dispositivo para el Centro de IoT.</li> </ul> <ul> <li>Despu\u00e9s de crear el dispositivo, lo abrimos desde la lista del panel de Dispositivos.  </li> </ul> <ul> <li>Copiamos la \u201cCadena de conexi\u00f3n principal o String\u201d (el c\u00f3digo del dispositivo utiliza esta cadena de conexi\u00f3n para comunicarse con el Centro IoT).  </li> </ul> <p>Nota</p> <ul> <li>De forma predeterminada, las claves y las cadenas de conexi\u00f3n est\u00e1n enmascaradas porque son informaci\u00f3n confidencial. Si se hace clic en el \u00edcono del ojo, se muestran. No es necesario mostrarlas para copiarlas.</li> </ul>"},{"location":"azure/AzurePractica1/#simulador","title":"Simulador","text":"<ul> <li>Enlace: Raspberry Pi Azure IoT Web Simulator</li> </ul> <p>Hay tres \u00e1reas en este simulador:  </p> <ul> <li> <p>Circuito: El circuito predeterminado consiste en una Raspberry Pi conectada a un sensor BME280 y un LED. No se puede modificar.  </p> </li> <li> <p>\u00c1rea de codificaci\u00f3n: Un editor de c\u00f3digo en l\u00ednea para programar con Raspberry Pi. La aplicaci\u00f3n de ejemplo predeterminada permite recopilar datos del sensor BME280 y enviarlos a Azure IoT Hub. La aplicaci\u00f3n es totalmente compatible con dispositivos reales. </p> </li> <li> <p>Ventana de consola integrada: Muestra la salida del c\u00f3digo. En la parte superior aparecen tres botones: </p> <pre><code>- **Run (Ejecutar)**: Ejecuta la aplicaci\u00f3n en el \u00e1rea de codificaci\u00f3n.  \n- **Reset (Restablecer)**: Restablece el \u00e1rea de codificaci\u00f3n a la aplicaci\u00f3n de ejemplo predeterminada.  \n- **Fold/Expand (Comprimir/Expandir)**: A la derecha hay un bot\u00f3n para plegar o expandir la ventana de la consola.\n</code></pre> </li> </ul>"},{"location":"azure/AzurePractica1/#ejecucion-en-simulador","title":"Ejecuci\u00f3n en simulador","text":"<ul> <li>Sobre el c\u00f3digo por defecto del simulador web, modificamos la linia 15 d\u00f3nde pone \"Your IoT hub device connection string\" para la cadena de conexi\u00f3n que hemos generado en Azure IoT Hub antes.</li> </ul> <ul> <li>Seleccionar \"Ejecutar\" o escribir \"npm start\" para ejecutar. Debemos de revisar el resultado del sensor y los mensajes generados.</li> </ul>"},{"location":"azure/AzurePractica1/#p12-conectar-raspberry-pi-a-azure-centro-iot-y-enrutamiento-a-una-cuenta-de-almacenamiento-storage-account","title":"P1.2. Conectar Raspberry Pi a Azure Centro IoT y Enrutamiento a una Cuenta de Almacenamiento (Storage Account)","text":""},{"location":"azure/AzurePractica1/#introduccion_1","title":"Introducci\u00f3n","text":"<ul> <li> Creamos una cuenta de almacenamiento \"Blob Storage Account\".</li> <li> Creamos un punto de conexi\u00f3n personalizado para que la cuenta de almacenamiento enrute los mensajes desde el Centro IoT.</li> <li> Revisamos los mensajes del dispositivo en el \"Blob Storage Account\".</li> </ul>"},{"location":"azure/AzurePractica1/#conexion-con-dispositivo","title":"Conexi\u00f3n con dispositivo","text":"<p>Acopiado este punto, vamos a recuperar los mensajes recibidos en el Azure Centre IoT a trav\u00e9s de un componente de Azure llamado \"enrutamiento de mensajes\" para almacenarlos en una Storage Account para poder explotarlos posteriormente.</p> <ol> <li>Vamos al Azure Centro IoT en el portal Azure.</li> <li>Seleccionamos \"Directivas de acceso compartido\" en la secci\u00f3n \"Configuraci\u00f3n de seguridad\".</li> <li>Seleccionamos la directiva \"iothubowner\".</li> <li>Copiamos el contenido de la \"cadena de conexi\u00f3n principal / string\"</li> <li>Ahora utilizaremos esta cadena de conexi\u00f3n para configurar el Explorador del Centro IoT. Hay que descargar e instalar el herramiento: Releases \u00b7 Azure/azure-iot-explorer (github.com)<pre><code>-       Abrimos el aplicativo \"**Azure IoT Explorer**\".\n-       Seleccionamos \"**Connect via IoT Hub connection string**\". Y a\u00f1adimos un nuevo dispositivo.\n-       Peguemos la cadena de conexi\u00f3n copiada anteriormente (iothubowner)\n-       Seleccionamos \"Guardar\".\n-       Ya conectados al Centro IoT, veremos un listado de los dispositivos. Seleccionamos lo creado.\n-       Seleccionamos \"Telemetr\u00eda\".\n-       Con el dispositivo en ejecuci\u00f3n, seleccionamos \"Start\". Si el dispositivo no se est\u00e1 ejecutando, no veremos la telemetr\u00eda. \n-       h.  Si todo esta funcionando, debemos visualizar los mensages que llegan al dispoisitvo, el mas reciente en la parte superior.\n-       Todos los mensajes re\u00fanen al punto de conexi\u00f3n integrado predeterminado del Centro IoT. A continuaci\u00f3n, vamos a crear un punto de conexi\u00f3n personalizado y enrutar algunos de los mensajes al conde de almacenamiento en funci\u00f3n de las propiedades del mensaje.\n</code></pre> </li> </ol> <p>Observaci\u00f3n</p> <pre><code>Observem els missatges entrants durant uns segons per comprovar que veiem diferents par\u00e0metres. Despr\u00e9s, podem parar el dispositiu.\n</code></pre> <p>Nota</p> <pre><code>Los mensajes dejar\u00e1n de aparecer en el Explorador IoT por que solo van al punto de conexi\u00f3n integrado cuando no coinciden con ninguna otra ruta en el Centro IoT\n</code></pre>"},{"location":"azure/AzurePractica1/#crear-cuenta-de-almacenamiento-storage-account","title":"Crear cuenta de almacenamiento (Storage Account)","text":"<p>Creamos un \"Storage Account\" y un contenedor dentro que tendr\u00e1 todos los mensajes del dispositivo que se asoman a la misma.</p> <ol> <li>Abrimos el Portal Azure y buscamos \"Cuentas de almacenamiento\"</li> <li>Seleccionamos \"Crear\".</li> <li>Proporcionamos los siguientes valores para la \"Cuentas de almacenamiento\": Suscripci\u00f3n, Grupos de recursos, N\u00famero del a cuenta de almacenamiento y Rendimiento.</li> <li>Seleccionamos \"Revisar y crear\".</li> <li>Una vez completada la validaci\u00f3n, seleccionamos \"Crear\".</li> <li>Una vez finalizada la implementaci\u00f3n, seleccionamos \"Ir al recurso\" o lo buscamos a trav\u00e9s del portal.</li> <li>En el men\u00fa de la \"cuenta de almacenamiento\", seleccionamos \"Contenedores en la secci\u00f3n Almacenamiento de datos\".</li> <li>Seleccionamos \"+ Contenedor\" para crear un nuevo contenedor.</li> <li>Proporcionaremos un nombre para el contenedor y seleccionamos \"Crear\".</li> </ol>"},{"location":"azure/AzurePractica1/#enrutamiento-a-una-cuenta-de-almacenamiento","title":"Enrutamiento a una cuenta de almacenamiento","text":"<p>Vamos a definir un nuevo punto de conexi\u00f3n que apunta a la \"cuenta de almacenamiento\" que me creado.  A continuaci\u00f3n, creamos una ruta sin ning\u00fan filtro a los mensajes y lo enrutamos en el punto de conexi\u00f3n de la \"cuenta de almacenamiento\"</p> <p>Nota</p> <p>Se pueden escribir datos en el Blob Storage con formato Apache Avro (predeterminado) o JSON. El formato de codificaci\u00f3n solo se puede establecer en el momento en el que se configura el punto de conexi\u00f3n del Blob Storage. No se puede cambiar el formato en un punto de conexi\u00f3n que ya est\u00e1 configurado. Cuando se utiliza codificaci\u00f3n JSON, se debe establecer contentType en JSON y contentEncoding en UTF-8 en las propiedades del sistema de mensajes.</p> <ol> <li>En Azure Portal, vamos hasta el Centro IoT.</li> <li>Seleccionamos \"Enrutamiento de mensajes\".</li> <li>En la pesta\u00f1a \"Rutas\", seleccionamos \"+ Agregar\".</li> <li>Seleccionamos en \"Agregar punto de conexi\u00f3n\" junto al campo \"Extremo\" y seleccionamos \"Almacenamiento\" en el menu desplegable.</li> <li>Proporcionamos la informaci\u00f3n para el nuevo punto de conexi\u00f3n: N\u00famero, Contenedor de Azure Storage y Encoding.</li> <li>Aceptamos los valores predeterminados para el resto de par\u00e1metros y seleccionamos \"Crear\".</li> <li>Seguimos con la creaci\u00f3n de la nueva ruta. Hemos agregado el punto de conexi\u00f3n al \"Storage Account\". Proporcionaremos la siguiente informaci\u00f3n para la nueva ruta:</li> <li>Guardamos Una vez creada la ruta en el Centro IoT y habilitada, iniciar\u00e1 inmediatamente el derrumbamiento de mensajes que cumpliendo la condici\u00f3n de la consulta del punto de conexi\u00f3n definido.</li> </ol>"},{"location":"azure/AzurePractica1/#supervision","title":"Supervisi\u00f3n","text":"<p>Volvamos a la sesi\u00f3n del explorador IoT. Recordemos que el explorador IoT supervisa el punto de conexi\u00f3n integrado para el centro IoT. Significa que ahora deber\u00eda ver solo mensajes que no se enrutan mediante la ruta personalizada creada. Volvemos a iniciar el simulador y la telemetr\u00eda. Observamos los mensajes entrantes durante unos instantes (si hay)</p>"},{"location":"azure/AzurePractica1/#visualizacion-de-mensages-en-la-cuenta-de-almacenamiento","title":"Visualizaci\u00f3n de mensages en la cuenta de almacenamiento","text":"<p>Comprobamos los mensajes est\u00e1n apretando al \"Storage account\":</p> <ol> <li>En Azure Portal, vamos a \"cuenta de almacenamiento\".</li> <li>Seleccionamos \"Contenedores\" en la secci\u00f3n de \"Almacenamiento\".</li> <li>Seleccionamos el contenedor.</li> <li>Debe haber una carpeta con el nombre del centro IoT. Exploramos en profundidad la estructura hasta que re\u00fanamos a ficheros avro.</li> <li>Seleccionamos el fichero avro y, a continuaci\u00f3n, lo descargamos. Y confirmamos que contiene mensajes del dispositivo establecido en el \"enrutamiento\".</li> <li>Paremos el simulador.</li> </ol>"},{"location":"azure/AzurePractica1/#ejercicio","title":"Ejercicio","text":"<p>En el \"enrutamiento\" creado en el \"Centro IoT\" hacia el Storage Account, se pueden definir condiciones que filtran los mensajes que ir\u00e1n destinados al Storage Account. Con la documentaci\u00f3n de Microsoft: Consulta sobre el enrutamiento de mensajes de Azure IoT Hub | Microsoft Learn</p> <p>Generar un filtro que destine solo mensajes filtrados por alguna de sus caracter\u00edsticas del JSON devuelto.</p>"},{"location":"azure/AzurePractica1/#p13-conectar-raspberry-pi-a-azure-centro-iot-y-enrutamiento-a-una-aplicacion-logica-y-envio-de-correos","title":"P1.3. Conectar Raspberry Pi a Azure Centro IoT y Enrutamiento a una Aplicaci\u00f3n L\u00f3gica y env\u00edo de correos","text":""},{"location":"hadoop/HadoopHDFS/","title":"HDFS","text":"<p>Claro, te voy a detallar HDFS (Hadoop Distributed File System) de forma t\u00e9cnica y tambi\u00e9n con un enfoque formativo para que te sirva para tus pr\u00e1cticas y para ampliar el entendimiento.</p>"},{"location":"hadoop/HadoopHDFS/#1-que-es-hdfs","title":"1. \u00bfQu\u00e9 es HDFS?","text":"<p>HDFS (Hadoop Distributed File System) es el sistema de archivos distribuido que forma el coraz\u00f3n del ecosistema Hadoop. Su objetivo es:</p> <ul> <li>Almacenar enormes cantidades de datos (terabytes/petabytes).</li> <li>Ser tolerante a fallos (los datos no se pierden aunque fallen partes del hardware).</li> <li>Escalar f\u00e1cilmente a\u00f1adiendo m\u00e1s nodos.</li> <li>Ser eficiente en el acceso secuencial de grandes ficheros.</li> </ul>"},{"location":"hadoop/HadoopHDFS/#2-arquitectura-de-hdfs","title":"2. Arquitectura de HDFS","text":""},{"location":"hadoop/HadoopHDFS/#componentes-clave","title":"Componentes Clave","text":"<ul> <li> <p>Namenode (Maestro)</p> </li> <li> <p>Guarda metadatos del sistema de archivos.</p> </li> <li>Ejemplo de metadatos: estructura de directorios, permisos, asignaci\u00f3n de bloques.</li> <li>No almacena datos reales, solo la \u201ctabla de contenidos\u201d de d\u00f3nde est\u00e1 todo.</li> <li> <p>Al arrancar, el Namenode carga:</p> <ul> <li>El \u00faltimo Fsimage.</li> <li>Aplica los cambios guardados en los Edits para reconstruir el estado actual.</li> <li>Datanodes (Esclavos)</li> </ul> </li> <li> <p>Son los nodos que almacenan los datos reales en bloques.</p> </li> <li>Cada bloque por defecto tiene 3 r\u00e9plicas distribuidas en nodos distintos para tolerancia a fallos.</li> <li>Reportan al Namenode su estado y la localizaci\u00f3n de los bloques.</li> </ul>"},{"location":"hadoop/HadoopHDFS/#datos-y-replicacion","title":"Datos y Replicaci\u00f3n","text":"<ul> <li>HDFS divide los ficheros grandes en bloques (por defecto 128 MB o 256 MB).</li> <li>Cada bloque se replica por defecto 3 veces (pero es configurable).</li> <li>Esto permite que si un nodo falla, los datos sigan disponibles en otras r\u00e9plicas.</li> </ul>"},{"location":"hadoop/HadoopHDFS/#3-funcionamiento-interno-metadatos-y-recuperacion","title":"3. Funcionamiento Interno (Metadatos y Recuperaci\u00f3n)","text":""},{"location":"hadoop/HadoopHDFS/#ficheros-clave-del-namenode","title":"Ficheros Clave del Namenode","text":"<ul> <li> <p>fsimage_000xxxx:</p> </li> <li> <p>Es una foto completa del sistema de archivos en un momento espec\u00edfico.</p> </li> <li> <p>Piensa en \u00e9l como un snapshot.</p> </li> <li> <p>edits_000xxxx:</p> </li> <li> <p>Registra todas las operaciones recientes (crear fichero, borrar, renombrar, etc.).</p> </li> <li> <p>Se va llenando con cada cambio que se realiza en HDFS.</p> </li> <li> <p>edits_inprogress_xxxx:</p> </li> <li> <p>Es el archivo de registro activo donde se est\u00e1n escribiendo los cambios en tiempo real.</p> </li> </ul>"},{"location":"hadoop/HadoopHDFS/#arranque-del-namenode","title":"Arranque del Namenode","text":"<p>Cuando ejecutas <code>start-dfs.sh</code>:</p> <ol> <li>El Namenode carga en memoria la \u00faltima copia estable de fsimage.</li> <li>Aplica todos los cambios no procesados que est\u00e1n en los ficheros edits.</li> <li>As\u00ed reconstruye el estado actual del sistema de ficheros.</li> </ol>"},{"location":"hadoop/HadoopHDFS/#secondary-namenode-no-es-un-respaldo","title":"Secondary Namenode (\u00a1No es un respaldo!)","text":"<ul> <li>Su tarea principal es consolidar los edits con el fsimage para evitar que los ficheros edits crezcan sin control.</li> <li> <p>Cada cierto tiempo (por defecto cada hora o cada 128MB de edits):</p> </li> <li> <p>Toma el fsimage y aplica los edits.</p> </li> <li>Genera un nuevo fsimage actualizado.</li> <li>Reinicia los edits.</li> </ul> <p>\u26a0\ufe0f Importante: El Secondary Namenode no es un respaldo del Namenode, solo ayuda a mantener limpios los metadatos.</p>"},{"location":"hadoop/HadoopHDFS/#4-modos-de-instalacion","title":"4. Modos de Instalaci\u00f3n","text":""},{"location":"hadoop/HadoopHDFS/#pseudodistribuido","title":"Pseudodistribuido","text":"<ul> <li>Todos los servicios (Namenode, Datanode) corren en una sola m\u00e1quina pero como procesos separados.</li> </ul>"},{"location":"hadoop/HadoopHDFS/#distribuido","title":"Distribuido","text":"<ul> <li>Servicios distribuidos en m\u00faltiples nodos reales.</li> <li>Para producci\u00f3n y grandes vol\u00famenes de datos.</li> </ul>"},{"location":"hadoop/HadoopHDFS/#5-practicas-recomendadas","title":"5. Pr\u00e1cticas recomendadas","text":"Pr\u00e1ctica Qu\u00e9 Aprender\u00e1s Pr\u00e1ctica 3 Instalaci\u00f3n y configuraci\u00f3n de HDFS en modo pseudodistribuido. Pr\u00e1ctica 4 C\u00f3mo se manejan internamente los ficheros fsimage y edits. Pr\u00e1ctica 5 Operaciones con ficheros en HDFS (<code>hdfs dfs -put</code>, <code>-get</code>, <code>-ls</code>). Pr\u00e1ctica 6 Administraci\u00f3n: balanceo, chequeo de bloques (<code>fsck</code>), gesti\u00f3n de espacio. Pr\u00e1ctica 7 Uso de snapshots para proteger y versionar datos en HDFS."},{"location":"hadoop/HadoopHDFS/#6-detalle-tecnico-como-evitar-la-corrupcion-de-metadatos","title":"6. Detalle t\u00e9cnico: C\u00f3mo evitar la corrupci\u00f3n de metadatos","text":"<ul> <li>Nunca toques manualmente los ficheros fsimage y edits.</li> <li>Usa siempre las herramientas de HDFS para operaciones administrativas.</li> <li>Mant\u00e9n monitoreado el espacio del Namenode (si se llena, HDFS puede fallar).</li> </ul>"},{"location":"hadoop/HadoopHDFS/#7-resumen-visual-simplificado","title":"7. Resumen Visual (Simplificado)","text":"<pre><code>[Usuario]\n   |\n   v\n[Namenode]  &lt;--- Metadatos (fsimage + edits)\n   |\n   v\n[Datanodes] &lt;--- Datos reales (bloques replicados)\n</code></pre>"},{"location":"hadoop/HadoopIntro/","title":"Hadoop","text":""},{"location":"hadoop/HadoopIntro/#introduccion","title":"Introducci\u00f3n","text":"<p>Hadoop es una de las tecnolog\u00edas m\u00e1s representativas del mundo del Big Data, hasta el punto de que, en muchos casos, ambos t\u00e9rminos se utilizan casi como sin\u00f3nimos. Se trata de un entorno distribuido que permite gestionar tanto grandes vol\u00famenes de datos como los procesos necesarios para analizarlos y extraer valor.</p> <p>Dise\u00f1ado siguiendo el modelo de los superordenadores de alto rendimiento (High Performance Super Computers), Hadoop destaca por su capacidad para escalar horizontalmente, es decir, puede ampliarse f\u00e1cilmente a\u00f1adiendo m\u00e1s m\u00e1quinas (nodos) que utilizan hardware relativamente econ\u00f3mico.</p>"},{"location":"hadoop/HadoopIntro/#caracteristicas-principales-de-hadoop","title":"Caracter\u00edsticas principales de Hadoop","text":"<ul> <li> <p>Capacidad: Almacena y procesa enormes cantidades de datos, de cualquier tipo (estructurados y no estructurados), de forma r\u00e1pida y eficiente.</p> </li> <li> <p>Potencia de procesamiento: Basado en computaci\u00f3n distribuida, aprovecha m\u00faltiples nodos para ejecutar tareas en paralelo.</p> </li> <li> <p>Tolerancia a fallos: Si un nodo falla, Hadoop redistribuye autom\u00e1ticamente las tareas a otros nodos disponibles, asegurando la continuidad del procesamiento.</p> </li> <li> <p>Flexibilidad: Permite elegir cu\u00e1ndo y c\u00f3mo se realiza el preprocesamiento de los datos.</p> </li> <li> <p>Bajo coste: Al ser un proyecto de c\u00f3digo abierto y funcionar sobre hardware est\u00e1ndar, su implantaci\u00f3n es accesible econ\u00f3micamente.</p> </li> <li> <p>Escalabilidad: Puede crecer desde unas pocas m\u00e1quinas hasta miles de nodos trabajando juntos de forma coordinada.</p> </li> </ul>"},{"location":"hadoop/HadoopIntro/#arquitectura-y-componentes","title":"Arquitectura y componentes","text":"<p>Hadoop implementa un modelo de procesamiento paralelo, ejecutando tareas distribuidas entre nodos mediante un sistema de archivos llamado Hadoop Distributed File System (HDFS).</p> <p>Sus principales componentes son:</p> <ul> <li> <p>Hadoop Common: Librer\u00edas y utilidades esenciales que permiten que los dem\u00e1s m\u00f3dulos funcionen.</p> </li> <li> <p>MapReduce: Modelo de programaci\u00f3n que permite procesar grandes conjuntos de datos dividi\u00e9ndolos en tareas menores que se ejecutan en paralelo.</p> </li> <li> <p>HDFS (Hadoop Distributed File System): Sistema de almacenamiento distribuido que guarda los datos en bloques replicados entre distintos nodos.</p> </li> </ul> <p>Este entorno est\u00e1 dise\u00f1ado para escalar desde un peque\u00f1o cl\u00faster de pocas m\u00e1quinas hasta miles de nodos, cada uno con su propia l\u00f3gica de procesamiento y capacidad de almacenamiento local.</p>"},{"location":"hadoop/HadoopIntro/#ecosistema-hadoop-otros-proyectos-relacionados","title":"Ecosistema Hadoop: Otros proyectos relacionados","text":"<p>Hadoop no es un sistema aislado, sino que forma parte de un amplio ecosistema de herramientas que complementan y ampl\u00edan sus capacidades. Entre ellas destacan:</p> <ul> <li> <p>HBase: Base de datos NoSQL orientada a columnas, que corre sobre HDFS y permite acceso en tiempo real a grandes cantidades de datos.</p> </li> <li> <p>Hive: Sistema que facilita la agregaci\u00f3n de datos y permite ejecutar consultas SQL sobre Hadoop usando MapReduce.</p> </li> <li> <p>Pig: Lenguaje de alto nivel que simplifica la escritura de flujos de datos complejos y su ejecuci\u00f3n sobre Hadoop.</p> </li> <li> <p>Mahout: Plataforma para el desarrollo de algoritmos de aprendizaje autom\u00e1tico (machine learning) sobre Hadoop.</p> </li> <li> <p>Zookeeper: Servicio centralizado que gestiona la configuraci\u00f3n y sincronizaci\u00f3n de servicios distribuidos.</p> </li> <li> <p>Sqoop: Herramienta dise\u00f1ada para transferir grandes vol\u00famenes de datos entre Hadoop y bases de datos relacionales tradicionales.</p> </li> </ul>"},{"location":"hadoop/HadoopIntro/#distribuciones","title":"Distribuciones","text":"<p>A lo largo de los a\u00f1os, el crecimiento del ecosistema Big Data ha llevado a la aparici\u00f3n de diversas empresas que ofrecen soluciones \u201cempaquetadas\u201d o distribuciones listas para usar basadas en Hadoop. Estas soluciones eliminan la complejidad de configurar manualmente todos los componentes y ofrecen plataformas robustas, seguras y optimizadas para uso empresarial.</p> <p>Estas distribuciones pueden desplegarse de varias formas:</p> <p>Mediante m\u00e1quinas virtuales preconfiguradas que podemos descargar y ejecutar localmente o en entornos corporativos.</p> <p>Utilizando servicios completamente gestionados en la nube, facilitando as\u00ed la escalabilidad y reduciendo la carga operativa.</p> <p>Entre los proveedores y plataformas m\u00e1s destacados se encuentran:</p> <ol> <li>Cloudera Uno de los l\u00edderes del mercado.</li> </ol> <p>Ofrece la Cloudera Data Platform (CDP), que integra Hadoop con herramientas modernas como Spark, Hive y machine learning.</p> <p>Disponible tanto para despliegues en la nube como en instalaciones locales.</p> <p>Incorpora seguridad avanzada, gesti\u00f3n de datos y anal\u00edtica en tiempo real.</p> <ol> <li>Hortonworks (ahora parte de Cloudera) Ofrec\u00eda la distribuci\u00f3n Hortonworks Data Platform (HDP), basada totalmente en software de c\u00f3digo abierto.</li> </ol> <p>En 2019, Hortonworks se fusion\u00f3 con Cloudera, combinando sus tecnolog\u00edas para ofrecer soluciones unificadas.</p> <ol> <li>IBM Open Platform IBM proporcionaba una distribuci\u00f3n basada en Hadoop y otros componentes del ecosistema, con fuerte integraci\u00f3n con su suite de anal\u00edtica e inteligencia artificial.</li> </ol> <p>Sus nuevas ofertas est\u00e1n centradas en IBM Cloud Pak for Data, que incorpora capacidades Big Data y AI sobre Kubernetes.</p> <p>Otras Plataformas y Soluciones Relevantes 4. MapR (Ahora HPE Ezmeral Data Fabric) Ofrec\u00eda una plataforma Big Data altamente optimizada que combinaba almacenamiento distribuido, procesamiento en tiempo real y gesti\u00f3n de flujos.</p> <p>Actualmente forma parte de la propuesta HPE Ezmeral, que permite trabajar con Big Data y AI en entornos h\u00edbridos.</p> <ol> <li>Amazon EMR (Elastic MapReduce) Servicio gestionado de Amazon Web Services (AWS) para ejecutar Hadoop, Spark, HBase y otros componentes.</li> </ol> <p>Altamente escalable y pago por uso, lo que permite procesar petabytes de datos sin necesidad de gestionar la infraestructura.</p> <ol> <li>Google Cloud Dataproc Soluci\u00f3n gestionada en Google Cloud que permite desplegar cl\u00fasteres Hadoop y Spark en cuesti\u00f3n de minutos.</li> </ol> <p>Integrado con otras herramientas como BigQuery, Google Cloud Storage y AI Platform.</p> <ol> <li>Azure HDInsight Plataforma en la nube de Microsoft basada en Hadoop que tambi\u00e9n soporta Spark, Hive, HBase, y m\u00e1s.</li> </ol> <p>Ofrece integraci\u00f3n con las herramientas de datos y anal\u00edtica del ecosistema Azure.</p>"},{"location":"hadoop/HadoopIntro/#tendencia-actual","title":"Tendencia actual","text":"<p>Aunque Hadoop sigue siendo un pilar importante, las empresas tambi\u00e9n est\u00e1n adoptando nuevas arquitecturas como:</p> <ul> <li> <p>Lago de datos (Data Lake) en la nube.</p> </li> <li> <p>Procesamiento distribuido en tiempo real con Apache Kafka, Apache Flink o Delta Lake.</p> </li> <li> <p>Herramientas de machine learning y anal\u00edtica avanzada sobre plataformas unificadas (por ejemplo, Databricks).</p> </li> </ul> <p>Elegir entre estas soluciones depender\u00e1 de las necesidades espec\u00edficas de cada organizaci\u00f3n:</p> <ul> <li> <p>On-premise vs. cloud.</p> </li> <li> <p>Procesamiento por lotes vs. en tiempo real.</p> </li> <li> <p>Presupuesto y requisitos de seguridad.</p> </li> </ul> <p>Las soluciones empaquetadas y en la nube han democratizado el acceso al Big Data, permitiendo que tanto grandes corporaciones como peque\u00f1as empresas puedan aprovechar su potencial sin grandes inversiones iniciales.</p>"},{"location":"hadoop/HadoopIntro/#practicas","title":"Pr\u00e1cticas","text":"<p>Para consolidar el aprendizaje, se propone realizar dos pr\u00e1cticas iniciales:</p> <ul> <li> <p>Pr\u00e1ctica 1: Configuraci\u00f3n b\u00e1sica del entorno Hadoop.</p> </li> <li> <p>Pr\u00e1ctica 2: Configuraci\u00f3n del sistema SSH, necesario para la comunicaci\u00f3n entre nodos del cl\u00faster Hadoop.</p> </li> </ul>"},{"location":"hadoop/HadoopMapReduce/","title":"MapReduce","text":""},{"location":"hadoop/HadoopMapReduce/#1-que-es-mapreduce","title":"1. \u00bfQu\u00e9 es MapReduce?","text":"<p>MapReduce es un modelo de programaci\u00f3n distribuido que permite procesar grandes cantidades de datos de forma paralela en un cl\u00faster Hadoop. Utiliza dos fases principales:</p> <ul> <li>Map \u2794 divide el trabajo en peque\u00f1as partes y las ejecuta en paralelo.</li> <li>Reduce \u2794 combina los resultados en una salida final.</li> </ul> <p>\u2714\ufe0f Se apoya en HDFS para leer los bloques y procesarlos cerca de donde residen, para minimizar el movimiento de datos.</p>"},{"location":"hadoop/HadoopMapReduce/#2-versiones-de-mapreduce","title":"2. Versiones de MapReduce","text":""},{"location":"hadoop/HadoopMapReduce/#mapreduce-v1-primera-version","title":"MapReduce V1 (Primera versi\u00f3n)","text":"<ul> <li>Monol\u00edtica: gestiona tanto la computaci\u00f3n como los recursos del cl\u00faster.</li> <li>Solo soporta trabajos tipo batch (procesamiento por lotes). Tiene problemas de:</li> <li>Escalabilidad \u2794 falla al superar \\~5000 nodos.</li> <li>Rendimiento \u2794 mal desempe\u00f1o en aplicaciones interactivas o en tiempo real.</li> <li>Gesti\u00f3n de recursos \u2794 centralizada y limitada.</li> </ul>"},{"location":"hadoop/HadoopMapReduce/#componentes-v1","title":"Componentes V1:","text":"<p>JobTracker (Maestro):   * Gestiona la planificaci\u00f3n y ejecuci\u00f3n de trabajos MapReduce.   * Asigna recursos y monitoriza tareas. TaskTracker (Esclavo):   * Ejecuta las tareas asignadas por el JobTracker.   * Informa sobre el estado de las tareas.</p>"},{"location":"hadoop/HadoopMapReduce/#mapreduce-v2-yarn","title":"MapReduce V2 (YARN)","text":"<p>Introduce YARN (Yet Another Resource Negotiator), que separa la gesti\u00f3n de recursos y el procesamiento. Admite otros modelos de procesamiento, no solo batch: streaming, SQL interactivo, machine learning, etc. Mucho m\u00e1s eficiente y escalable.</p>"},{"location":"hadoop/HadoopMapReduce/#componentes-v2-yarn","title":"Componentes V2 (YARN):","text":"<ul> <li> <p>ResourceManager (RM):</p> </li> <li> <p>Es el maestro de YARN.</p> </li> <li> <p>Gestiona los recursos globales del cl\u00faster.</p> </li> <li> <p>NodeManager (NM):</p> </li> <li> <p>Corre en cada nodo esclavo.</p> </li> <li> <p>Administra recursos y contenedores en el nodo espec\u00edfico.</p> </li> <li> <p>ApplicationMaster (AM):</p> </li> <li> <p>Uno por cada aplicaci\u00f3n o trabajo lanzado.</p> </li> <li>Gestiona la vida y planificaci\u00f3n del trabajo concreto.</li> <li>Pide recursos al ResourceManager y coordina los NodeManagers.</li> </ul>"},{"location":"hadoop/HadoopMapReduce/#ventajas-yarn","title":"Ventajas YARN:","text":"<ul> <li>Separa recursos y computaci\u00f3n \u2794 mayor eficiencia.</li> <li>Admite otros frameworks adem\u00e1s de MapReduce (como Spark, Tez, Flink).</li> <li>Escala mejor en clusters grandes.</li> </ul>"},{"location":"hadoop/HadoopMapReduce/#3-funcionamiento-detallado-de-mapreduce","title":"3. Funcionamiento detallado de MapReduce","text":""},{"location":"hadoop/HadoopMapReduce/#fases","title":"Fases","text":""},{"location":"hadoop/HadoopMapReduce/#1-map","title":"1. Map","text":"<ul> <li>Cada fichero se divide en bloques HDFS.</li> <li>Cada bloque es procesado por un Mapper.</li> <li>Mappers suelen ejecutarse en el nodo donde reside el bloque (data locality).</li> <li>Generan pares clave-valor intermedios.</li> </ul>"},{"location":"hadoop/HadoopMapReduce/#2-shuffle-and-sort","title":"2. Shuffle and Sort","text":"<ul> <li>Reorganiza y agrupa los datos por clave.</li> <li>Los datos intermedios son transferidos a los nodos donde se ejecutar\u00e1n los Reducers.</li> <li>Es un paso costoso en red, pero esencial para agrupar las claves.</li> </ul>"},{"location":"hadoop/HadoopMapReduce/#3-reduce","title":"3. Reduce","text":"<ul> <li>Cada Reducer procesa todas las claves asignadas.</li> <li>Combina, suma, cuenta o realiza la operaci\u00f3n deseada.</li> <li>Genera la salida final, que se guarda normalmente en HDFS.</li> </ul>"},{"location":"hadoop/HadoopMapReduce/#4-comparativa","title":"4. Comparativa","text":"MapReduce V1 YARN (V2) Solo Batch Batch + streaming + ML Mala escalabilidad (&gt;5000 nodos) Escala a miles de nodos sin problemas Gesti\u00f3n centralizada (JobTracker) Gesti\u00f3n distribuida (RM + AM) Solo MapReduce Admite Spark, Hive, Flink\u2026"},{"location":"hadoop/HadoopMapReduce/#5-practicas-recomendadas","title":"5. Pr\u00e1cticas recomendadas","text":"Pr\u00e1ctica Qu\u00e9 haremos Pr\u00e1ctica 8 Configuraci\u00f3n de YARN, asignaci\u00f3n de recursos, y c\u00f3mo lanzar trabajos MapReduce sobre YARN. Pr\u00e1ctica 9 MapReduce con WordCount Pr\u00e1ctica 10 MapReduce con \"Contar palabras.java\""},{"location":"hadoop/HadoopMapReduce/#6-por-que-yarn-mejora-mapreduce","title":"6. \u00bfPor qu\u00e9 YARN mejora MapReduce?","text":"<p>En V1, todo depend\u00eda del JobTracker, creando un cuello de botella.</p> <p>En V2 (YARN):   * El ResourceManager solo asigna recursos.   * Cada ApplicationMaster gestiona su propia aplicaci\u00f3n \u2794 paralelismo y escalabilidad.</p> <p>Adem\u00e1s, frameworks modernos como Apache Spark se ejecutan sobre YARN, aprovechando su gesti\u00f3n de recursos pero sin estar atados al viejo modelo MapReduce.</p>"},{"location":"hadoop/HadoopMapReduce/#7-resumen-visual","title":"7. Resumen visual","text":"<pre><code>MapReduce V1:\n [JobTracker] ---&gt; [TaskTrackers]\n  (Gesti\u00f3n + Computaci\u00f3n juntas)\n\nYARN (MapReduce V2):\n [ResourceManager] ---&gt; [NodeManagers]\n        |\n   [ApplicationMaster]\n        |\n     [Map &amp; Reduce Tasks]\n</code></pre>"},{"location":"hadoop/IntroBigData/","title":"Introducci\u00f3n Big Data","text":""},{"location":"hadoop/IntroBigData/#que-es-el-big-data","title":"\u00bfQu\u00e9 es el Big Data?","text":"<p>El Big Data se define como la convergencia y gesti\u00f3n de enormes vol\u00famenes de datos, tanto estructurados como no estructurados. Hablamos de una cantidad masiva de informaci\u00f3n que se genera y almacena continuamente en nuestra sociedad digital.</p> <p>Cada d\u00eda se crean petabytes (millones de gigabytes) de datos procedentes de m\u00faltiples fuentes, entre las que destacan:</p> <ul> <li> <p>Redes sociales: millones de publicaciones, comentarios, im\u00e1genes y v\u00eddeos generados por los usuarios.</p> </li> <li> <p>Dispositivos m\u00f3viles: registros de llamadas, geolocalizaci\u00f3n, uso de aplicaciones, entre otros.</p> </li> <li> <p>Sensores: dispositivos instalados en veh\u00edculos, f\u00e1bricas, infraestructuras y sistemas medioambientales que recopilan datos en tiempo real.</p> </li> <li> <p>Datos cient\u00edficos: investigaciones en \u00e1reas como la gen\u00f3mica, la f\u00edsica, y la medicina generan grandes vol\u00famenes de informaci\u00f3n.</p> </li> <li> <p>Ciudades inteligentes (Smart Cities): sistemas urbanos conectados que recogen y analizan datos sobre tr\u00e1fico, consumo energ\u00e9tico, calidad del aire, etc.</p> </li> <li> <p>Y muchas otras fuentes que contin\u00faan creciendo cada d\u00eda.</p> </li> </ul> <p>La finalidad del Big Data no es solo almacenar esta gran cantidad de datos, sino procesarlos y analizarlos para extraer informaci\u00f3n valiosa que facilite la toma de decisiones, optimice procesos y genere nuevas oportunidades de negocio.</p>"},{"location":"hadoop/IntroBigData/#las-tres-vs-del-big-data","title":"Las Tres V\u2019s del Big Data","text":"<p>Para entender la dimensi\u00f3n y los retos que plantea el Big Data, es esencial conocer sus tres caracter\u00edsticas fundamentales, conocidas como las tres V's:</p> <ul> <li> <p>Volumen: Se refiere a la inmensa cantidad de datos generados cada segundo.</p> </li> <li> <p>Velocidad: La rapidez con la que estos datos deben ser procesados para que su an\u00e1lisis sea relevante.</p> </li> <li> <p>Variedad: La diversidad de formatos y fuentes de los datos, que pueden ser estructurados (bases de datos) o no estructurados (v\u00eddeos, textos, im\u00e1genes).</p> </li> </ul>"},{"location":"hadoop/IntroBigData/#la-necesidad-de-nuevas-tecnologias","title":"La necesidad de nuevas tecnolog\u00edas","text":"<p>Las tecnolog\u00edas tradicionales de almacenamiento y procesamiento de datos no son capaces de manejar estas cantidades y caracter\u00edsticas de informaci\u00f3n. Por ello, se han desarrollado soluciones espec\u00edficas dentro del \u00e1mbito del Big Data, que permiten:</p> <ul> <li> <p>Gestionar grandes vol\u00famenes de datos distribuidos en m\u00faltiples servidores.</p> </li> <li> <p>Procesar la informaci\u00f3n en tiempo real.</p> </li> <li> <p>Analizar datos complejos y heterog\u00e9neos para generar valor.</p> </li> </ul> <p>Entre estas tecnolog\u00edas se encuentran sistemas como Hadoop, Spark, y herramientas de bases de datos NoSQL, que permiten trabajar eficientemente con el ecosistema del Big Data.</p>"},{"location":"hadoop/PreviaBigData/","title":"Consulta previa Big Data","text":""},{"location":"hadoop/PreviaBigData/#ejercicio","title":"Ejercicio","text":"<p>Para vosotros, \u00bfqu\u00e9 es el Big Data? \u00bfC\u00f3mo lo definir\u00edais en 3 palabras?</p> <p>Deber\u00e9is introducir, de forma an\u00f3nima, tres palabras que, seg\u00fan vuestra opini\u00f3n, definan el concepto de Big Data.</p> <p>M\u00e1s adelante, en clase, analizaremos y debatiremos juntos este nube de palabras que construiremos entre todos.</p> <p>El c\u00f3digo necesario para acceder lo facilitar\u00e1 el profesor durante la sesi\u00f3n.</p>"},{"location":"hadoop/PreviaBigData/#consideraciones","title":"Consideraciones","text":"<ul> <li> <p>Acceso: Mentimeter</p> </li> <li> <p>Responder: \u00bfQue entendeis por Big Data?</p> </li> <li> <p>Reflexi\u00f3n: Reflexionar las diferentes respuestas. </p> </li> </ul>"},{"location":"introduccionBI/BIIntroduccion/","title":"Introducci\u00f3n Bussines Intelligence (BI)","text":"<p>Para entender el mundo del BI, hay que llegar a conocer cu\u00e1les son los sistemas de bases de datos m\u00e1s importantes y sus caracter\u00edsticas. Por eso, vamos a revisar los sistemas operacionales, transaccionales y noSQL.</p>"},{"location":"introduccionBI/BIIntroduccion/#sistema-operacional-oltp","title":"Sistema operacional (OLTP)","text":"<p>Generalmente, la informaci\u00f3n que se quiere investigar sobre un cierto dominio de la organizaci\u00f3n se encuentra en bases de datos y otras fuentes muy diversas, tanto internas como externas.</p> <ul> <li> <p> Muchas de estas fuentes son las que se utilizan para el trabajo transaccional diario (conocido como OLTP, On-Line Transactional Processing).</p> </li> <li> <p> Sobre estas mismas bases de datos de trabajo ya se puede extraer conocimiento (visi\u00f3n tradicional).</p> </li> </ul> <p>Problemas para realizar el an\u00e1lisis en este sistema (OLTP):</p> <ul> <li> <p>La informaci\u00f3n se encuentra en varias y heterog\u00e9neas bases de datos y, en consecuencia, no se puede explotar en conjunto.</p> </li> <li> <p>Solo es posible el an\u00e1lisis de los datos actuales, no suele mantener valores hist\u00f3ricos para analizar hist\u00f3ricos y evolutivos.</p> </li> <li> <p>Se dificulta el trabajo transaccional diario de los sistemas de informaci\u00f3n originales, no permite el an\u00e1lisis on-line.</p> </li> <li> <p>La base de datos est\u00e1 dise\u00f1ada para el trabajo transaccional, no para el an\u00e1lisis de los datos.</p> </li> </ul>"},{"location":"introduccionBI/BIIntroduccion/#sistema-analitico-olap","title":"Sistema anal\u00edtico (OLAP)","text":"<p>Disponer de una base de datos que permita extraer conocimiento de la informaci\u00f3n hist\u00f3rica almacenada en la organizaci\u00f3n.</p> <p>Objetivos:</p> <ul> <li> <p> An\u00e1lisis de la organizaci\u00f3n</p> </li> <li> <p> An\u00e1lisis hist\u00f3ricos</p> </li> <li> <p> Dise\u00f1o de planes estrat\u00e9gicos</p> </li> </ul> <p>Algunas caracter\u00edsticas:</p> <ul> <li> <p>Almac\u00e9n de datos para el an\u00e1lisis y toma de decisiones.</p> </li> <li> <p>Permite realizar an\u00e1lisis on-line, por lo que aumenta el poder de toma de decisiones por parte de los responsables en cuesti\u00f3n</p> </li> <li> <p>Unifica informaci\u00f3n dispersa de diversos sistemas operacionales</p> </li> <li> <p>Contiene la informaci\u00f3n relevante de la organizaci\u00f3n</p> </li> <li> <p>Dise\u00f1os orientados a conceptos de negocio manejados por el usuario</p> </li> <li> <p>Sistema OLAP (On-Line Analitical Procesing)</p> </li> </ul>"},{"location":"introduccionBI/BIIntroduccion/#sistema-nosql","title":"Sistema NoSQL","text":"<p>Se refiere a un conjunto de sistemas de gesti\u00f3n de bases de datos que no siguen estrictamente el modelo relacional (tablas con filas y columnas). Fueron creadas para manejar grandes vol\u00famenes de datos, de forma r\u00e1pida, flexible y escalable, algo que a veces las bases de datos SQL tradicionales no logran con la misma eficiencia.</p> <p>Algunas caracter\u00edsticas:</p> <ul> <li> <p>No usan tablas fijas como en SQL (pueden ser documentos, grafos, pares clave-valor, etc.).</p> </li> <li> <p>Escalabilidad horizontal \u2192 se pueden repartir los datos en muchos servidores (cl\u00fasteres) f\u00e1cilmente.</p> </li> <li> <p>Alta velocidad de lectura y escritura, \u00fatil para apps en tiempo real (redes sociales, e-commerce, videojuegos online).</p> </li> <li> <p>Estructura flexible \u2192 permiten trabajar con datos semi-estructurados (JSON, XML) o sin esquema definido.</p> </li> <li> <p>Consistencia eventual \u2192 en muchos casos priorizan la disponibilidad y la velocidad sobre la consistencia estricta (a diferencia de SQL, que es muy r\u00edgido con transacciones).</p> </li> </ul>"},{"location":"introduccionBI/BIIntroduccion/#oltp-vs-olap-vs-nosql","title":"OLTP vs OLAP vs NoSQL","text":"Caracter\u00edstica Operacional (OLTP) Datawarehouse (OLAP) NoSQL Tipo de datos Almac\u00e9n de datos actuales Almac\u00e9n de datos hist\u00f3ricos Datos flexibles: documentos, clave-valor, grafos, columnas distribuidas Nivel de detalle Almacena datos al detalle Almacena datos al detalle y agregados a distintos niveles Puede almacenar datos al detalle o agregados, sin esquema fijo Tama\u00f1o de la base de datos Bases de datos medianas Bases de datos grandes Bases de datos muy grandes y distribuidas Naturaleza de los datos Los datos son actuales Los datos son est\u00e1ticos Los datos son semi-estructurados, din\u00e1micos o no estructurados Tipo de procesos Los procesos son repetitivos Los procesos no son previsibles Procesos distribuidos, consultas flexibles y adaptables Tiempo de respuesta Tiempo de respuesta peque\u00f1o Tiempo de respuesta variable Alta velocidad de lectura/escritura, escalable horizontalmente Tipo de decisiones que soporta Soporta decisiones diarias Soporta decisiones estrat\u00e9gicas Soporta decisiones operativas y en tiempo real (apps web, IoT, Big Data)"},{"location":"introduccionBI/BIIntroduccion/#componentes-de-un-sistema-bi","title":"Componentes de un sistema BI","text":""},{"location":"introduccionBI/BIIntroduccion/#sistema-etl","title":"Sistema ETL","text":"<p>En un sistema ETL, las fases esenciales son las siguientes:</p> <ul> <li> <p>Extracci\u00f3n: elaboraci\u00f3n de interfaces entre sistemas operacionales or\u00edgenes de datos</p> </li> <li> <p>Transformaci\u00f3n: validaci\u00f3n de los datos extra\u00eddos, transform\u00e1ndolos al formato deseado</p> </li> <li> <p>Carga: cargar f\u00edsicamente los datos extra\u00eddos de los sistemas operacionales y ya transformados en el modelo de datos del Datawarehouse</p> </li> </ul>"},{"location":"introduccionBI/BIIntroduccion/#presentacion","title":"Presentaci\u00f3n","text":"<p>En la capa de presentaci\u00f3n de los datos, puede tener diferentes enfoques.</p> <p>DSS (Decisi\u00f3n Support System): Informes din\u00e1micos, normalmente para representar un indicador o un grupo de indicadores relacionados funcionalmente en tablas de datos. Adem\u00e1s de estos informes predefinidos, los usuarios con capacidades de an\u00e1lisis podr\u00e1n crear sus propios informes y obtener respuestas en demandas de informaci\u00f3n puntuales.</p> <p>EIS (Executive Information System): Informes est\u00e1ticos, predefinidos, f\u00e1ciles de manejar y orientados a los directores. Son informes muy visuales (normalmente se utilizan exclusivamente gr\u00e1ficos) y con un nivel alto de agregaci\u00f3n de la informaci\u00f3n. Deben estar orientados a la gesti\u00f3n estrat\u00e9gica.</p> <p>BSC (Balance Scored Card): Informes est\u00e1ticos para la gesti\u00f3n estrat\u00e9gica de la empresa. Permite medir el rendimiento de \u00e1reas o personas mediante el an\u00e1lisis de diversos de un conjunto de indicadores con valores objetivo y pesos.</p>"},{"location":"introduccionBI/BIIntroduccion/#modelo-logico","title":"Modelo l\u00f3gico","text":"<p>Existe un modelo l\u00f3gico a definir que representa el an\u00e1lisis de los datos y que no tiene el por qu\u00e9 ser igual que el modelo f\u00edsico de la base de datos.</p> <p>Ejemplo</p> <p>Organizaci\u00f3n: Cadena de supermercados.</p> <p>Actividad objeto de an\u00e1lisis: ventas de productos. Informaci\u00f3n registrada sobre una venta: \u201cdel producto\u201cX\u201d se han vendido en el almac\u00e9n\u201cnro.1\u201d el d\u00eda 17/2/2008, 5 unidadespor un importede 100 euros.\u201d</p>"},{"location":"introduccionBI/BIIntroduccion/#modelo-multidimensional","title":"Modelo multidimensional","text":"<p>En un esquema multidimensional se representa una actividad que es objeto de an\u00e1lisis (hecho) y las dimensiones que caracterizan la actividad (dimensiones).</p> <ul> <li> <p> La informaci\u00f3n relevante sobre el hecho (actividad) se representa por un conjunto de indicadores.</p> </li> <li> <p> La informaci\u00f3n descriptiva de cada dimensi\u00f3n se representa por un conjunto de atributos (atributos de dimensi\u00f3n).</p> </li> </ul>"},{"location":"introduccionBI/BIIntroduccion/#business-intelligence-bi","title":"Business Intelligence (BI)","text":"<p>Business Intelligence suele definirse como la transformaci\u00f3n de los datos de la compa\u00f1\u00eda en conocimiento para obtener una ventaja competitiva. Desde un punto de vista m\u00e1s pragm\u00e1tico, y asoci\u00e1ndolo directamente a las tecnolog\u00edas de la informaci\u00f3n, podemos definir Business Intelligence como:</p> <p>El conjunto de metodolog\u00edas, aplicaciones y tecnolog\u00edas que permiten reunir, depurar y transformar datos de los sistemas transaccionales e informaci\u00f3n desestructurada (interna y externa a la compa\u00f1\u00eda) en informaci\u00f3n estructurada, para su explotaci\u00f3n directa (reporting, an\u00e1lisis OLAP...) o para su an\u00e1lisis y conversi\u00f3n en conocimiento soporte a la toma de decisiones sobre el negocio.</p>"},{"location":"introduccionBI/BIIntroduccion/#caracteristicas-de-un-en-entorno-big-data-frente-entornos-bi","title":"Caracter\u00edsticas de un en entorno Big Data frente entornos BI","text":"<p>Aunque el proceso ETL se puede realizar con distintas herramientas, en entornos de Big Data se requieren capacidades especiales para manejar la complejidad y volumen de datos:</p> <ul> <li>Flexibilidad en los formatos: debe poder procesar datos estructurados, semiestructurados y no estructurados.</li> <li>Escalabilidad y tolerancia a fallos: el sistema debe crecer seg\u00fan la demanda y continuar funcionando, aunque haya errores.</li> <li>Conectores para m\u00faltiples fuentes de datos: integraci\u00f3n con diferentes sistemas y tecnolog\u00edas.</li> </ul>"},{"location":"introduccionBI/BIIntroduccion/#terminologia","title":"Terminolog\u00eda","text":"Concepto Descripci\u00f3n Hecho Un hecho es un elemento de informaci\u00f3n del negocio, es decir, algo que se puede medir. Indicador F\u00f3rmula matem\u00e1tica aplicada a un conjunto de hechos. Dimensi\u00f3n / Eje de an\u00e1lisis Aspecto o perspectiva mediante la cual se puede acceder y analizar los hechos. Atributo Caracter\u00edstica espec\u00edfica de una dimensi\u00f3n. Jerarqu\u00eda Relaci\u00f3n padre/hijo utilizada para agrupar los atributos de una dimensi\u00f3n. Agregaci\u00f3n M\u00e9todo por el cual los datos son agrupados para crear una tabla de hechos espec\u00edfica. Sumarizaci\u00f3n M\u00e9todo por el cual los datos se cambian a un nivel diferente de granularidad. Desnormalizaci\u00f3n Proceso de introducir redundancias en la informaci\u00f3n de las tablas de la base de datos para mejorar el rendimiento de la aplicaci\u00f3n. Datamart Subconjunto departamental de un Datawarehouse corporativo o almac\u00e9n de datos enfocado en un \u00e1rea espec\u00edfica, utilizado para el an\u00e1lisis y la toma de decisiones. Drill Down / Up Navegar dentro de la informaci\u00f3n, desde niveles m\u00e1s altos a niveles m\u00e1s bajos o viceversa. DataMining B\u00fasqueda de patrones de comportamiento dentro de los datos, ya sea en un Datawarehouse o cualquier otra fuente de informaci\u00f3n."},{"location":"introduccionBI/BIIntroduccion/#herramientas","title":"Herramientas","text":""},{"location":"introduccionBI/BIIntroduccion/#herramientas-de-bi","title":"\ud83e\udde0 Herramientas de BI","text":"<p>Estas herramientas permiten analizar, visualizar y presentar datos para apoyar la toma de decisiones estrat\u00e9gicas y operativas.</p> <ul> <li>Tableau \u2013 Visualizaci\u00f3n de datos interactiva y potente, ideal para an\u00e1lisis avanzados.  </li> <li>Power BI \u2013 Plataforma de Microsoft para reportes y dashboards.  </li> <li>Qlik Sense / QlikView \u2013 BI asociativo para exploraci\u00f3n y an\u00e1lisis de datos.  </li> <li>Looker (Google) \u2013 BI en la nube, enfocado en an\u00e1lisis colaborativo.  </li> <li>MicroStrategy \u2013 Plataforma empresarial de an\u00e1lisis y visualizaci\u00f3n.  </li> <li>SAP BusinessObjects \u2013 BI empresarial con integraci\u00f3n en SAP.  </li> <li>IBM Cognos Analytics \u2013 BI con capacidades avanzadas de reportes y anal\u00edtica.  </li> <li>Metabase \u2013 BI open source, sencilla y colaborativa.  </li> <li>Redash \u2013 Open source para visualizaci\u00f3n y consultas SQL.  </li> <li>Elasticsearch (con Kibana) \u2013 Motor de b\u00fasqueda y anal\u00edtica en tiempo real, ideal para dashboards y monitoreo.</li> </ul>"},{"location":"introduccionBI/BIIntroduccion/#herramientas-de-etl-extract-transform-load","title":"\ud83d\udd04 Herramientas de ETL (Extract, Transform, Load)","text":"<p>Herramientas enfocadas en integrar datos desde diversas fuentes hacia Data Warehouses o Data Lakes, aplicando procesos de extracci\u00f3n, transformaci\u00f3n y carga.</p> <ul> <li>ODI (Oracle Data Integrator) \u2013 Soluci\u00f3n empresarial de Oracle para procesos ETL robustos y escalables.  </li> <li>Talend \u2013 Plataforma completa para integraci\u00f3n y calidad de datos (Open Source y Enterprise).  </li> <li>Informatica PowerCenter \u2013 Plataforma empresarial l\u00edder en integraci\u00f3n de datos.  </li> <li>Pentaho Data Integration (PDI) \u2013 Herramienta ETL open source muy popular.  </li> <li>Apache NiFi \u2013 ETL para flujos de datos en tiempo real y por lotes, con interfaz visual.  </li> <li>Microsoft SQL Server Integration Services (SSIS) \u2013 ETL integrado con el ecosistema Microsoft.  </li> <li>AWS Glue \u2013 ETL serverless administrado por Amazon Web Services.  </li> <li>Azure Data Factory \u2013 ETL en la nube dentro del ecosistema Azure.  </li> <li>Google Cloud Dataflow \u2013 ETL basado en Apache Beam para Google Cloud.  </li> <li>Apache Kafka (Streaming) \u2013 Procesamiento y transporte de datos en tiempo real.  </li> </ul>"},{"location":"introduccionBI/BIIntroduccion/#herramientas-de-busqueda-y-analitica-de-datos","title":"\ud83d\udd0d Herramientas de b\u00fasqueda y anal\u00edtica de datos","text":"<p>Estas herramientas se enfocan en indexaci\u00f3n, b\u00fasqueda y an\u00e1lisis avanzado de datos.</p> <ul> <li>Elasticsearch \u2013 Motor de b\u00fasqueda y anal\u00edtica distribuido, ideal para grandes vol\u00famenes de datos.  </li> <li>Kibana \u2013 Interfaz gr\u00e1fica para visualizaci\u00f3n y monitoreo de datos indexados en Elasticsearch.  </li> <li>Logstash \u2013 Herramienta para ingesta, transformaci\u00f3n y env\u00edo de datos hacia Elasticsearch.  </li> </ul>"},{"location":"introduccionBI/BIIntroduccion/#recomendaciones","title":"\ud83d\udccc Recomendaciones","text":"<ul> <li>BI empresarial: Tableau, Power BI, Qlik.  </li> <li>ETL empresarial: ODI, Informatica PowerCenter, Talend.  </li> <li>ETL Open Source: Apache NiFi, Pentaho, Airbyte.  </li> <li>Nube: AWS Glue, Azure Data Factory, Google Dataflow.  </li> <li>Anal\u00edtica y b\u00fasqueda: Elasticsearch + Kibana + Logstash (ELK Stack).  </li> </ul>"},{"location":"introduccionBI/ETLintroduccion/","title":"Introducci\u00f3n a ETL","text":""},{"location":"introduccionBI/ETLintroduccion/#ingesta-de-datos","title":"Ingesta de datos","text":"<p>La ingesta de datos es el proceso de obtener, guardar y procesar datos provenientes de diferentes fuentes. Este proceso se puede realizar en cualquier lenguaje de programaci\u00f3n y en distintos sistemas, pero cuando trabajamos en entornos de Big Data, comienzan a aparecer retos importantes relacionados con el volumen, la variedad y la velocidad de los datos.</p>"},{"location":"introduccionBI/ETLintroduccion/#obtencion-y-almacenamiento-de-datos","title":"Obtenci\u00f3n y almacenamiento de datos","text":"<p>La ingesta implica trabajar con datos que pueden provenir de diversas fuentes, estructuras y caracter\u00edsticas. Algunos ejemplos incluyen:</p> <ul> <li>Fuentes distintas: bases de datos, APIs, sensores IoT, archivos planos, etc.  </li> <li>Estructuras diferentes: datos estructurados, semi-estructurados y no estructurados.  </li> <li>Caracter\u00edsticas variadas: distintos formatos, velocidades de llegada, calidades y tama\u00f1os.</li> </ul> <p>Nota: No se puede improvisar cada vez que llega un nuevo dato. Es necesario dise\u00f1ar y construir una pipeline de ingesta, que permita un proceso automatizado y escalable.</p>"},{"location":"introduccionBI/ETLintroduccion/#pipeline-de-datos","title":"Pipeline de datos","text":"<p>Una pipeline de ingesta es un flujo estructurado que define los pasos y tecnolog\u00edas necesarios para procesar, mover y almacenar datos de forma eficiente.</p> <p>Caracter\u00edsticas clave: - Dividida en fases con objetivos claros. - Define una secuencia de tecnolog\u00edas para cada etapa. - Considera el rendimiento:   - Los datos no se analizan en el sistema donde se generan.   - Uso diferenciado de sistemas OLTP y OLAP:     - OLTP (Online Transaction Processing): orientado a la gesti\u00f3n y actualizaci\u00f3n r\u00e1pida de datos transaccionales.     - OLAP (Online Analytical Processing): orientado al an\u00e1lisis de grandes vol\u00famenes de datos para la toma de decisiones.</p>"},{"location":"introduccionBI/ETLintroduccion/#tipos-de-soluciones-y-consideraciones","title":"Tipos de soluciones y consideraciones","text":"<p>Existen m\u00faltiples herramientas y soluciones, tanto libres como comerciales, para implementar pipelines de ingesta. Al elegirlas, se deben tener en cuenta factores como:</p> <ul> <li>Procesamiento Batch vs Stream </li> <li>Batch: procesamiento en lotes, datos acumulados y procesados en intervalos.  </li> <li> <p>Stream: procesamiento en tiempo real o cercano al tiempo real.</p> </li> <li> <p>Escalabilidad </p> </li> <li> <p>Pensar en si se trabajar\u00e1 con pocos datos o con grandes vol\u00famenes (Big Data).</p> </li> <li> <p>Rendimiento </p> </li> <li> <p>Selecci\u00f3n de tecnolog\u00edas seg\u00fan la velocidad y eficiencia requerida.</p> </li> <li> <p>Interfaz y usabilidad </p> </li> <li> <p>Herramientas con interfaz gr\u00e1fica vs l\u00ednea de comandos.</p> </li> <li> <p>Licenciamiento </p> </li> <li> <p>Software libre vs software propietario.</p> </li> <li> <p>Integraci\u00f3n </p> </li> <li> <p>Uso de herramientas individuales o una suite integrada.</p> </li> <li> <p>Infraestructura </p> </li> <li>Procesamiento local vs en la nube.</li> </ul>"},{"location":"introduccionBI/ETLintroduccion/#resumen","title":"Resumen","text":"<p>La ingesta de datos es un paso cr\u00edtico en cualquier arquitectura de datos. Para manejar la complejidad de entornos de Big Data, se debe:</p> <ol> <li>Dise\u00f1ar una pipeline robusta y escalable.  </li> <li>Seleccionar las herramientas adecuadas considerando factores como el volumen, la velocidad y la variedad de los datos.  </li> <li>Distinguir entre procesamiento transaccional (OLTP) y anal\u00edtico (OLAP).  </li> <li>Evaluar opciones de infraestructura, licenciamiento y escalabilidad.</li> </ol>"},{"location":"introduccionBI/ETLintroduccion/#extraccion-transformacion-y-carga-etl","title":"Extracci\u00f3n, Transformaci\u00f3n y Carga (ETL)","text":"<p>La ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga) es un proceso fundamental en la ingenier\u00eda de datos y constituye un tipo espec\u00edfico de pipeline. Su objetivo es obtener datos de diversas fuentes, transformarlos para adaptarlos a las necesidades de an\u00e1lisis y finalmente cargarlos en un sistema de destino, como un Data Warehouse o un Data Lake.</p>"},{"location":"introduccionBI/ETLintroduccion/#origenes-de-datos","title":"Or\u00edgenes de datos","text":"<p>Las fuentes de datos pueden ser muy variadas, lo que aumenta la complejidad del proceso de extracci\u00f3n. Algunos ejemplos incluyen:</p> <ul> <li>Archivos Excel (XLS, XLSX)</li> <li>Documentos PDF</li> <li>Archivos de texto TXT</li> <li>Archivos CSV</li> <li>Bases de datos relacionales (SQL)</li> <li>Bases de datos no relacionales (NoSQL)</li> <li>Archivos JSON</li> <li>Mensajes en redes sociales, como Twitter</li> </ul>"},{"location":"introduccionBI/ETLintroduccion/#buenas-practicas-en-la-extraccion","title":"Buenas pr\u00e1cticas en la extracci\u00f3n","text":"<p>Durante la fase de extracci\u00f3n, es fundamental seguir ciertos principios para evitar problemas y asegurar la calidad de los datos:</p> <ul> <li>Verificar la calidad y el formato de los datos antes de procesarlos.</li> <li>La extracci\u00f3n debe ser r\u00e1pida y ligera, sin afectar el rendimiento del sistema origen.</li> <li>No poner en riesgo la fuente de datos, evitando bloqueos, ca\u00eddas o p\u00e9rdida de informaci\u00f3n.</li> </ul>"},{"location":"introduccionBI/ETLintroduccion/#fase-de-transformacion","title":"Fase de Transformaci\u00f3n","text":"<p>En esta etapa se procesan los datos para prepararlos para el an\u00e1lisis. Algunas tareas comunes incluyen:</p> <ul> <li>Cambios de codificaci\u00f3n para unificar formatos.</li> <li>Eliminaci\u00f3n de datos duplicados.</li> <li>Cruzar y relacionar diferentes fuentes de datos.</li> <li>Agregaci\u00f3n de informaci\u00f3n para obtener res\u00famenes y m\u00e9tricas.</li> <li>Filtrar y seleccionar partes de los datos relevantes.</li> <li>Creaci\u00f3n de c\u00f3digos, claves o identificadores \u00fanicos.</li> <li>Generaci\u00f3n de informaci\u00f3n nueva a partir de la existente.</li> <li>Estructuraci\u00f3n de los datos para cumplir con modelos anal\u00edticos.</li> <li>Generaci\u00f3n de indicadores clave (KPIs).</li> <li>Normalizaci\u00f3n para mantener consistencia y calidad.</li> </ul>"},{"location":"introduccionBI/ETLintroduccion/#fase-de-carga","title":"Fase de Carga","text":"<p>La etapa final de una ETL es la carga de los datos transformados en el sistema de destino, que puede ser:</p> <ul> <li>Un Data Warehouse para an\u00e1lisis estructurado.  </li> <li>Una base de datos para consumo operativo o aplicaciones.  </li> </ul>"},{"location":"introduccionBI/ETLintroduccion/#requisitos-clave-en-la-carga","title":"Requisitos clave en la carga:","text":"<ul> <li>M\u00ednimo tiempo de transacci\u00f3n, para evitar cuellos de botella.</li> <li>Cada sistema puede requerir un tipo de carga diferente:</li> <li>Bases de datos SQL.</li> <li>Archivos planos como CSV o TXT.</li> <li>Cargadores espec\u00edficos para herramientas propietarias.</li> </ul>"},{"location":"introduccionBI/ETLintroduccion/#aspectos-a-considerar","title":"Aspectos a considerar:","text":"<ul> <li>Gesti\u00f3n de \u00edndices para optimizar la consulta de datos.</li> <li>Gesti\u00f3n de claves de distribuci\u00f3n y particionamiento, especialmente en sistemas distribuidos.</li> <li>Tama\u00f1o de las transacciones y commits, asegurando la eficiencia y la integridad de los datos.</li> </ul>"},{"location":"introduccionBI/ETLintroduccion/#flujo-general-de-una-etl","title":"Flujo general de una ETL","text":"<ol> <li>Extracci\u00f3n (Extract) </li> <li>Captura de datos desde diferentes fuentes.  </li> <li> <p>Debe ser r\u00e1pida, eficiente y sin afectar el rendimiento del sistema original.</p> </li> <li> <p>Transformaci\u00f3n (Transform) </p> </li> <li>Limpieza, validaci\u00f3n y enriquecimiento de los datos.  </li> <li> <p>Incluye normalizaci\u00f3n, estandarizaci\u00f3n y generaci\u00f3n de nueva informaci\u00f3n \u00fatil para el an\u00e1lisis.</p> </li> <li> <p>Carga (Load) </p> </li> <li>Inserci\u00f3n de los datos procesados en el sistema de destino.  </li> <li>Optimizaci\u00f3n para consultas r\u00e1pidas y seguras.</li> </ol>"},{"location":"introduccionBI/ETLintroduccion/#resumen_1","title":"Resumen","text":"<p>El proceso ETL permite que las organizaciones dispongan de datos:</p> <ul> <li>Confiables y de calidad.  </li> <li>Procesados eficientemente, listos para an\u00e1lisis avanzado.  </li> <li>Disponibles sin comprometer el rendimiento ni la seguridad de los sistemas de origen.</li> </ul> <p>Una ETL bien dise\u00f1ada es la base para cualquier estrategia de anal\u00edtica de datos o Big Data, asegurando que la informaci\u00f3n llegue en el formato y momento adecuado para la toma de decisiones.</p>"},{"location":"introduccionBI/ETLintroduccion/#soluciones-comerciales-etl","title":"Soluciones comerciales ETL","text":"<p>Existen diversas herramientas comerciales especializadas en procesos ETL que facilitan la extracci\u00f3n, transformaci\u00f3n y carga de datos. Estas soluciones suelen ofrecer interfaces visuales, conectores listos para usar y funcionalidades avanzadas de integraci\u00f3n, escalabilidad y seguridad.</p>"},{"location":"introduccionBI/ETLintroduccion/#caracteristicas-comunes","title":"Caracter\u00edsticas comunes","text":"<p>Las herramientas comerciales suelen incluir:</p> <ul> <li>Interfaces gr\u00e1ficas intuitivas para dise\u00f1ar y gestionar pipelines.</li> <li>Gran variedad de conectores nativos para distintas fuentes y destinos de datos.</li> <li>Escalabilidad para manejar grandes vol\u00famenes de informaci\u00f3n en entornos Big Data.</li> <li>Soporte t\u00e9cnico y actualizaciones garantizadas por el proveedor.</li> <li>M\u00f3dulos avanzados para:</li> <li>Automatizaci\u00f3n de flujos.</li> <li>Monitorizaci\u00f3n en tiempo real.</li> <li>Gesti\u00f3n de errores y tolerancia a fallos.</li> <li>Seguridad y cumplimiento normativo, incluyendo cifrado y gesti\u00f3n de accesos.</li> </ul>"},{"location":"introduccionBI/ETLintroduccion/#ejemplos-no-open-source","title":"Ejemplos (no open source)","text":"<p>A continuaci\u00f3n, se presentan algunas de las herramientas m\u00e1s utilizadas en la industria:</p>"},{"location":"introduccionBI/ETLintroduccion/#1-informatica-powercenter","title":"1. Informatica PowerCenter","text":"<ul> <li>Plataforma l\u00edder en integraci\u00f3n de datos empresariales.</li> <li>Conectores para una amplia variedad de sistemas, bases de datos y servicios en la nube.</li> <li>Enfoque en escalabilidad, alta disponibilidad y seguridad.</li> <li>Adecuada para entornos corporativos con gran volumen de datos.</li> </ul>"},{"location":"introduccionBI/ETLintroduccion/#2-talend-data-fabric","title":"2. Talend Data Fabric","text":"<ul> <li>Soluci\u00f3n comercial basada en la tecnolog\u00eda de Talend, con versiones de pago y funcionalidades avanzadas.</li> <li>Amplia biblioteca de conectores para sistemas cloud y on-premise.</li> <li>Soporte para procesamiento batch y streaming.</li> <li>Integraci\u00f3n con herramientas de gobernanza y calidad de datos.</li> </ul>"},{"location":"introduccionBI/ETLintroduccion/#3-microsoft-sql-server-integration-services-ssis","title":"3. Microsoft SQL Server Integration Services (SSIS)","text":"<ul> <li>Herramienta incluida en Microsoft SQL Server.</li> <li>Ideal para empresas que ya trabajan en el ecosistema Microsoft.</li> <li>Ofrece:</li> <li>Dise\u00f1o visual de ETL.</li> <li>Transformaciones predefinidas.</li> <li>Integraci\u00f3n directa con Azure y servicios cloud de Microsoft.</li> </ul>"},{"location":"introduccionBI/ETLintroduccion/#4-oracle-data-integrator-odi","title":"4. Oracle Data Integrator (ODI)","text":"<ul> <li>Enfocado en entornos corporativos que utilizan soluciones Oracle.</li> <li>Compatible con m\u00faltiples bases de datos y tecnolog\u00edas.</li> <li>Permite procesamiento masivo con alto rendimiento.</li> <li>Integraci\u00f3n con otros productos de Oracle como Oracle Cloud Infrastructure.</li> </ul>"},{"location":"introduccionBI/ETLintroduccion/#5-ibm-datastage","title":"5. IBM DataStage","text":"<ul> <li>Parte de la suite IBM InfoSphere.</li> <li>Especializado en entornos empresariales complejos.</li> <li>Destacado por:</li> <li>Alta capacidad de paralelizaci\u00f3n.</li> <li>Gesti\u00f3n avanzada de metadatos.</li> <li>Compatibilidad con soluciones de Big Data y cloud.</li> </ul>"},{"location":"introduccionBI/ETLintroduccion/#6-aws-glue","title":"6. AWS Glue","text":"<ul> <li>Servicio ETL totalmente gestionado en la nube de Amazon Web Services (AWS).</li> <li>No requiere infraestructura local.</li> <li>Compatible con m\u00faltiples formatos de datos y servicios cloud.</li> <li>Ideal para Data Lakes y entornos serverless.</li> </ul>"},{"location":"introduccionBI/ETLintroduccion/#comparativa-general","title":"Comparativa general","text":"Herramienta Tipo Entorno principal Puntos fuertes Informatica PowerCenter Comercial On-premise / Cloud h\u00edbrido Estabilidad, escalabilidad, seguridad. Talend Data Fabric Comercial Cloud y local Flexibilidad, conectores, gobernanza. SSIS (Microsoft) Comercial Ecosistema Microsoft Integraci\u00f3n con SQL Server y Azure. Oracle Data Integrator Comercial Ecosistema Oracle Rendimiento y compatibilidad. IBM DataStage Comercial Empresas de gran escala Paralelizaci\u00f3n y Big Data. AWS Glue Comercial 100% Cloud (AWS) Sin servidores, escalabilidad nativa."},{"location":"introduccionBI/ETLintroduccion/#beneficios-de-usar-soluciones-comerciales","title":"Beneficios de usar soluciones comerciales","text":"<ul> <li>Ahorro de tiempo gracias a herramientas preconfiguradas.</li> <li>Reducci\u00f3n de errores mediante interfaces visuales y validaciones integradas.</li> <li>Soporte profesional con actualizaciones continuas.</li> <li>Integraci\u00f3n nativa con otros servicios empresariales y en la nube.</li> <li>Escalabilidad sin necesidad de dise\u00f1ar toda la infraestructura desde cero.</li> </ul>"},{"location":"introduccionBI/ETLintroduccion/#conclusion","title":"Conclusi\u00f3n","text":"<p>Las soluciones comerciales ETL son ideales para organizaciones que requieren: - Alta confiabilidad y disponibilidad. - Soporte oficial y actualizaciones regulares. - Gesti\u00f3n eficiente de grandes vol\u00famenes de datos. - Integraci\u00f3n con ecosistemas espec\u00edficos como AWS, Azure, Oracle o IBM.</p> <p>La elecci\u00f3n de la herramienta adecuada depender\u00e1 del presupuesto, la infraestructura existente y los objetivos estrat\u00e9gicos de la empresa.</p>"},{"location":"introduccionBI/analitica/","title":"Anal\u00edtica de datos","text":""},{"location":"introduccionBI/analitica/#que-es-el-data-analytics","title":"\u00bfQue es el Data Analytics?","text":"<p>Gesti\u00f3n de los datos para todos los usos (operacional y anal\u00edtico) y el an\u00e1lisis de datos para conducir los procesos comerciales y mejorar los resultados a trav\u00e9s de toma de decisiones m\u00e1s eficaz y basadas en la experiencia del cliente.</p>"},{"location":"introduccionBI/analitica/#roles-en-data-analytics","title":"Roles en Data Analytics","text":"<ul> <li>Data Analyst</li> </ul> <ul> <li>Data Engineer</li> </ul> <ul> <li>Data Scientist</li> </ul> <ul> <li>Data Architect</li> </ul> <ul> <li>Business Analyst</li> </ul>"},{"location":"introduccionBI/analitica/#que-es-el-analisis-de-datos","title":"\u00bfQue es el an\u00e1lisis de datos?","text":"<p>El proceso de identificar, limpiar, transformar y modelar los datos para detectar informaci\u00f3n significativa y \u00fatil. Despu\u00e9s, los datos se convierten en una historia a trav\u00e9s de informes para el an\u00e1lisis con el fin de admitir el proceso cr\u00edtico de toma de decisiones</p>"},{"location":"introduccionBI/analitica/#metodologias","title":"Metodolog\u00edas","text":"<ul> <li> <p>An\u00e1lisis Exploratorio de Datos (EDA) que pretende encontrar patrones y relaciones en los datos.</p> </li> <li> <p>An\u00e1lisis Confirmatorio de Datos (CDA) aplica t\u00e9cnicas estad\u00edsticas para determinar si las hip\u00f3tesis sobre un conjunto de datos son verdaderas o falsas</p> </li> </ul>"},{"location":"introduccionBI/analitica/#tipos","title":"Tipos","text":""},{"location":"introduccionBI/analitica/#analitica-descriptiva","title":"Anal\u00edtica descriptiva","text":"<p>El an\u00e1lisis descriptivo ayuda a responder preguntas sobre lo QU\u00c9 HA SUCEDIDO</p> <ul> <li> <p>Mediante el desarrollo de indicadores clave de rendimiento (KPI), se puede facilitar el seguimiento del \u00e9xito o el fracaso de los objetivos clave.</p> </li> <li> <p>Un ejemplo de an\u00e1lisis descriptivo es la generaci\u00f3n de informes para proporcionar una visi\u00f3n de los datos financieros y de ventas de una organizaci\u00f3n.</p> </li> </ul>"},{"location":"introduccionBI/analitica/#analitica-diagnostica","title":"Anal\u00edtica diagn\u00f3stica","text":"<ul> <li> <p>El an\u00e1lisis de diagn\u00f3stico ayuda a responder preguntas sobre POR QU\u00c9 SE HA PRODUCIDO un evento.</p> </li> <li> <p>Las t\u00e9cnicas de an\u00e1lisis de diagn\u00f3stico complementan el an\u00e1lisis descriptivo b\u00e1sico y usan los resultados del an\u00e1lisis descriptivo para identificar la causa de estos eventos.</p> </li> </ul>"},{"location":"introduccionBI/analitica/#analitica-predictiva","title":"Anal\u00edtica predictiva","text":"<p>El an\u00e1lisis predictivo ayuda a responder a preguntas sobre lo QU\u00c9 OCURRIR\u00c1 EN EL FUTURO.</p> <ul> <li> <p>Las t\u00e9cnicas de an\u00e1lisis predictivo usan datos hist\u00f3ricos para identificar tendencias y determinar la probabilidad de que se repitan.</p> </li> <li> <p>Engloban diversas t\u00e9cnicas estad\u00edsticas y de aprendizaje autom\u00e1tico, como las de redes neuronales, \u00e1rboles de decisi\u00f3n y regresi\u00f3n.</p> </li> </ul>"},{"location":"introduccionBI/analitica/#analitica-presctiptiva","title":"Anal\u00edtica presctiptiva","text":"<p>El an\u00e1lisis prescriptivo ayuda a responder preguntas sobre las acciones que se deben llevar a cabo para lograr un objetivo.</p> <ul> <li> <p>Esta t\u00e9cnica permite que, en caso de incertidumbre, las empresas tomen decisiones fundamentadas.</p> </li> <li> <p>Las t\u00e9cnicas de an\u00e1lisis prescriptivo dependen de estrategias de aprendizaje autom\u00e1tico para buscar patrones en conjuntos de datos de gran tama\u00f1o.</p> </li> <li> <p>Mediante el an\u00e1lisis de eventos y decisiones anteriores, las organizaciones pueden calcular la probabilidad de otros resultados.</p> </li> </ul>"},{"location":"introduccionBI/analitica/#proceso-de-analisis-de-datos","title":"Proceso de an\u00e1lisis de datos","text":"<ol> <li>Preparaci\u00f3n</li> <li>Modelado</li> <li>Visualizaci\u00f3n</li> <li>An\u00e1lisis</li> <li>Administraci\u00f3n</li> </ol>"},{"location":"introduccionBI/analitica/#1-preparacion","title":"1. Preparaci\u00f3n","text":"<p>La preparaci\u00f3n de los datos ocupa gran parte del trabajo.</p> <ul> <li> <p>Los datos imprecisos o incorrectos pueden tener un gran impacto y generar informes no v\u00e1lidos, una p\u00e9rdida de confianza y un efecto negativo en las decisiones empresariales</p> </li> <li> <p>La preparaci\u00f3n de datos es el proceso de generaci\u00f3n de perfiles,de limpieza y transformaci\u00f3n de los datos para prepararlos para el modelado y la visualizaci\u00f3n.</p> </li> <li> <p>La preparaci\u00f3n de datos consiste en:</p> </li> </ul> <p>\u25cb garantizar la integridad de los datos,</p> <p>\u25cb corregir datos incorrectos o inexactos,</p> <p>\u25cb identificar los datos que falten,</p> <p>\u25cb convertir datos de una estructura o hacer que los datos sean m\u00e1s legibles.</p> <ul> <li>Las garant\u00edas de privacidad y seguridad tambi\u00e9n son importantes. Estas pueden incluir la anonimizaci\u00f3n de los datos para evitar que se compartan en exceso o impedir que los usuarios vean informaci\u00f3n de identificaci\u00f3n personal cuando no es necesario.</li> </ul>"},{"location":"introduccionBI/analitica/#2-modelado","title":"2. Modelado","text":"<p>El modelado de datos es el proceso de determinar c\u00f3mo se relacionan las tablas entre s\u00ed.</p> <ul> <li> <p>Este proceso se realiza mediante la definici\u00f3n y creaci\u00f3n de relaciones entre las tablas.</p> </li> <li> <p>A partir de ah\u00ed, puede mejorar el modelo si define m\u00e9tricas y agrega c\u00e1lculos personalizados para enriquecer los datos.</p> </li> <li> <p>Un modelo de datos eficaz hace que los informes sean m\u00e1s precisos, permite que los datos se exploren de manera m\u00e1s r\u00e1pida y eficaz, reduce la duraci\u00f3n del proceso de creaci\u00f3n de informes y simplifica el mantenimiento futuro del informe.</p> </li> <li> <p>Un modelo mal dise\u00f1ado puede tener un impacto negativo en la precisi\u00f3n general y el rendimiento del informe.</p> </li> </ul>"},{"location":"introduccionBI/analitica/#3-visualizacion","title":"3. Visualizaci\u00f3n","text":"<p>En la tarea de visualizaci\u00f3n es donde se hace que los datos cobren vida. El objetivo final de la tarea de visualizaci\u00f3n es solucionar los problemas de la empresa. Un informe bien dise\u00f1ado debe contar una historia atractiva sobre esos datos, lo que permite a los responsables de la toma de decisiones empresariales obtener r\u00e1pidamente las conclusiones que necesitan. </p> <ul> <li> <p>Con las visualizaciones e interacciones adecuadas, puede proporcionar un informe eficiente que gu\u00ede al lector a trav\u00e9s del contenido de forma r\u00e1pida y eficaz, lo que le permitir\u00e1 seguir una narrativa en los datos.</p> </li> <li> <p>Los informes que se crean durante la tarea de visualizaci\u00f3n ayudan a las empresas y a los responsables de la toma de decisiones a comprender el significado de los datos para que se puedan tomar decisiones acertadas y precisas.</p> </li> <li> <p>Como analista de datos, se debe dedicar tiempo a comprender por completo el problema que la empresa intenta resolver.</p> </li> </ul>"},{"location":"introduccionBI/analitica/#4-analisis","title":"4. An\u00e1lisis","text":"<p>La tarea de an\u00e1lisis es el paso importante de entender e interpretar la informaci\u00f3n que se muestra en el informe.</p> <ul> <li>Con el an\u00e1lisis avanzado, las organizaciones pueden profundizar en los datos para predecir patrones y tendencias, identificar actividades, comportamientos, y permitir a las empresas formular las preguntas adecuadas sobre sus datos.</li> </ul>"},{"location":"introduccionBI/analitica/#5-administracion","title":"5. Administraci\u00f3n","text":"<ul> <li> <p>La administraci\u00f3n del contenido ayuda a fomentar la colaboraci\u00f3n entre equipos y usuarios. El uso compartido y la detecci\u00f3n de contenido es importante para que las personas adecuadas obtengan las respuestas que necesitan. Tambi\u00e9n es importante asegurarse de que los elementos sean seguros.</p> </li> <li> <p>La administraci\u00f3n adecuada tambi\u00e9n puede ayudar a reducir los silos de datos dentro de la organizaci\u00f3n. La duplicaci\u00f3n de los datos puede dificultar la administraci\u00f3n y la introducci\u00f3n de latencia de datos cuando los recursos se usan en exceso</p> </li> </ul>"},{"location":"introduccionBI/analitica/#herramientas-de-analisis-de-datos","title":"Herramientas de an\u00e1lisis de datos","text":"<p>Podemos ver seg\u00fan el prestigioso cuadrante de Gatner en 2025  las herramientas de anal\u00edtica de datos mas utilizadas.</p>"},{"location":"introduccionBI/presentacions/","title":"Presentaci\u00f3n","text":""},{"location":"introduccionBI/presentacions/#nos-conocemos","title":"\u00bfNos conocemos?","text":""},{"location":"introduccionBI/presentacions/#por-que-has-decidido-hacer-esta-especializacion","title":"\u00bfPor qu\u00e9 has decidido hacer esta especializaci\u00f3n?","text":"<p>\u00bfQu\u00e9 te motiv\u00f3 principalmente a elegir esta especializaci\u00f3n?</p> <ul> <li> Inter\u00e9s personal por el tema.</li> <li> Mejora de oportunidades laborales.</li> <li> Recomendaci\u00f3n de otros.</li> <li> Requisito acad\u00e9mico.</li> </ul>"},{"location":"introduccionBI/presentacions/#nube-de-conceptos-inicial","title":"Nube de conceptos inicial","text":"<p>https://www.menti.com/al7x52rbebkr </p>"},{"location":"introduccionBI/presentacions/#reflexion-y-discusion","title":"Reflexi\u00f3n y discusi\u00f3n","text":"<p>A trav\u00e9s de la siguiente infograf\u00eda, reflexionar sobre las finalidades del BI y hac\u00eda donde cre\u00e9is que vamos.</p>"},{"location":"introduccionBI/presentacions/#evaluacion-de-la-asignatura","title":"Evaluaci\u00f3n de la asignatura","text":"<p>La evaluaci\u00f3n se realizar\u00e1 a trav\u00e9s de las siguientes actividades:</p> Instrumento de evaluaci\u00f3n Descripci\u00f3n Momento de entrega Tipo de evaluaci\u00f3n Pr\u00e1cticas en la plataforma Aules Entrega de todas las pr\u00e1cticas realizadas durante el curso A lo largo del curso Evaluaci\u00f3n continua Examen intermedio Examen de todo el contenido visto hasta la mitad del curso Mitad de curso Evaluaci\u00f3n te\u00f3rica Proyecto final Desarrollo y entrega del proyecto final integrador de todos los contenidos Final de curso Evaluaci\u00f3n pr\u00e1ctica"},{"location":"introduccionBI/tiposdedatos/","title":"Tipos de Datos","text":"<p>En el \u00e1mbito de la ingenier\u00eda de datos y el Big Data, los datos se clasifican principalmente en tres tipos:  </p> <ul> <li> <p>estructurados, </p> </li> <li> <p>semiestructurados y</p> </li> <li> <p>no estructurados. </p> </li> </ul> <p>Esta clasificaci\u00f3n ayuda a definir la estrategia de almacenamiento, procesamiento y an\u00e1lisis que se debe aplicar en cada caso.</p>"},{"location":"introduccionBI/tiposdedatos/#datos-estructurados","title":"Datos estructurados","text":"<p>Los datos estructurados son aquellos que siguen un formato fijo y bien definido, normalmente almacenados en tablas con filas y columnas.  Cada elemento de informaci\u00f3n se ubica en un campo espec\u00edfico y tiene un tipo de dato predeterminado.</p>"},{"location":"introduccionBI/tiposdedatos/#caracteristicas","title":"Caracter\u00edsticas","text":"<ul> <li>Organizaci\u00f3n r\u00edgida y bien definida.</li> <li>F\u00e1cil acceso mediante consultas SQL u otros lenguajes estructurados.</li> <li>Alta calidad y fiabilidad, ya que suelen provenir de sistemas transaccionales.</li> <li>F\u00e1ciles de validar y procesar autom\u00e1ticamente.</li> </ul>"},{"location":"introduccionBI/tiposdedatos/#ejemplos","title":"Ejemplos","text":"<ul> <li>Bases de datos relacionales (MySQL, PostgreSQL, Oracle).</li> <li>Sistemas de ERP (gesti\u00f3n empresarial).</li> <li>Registros de ventas o facturas.</li> <li>Informaci\u00f3n de clientes con campos como Nombre, Direcci\u00f3n, Tel\u00e9fono.</li> </ul>"},{"location":"introduccionBI/tiposdedatos/#ventajas","title":"Ventajas","text":"<ul> <li>Eficiencia en almacenamiento gracias a su estructura fija.</li> <li>Alta velocidad de b\u00fasqueda y an\u00e1lisis mediante herramientas est\u00e1ndar.</li> <li>Mayor facilidad para aplicar reglas de validaci\u00f3n y control de calidad.</li> </ul>"},{"location":"introduccionBI/tiposdedatos/#desventajas","title":"Desventajas","text":"<ul> <li>Falta de flexibilidad ante cambios en el modelo de datos.</li> <li>No adecuado para informaci\u00f3n compleja o en formatos variables.</li> </ul>"},{"location":"introduccionBI/tiposdedatos/#datos-semiestructurados","title":"Datos semiestructurados","text":"<p>Los datos semiestructurados contienen informaci\u00f3n organizada, pero no siguen una estructura r\u00edgida como las bases de datos tradicionales. Aunque no se ajustan a un esquema fijo, incluyen etiquetas o delimitadores que facilitan su comprensi\u00f3n y an\u00e1lisis.</p>"},{"location":"introduccionBI/tiposdedatos/#caracteristicas_1","title":"Caracter\u00edsticas","text":"<ul> <li>Estructura flexible y adaptable a distintos tipos de datos.</li> <li>Uso de metadatos para describir la informaci\u00f3n.</li> <li>M\u00e1s complejos de procesar que los estructurados, pero m\u00e1s manejables que los no estructurados.</li> <li>Muy comunes en entornos web y aplicaciones modernas.</li> </ul>"},{"location":"introduccionBI/tiposdedatos/#ejemplos_1","title":"Ejemplos","text":"<ul> <li>JSON (JavaScript Object Notation).</li> <li>XML (Extensible Markup Language).</li> <li>Archivos de logs con campos din\u00e1micos.</li> <li>Datos provenientes de APIs.</li> <li>Informaci\u00f3n de sensores en IoT (Internet of Things).</li> </ul>"},{"location":"introduccionBI/tiposdedatos/#ventajas_1","title":"Ventajas","text":"<ul> <li>Mayor flexibilidad en la forma de representar la informaci\u00f3n.</li> <li>Facilidad de integraci\u00f3n entre sistemas con distintos formatos.</li> <li>Permiten evoluci\u00f3n gradual de los datos sin cambios dr\u00e1sticos en el modelo.</li> </ul>"},{"location":"introduccionBI/tiposdedatos/#desventajas_1","title":"Desventajas","text":"<ul> <li>Mayor complejidad de an\u00e1lisis respecto a datos estructurados.</li> <li>Necesidad de herramientas espec\u00edficas para la transformaci\u00f3n y normalizaci\u00f3n.</li> <li>Posibilidad de inconsistencias en los datos si no se gestionan correctamente.</li> </ul>"},{"location":"introduccionBI/tiposdedatos/#datos-no-estructurados","title":"Datos no estructurados","text":"<p>Los datos no estructurados no siguen ning\u00fan modelo predefinido ni contienen una organizaci\u00f3n clara. Representan la mayor parte de la informaci\u00f3n generada actualmente, especialmente en entornos digitales y redes sociales.</p>"},{"location":"introduccionBI/tiposdedatos/#caracteristicas_2","title":"Caracter\u00edsticas","text":"<ul> <li>Sin esquema fijo, cada archivo o documento puede tener un formato diferente.</li> <li>Gran variedad de formatos: texto libre, im\u00e1genes, audio, video, etc.</li> <li>Dificultad para su procesamiento y an\u00e1lisis autom\u00e1tico.</li> <li>Necesidad de t\u00e9cnicas avanzadas como Machine Learning o Procesamiento de Lenguaje Natural (NLP) para extraer valor.</li> </ul>"},{"location":"introduccionBI/tiposdedatos/#ejemplos_2","title":"Ejemplos","text":"<ul> <li>Im\u00e1genes y videos (fotograf\u00edas, grabaciones, c\u00e1maras de seguridad).</li> <li>Documentos de texto libre (Word, PDF, correos electr\u00f3nicos).</li> <li>Publicaciones en redes sociales (Twitter, Facebook, Instagram).</li> <li>Grabaciones de audio (llamadas, m\u00fasica, notas de voz).</li> <li>Datos de sensores en bruto.</li> </ul>"},{"location":"introduccionBI/tiposdedatos/#ventajas_2","title":"Ventajas","text":"<ul> <li>Representan la informaci\u00f3n m\u00e1s abundante y rica en contexto.</li> <li>Permiten capturar datos complejos que no caben en modelos estructurados.</li> <li>Fuente clave para t\u00e9cnicas avanzadas de anal\u00edtica predictiva e inteligencia artificial.</li> </ul>"},{"location":"introduccionBI/tiposdedatos/#desventajas_2","title":"Desventajas","text":"<ul> <li>Dif\u00edciles de almacenar y procesar con herramientas tradicionales.</li> <li>Necesitan gran capacidad de c\u00f3mputo y almacenamiento.</li> <li>Alto costo en tiempo y recursos para limpieza y preparaci\u00f3n.</li> </ul>"},{"location":"introduccionBI/tiposdedatos/#comparativa-general","title":"Comparativa general","text":"Caracter\u00edstica Estructurados Semiestructurados No estructurados Estructura fija S\u00ed Parcial No Facilidad de an\u00e1lisis Alta Media Baja Flexibilidad Baja Media Alta Herramientas comunes SQL, RDBMS JSON, XML, APIs Big Data, NLP, IA Ejemplo t\u00edpico Tabla de clientes Archivo JSON de logs Video de una c\u00e1mara Volumen en la actualidad Menor al 10% Aproximadamente 20% M\u00e1s del 70%"},{"location":"introduccionBI/tiposdedatos/#estrategias-de-almacenamiento-y-analisis","title":"Estrategias de almacenamiento y an\u00e1lisis","text":"<p>Cada tipo de dato requiere un enfoque espec\u00edfico en su gesti\u00f3n.</p> <ul> <li> <p>Estructurados:</p> <ul> <li>Bases de datos relacionales (SQL).</li> <li>Data Warehouses para an\u00e1lisis empresarial.</li> </ul> </li> <li> <p>Semiestructurados:</p> <ul> <li>Bases de datos NoSQL (MongoDB, Cassandra).</li> <li>Herramientas de integraci\u00f3n y transformaci\u00f3n (ETL/ELT).</li> </ul> </li> <li> <p>No estructurados:</p> <ul> <li>Sistemas de archivos distribuidos como Hadoop HDFS.</li> <li>Data Lakes para almacenamiento masivo.</li> <li>Algoritmos de IA y ML para an\u00e1lisis avanzado.</li> </ul> </li> </ul>"},{"location":"introduccionBI/tiposdedatos/#resumen","title":"Resumen","text":"<ul> <li> <p>Datos estructurados: ideales para informaci\u00f3n tabular y bien definida.  </p> </li> <li> <p>Datos semiestructurados: flexibles y adaptables, puente entre lo estructurado y lo no estructurado.  </p> </li> <li> <p>Datos no estructurados: mayor volumen y riqueza, pero requieren t\u00e9cnicas avanzadas para extraer valor.</p> </li> </ul> <p>En el mundo actual, la mayor\u00eda de los datos generados son no estructurados, por lo que las empresas deben prepararse para trabajar con ellos mediante herramientas de Big Data y tecnolog\u00edas de inteligencia artificial.</p>"},{"location":"multidimensional/entornos/","title":"Tipos de entornos y arquitecturas de datos","text":"<p>Los entornos se clasifican tanto por su ubicaci\u00f3n f\u00edsica (instalaci\u00f3n) como por las capas l\u00f3gicas que gestionan el flujo y la calidad de los datos.</p>"},{"location":"multidimensional/entornos/#entornos","title":"Entornos","text":"<p>Existen dos enfoques principales para la instalaci\u00f3n de los Sistemas de Big Data y BI:</p> Entorno Caracter\u00edsticas On-premise Implica una instalaci\u00f3n local. Requiere una infraestructura complicada, un alto coste y mantenimiento constante. Cloud (Nube) Implica una instalaci\u00f3n remota. Ofrece reducci\u00f3n de costes, mantenimiento automatizado y escalabilidad."},{"location":"multidimensional/entornos/#capas-logicas-y-zonas-de-la-arquitectura","title":"Capas l\u00f3gicas y zonas de la arquitectura","text":"<p>Existen diversas capas donde se procesan y almacenan los datos dentro de una arquitectura de DW:</p> <ul> <li>Staging Area: Es una zona de base de datos temporal donde se extrae la informaci\u00f3n. Aqu\u00ed no se suele hacer ninguna transformaci\u00f3n, y la informaci\u00f3n se trunca/inserta continuamente.</li> <li>ODS (Operational Data Store): Una zona de base de datos donde la informaci\u00f3n se prepara en un modelo multidimensional con tablas maestras y estrellas.</li> <li>Data Warehouse (DW): Representa la combinaci\u00f3n de tablas de diferentes capas, como Stage y ODS. Puede incluir capas agregadas con datos precalculados.</li> <li>Data Mart (DM): Es un conjunto de tablas multidimensionales que representan un subconjunto de datos de la empresa (por ejemplo, el departamento de recursos humanos o secretar\u00eda).</li> <li>AUDIT ZONE: Zona de base de datos utilizada para guardar los logs de los procesos y ejecuciones que se desean registrar para un control m\u00e1s robusto de la soluci\u00f3n.</li> </ul>"},{"location":"multidimensional/entornos/#modelo-analitico-multidimensional-en-la-nube-estandar","title":"Modelo anal\u00edtico multidimensional en la nube (Est\u00e1ndar)","text":"<p>Para los entornos Cloud, se identifican zonas de flujo de datos basadas en la calidad y el estado de la informaci\u00f3n, que suelen denominarse Bronce, Silver y Gold:</p> Zona Nombre y Prop\u00f3sito Bronce TEMP LANDING ZONE (05): Zona temporal que ubica las extracciones de informaci\u00f3n sin ninguna transformaci\u00f3n. Silver HISTORY ZONE (10): Almacena una copia hist\u00f3rica de cada extracci\u00f3n realizada, tanto en formato persistente como en base de datos. Silver CURRENT ZONE (15): Mantiene el \u00faltimo estado de los datos, ya limpios y con las transformaciones necesarias, en un modelo Datalake/Deltalake. Gold CONSUME ZONE (30): La zona final donde el modelo de base de datos (ej., SQL Server) est\u00e1 preparado para realizar el an\u00e1lisis posterior con herramientas de BI como PowerBI o Microstrategy."},{"location":"multidimensional/multidimensional/","title":"Modelo Multidimensional (OLAP)","text":""},{"location":"multidimensional/multidimensional/#conceptos-fundamentales","title":"Conceptos fundamentales","text":"<p>El modelo multidimensional es la base para el dise\u00f1o de Almacenes de Datos (Data Warehouses), cuyo objetivo es soportar el an\u00e1lisis de informaci\u00f3n para la toma de decisiones. A diferencia de los modelos transaccionales (OLTP), el modelo multidimensional busca optimizar la rapidez y simplicidad de las consultas.</p> <p>El dise\u00f1o de estos modelos se basa en transformar las preguntas del negocio en dos componentes esenciales: Hechos y Dimensiones.</p>"},{"location":"multidimensional/multidimensional/#componentes-clave","title":"Componentes clave","text":""},{"location":"multidimensional/multidimensional/#1-tabla-de-hechos-fact-table","title":"1. Tabla de Hechos (Fact Table)","text":"<p>La tabla de hechos es la tabla principal o central del modelo dimensional.</p> <ul> <li>Prop\u00f3sito: Registra los eventos del negocio que se desean analizar, como las compras o las ventas.</li> <li>M\u00e9tricas o Indicadores: Contiene los valores num\u00e9ricos, medibles y cuantitativos del evento que se van a analizar (por ejemplo, el financiamiento invertido o el total de pel\u00edculas producidas). Estos valores son generalmente agregables.</li> <li>Granularidad: Los hechos se registran a un alto nivel de atomicidad o m\u00ednima expresi\u00f3n, lo que se conoce como granularidad.</li> </ul>"},{"location":"multidimensional/multidimensional/#2-tablas-de-dimensiones-dimension-tables","title":"2. Tablas de Dimensiones (Dimension Tables)","text":"<p>Las dimensiones proporcionan el contexto o la perspectiva de an\u00e1lisis del hecho. Son atributos que describen los datos indicados en los hechos (metadatos).</p> <ul> <li>Contenido: Las dimensiones contienen la informaci\u00f3n descriptiva de los hechos. Aunque suelen tener menos registros que la tabla de hechos, cada registro puede tener un gran n\u00famero de atributos descriptivos.</li> <li>Ejemplos comunes: Los contextos t\u00edpicos incluyen el tiempo (fecha, mes, a\u00f1o), la geograf\u00eda (pa\u00eds, provincia, ciudad), los productos, o los proveedores.</li> <li>Jerarqu\u00edas: Las dimensiones a menudo contienen jerarqu\u00edas o relaciones internas. Por ejemplo, a partir de una dimensi\u00f3n base como la fecha de estreno, se pueden derivar el trimestre, el mes, el a\u00f1o y la d\u00e9cada.</li> </ul>"},{"location":"multidimensional/multidimensional/#esquemas-comunes","title":"Esquemas comunes","text":"<p>El modelo multidimensional se representa t\u00edpicamente mediante dos esquemas:</p>"},{"location":"multidimensional/multidimensional/#esquema-en-estrella-star-schema","title":"Esquema en Estrella (Star Schema)","text":"<p>El Esquema en Estrella es el modelo dimensional ideal por su simplicidad y velocidad para el an\u00e1lisis (OLAP).</p> <ul> <li>Estructura: Una tabla de hechos central est\u00e1 rodeada por tablas de dimensiones.</li> <li>Ventajas: Las consultas (JOINs) solo involucran a la tabla de hechos y a las de dimensiones, simplificando la consulta y mejorando el rendimiento. Se considera el esquema ideal para la visualizaci\u00f3n de datos.</li> </ul>"},{"location":"multidimensional/multidimensional/#esquema-en-copo-de-nieve-snowflake-schema","title":"Esquema en Copo de Nieve (Snowflake Schema)","text":"<p>Este esquema es m\u00e1s complejo y se presenta cuando las dimensiones est\u00e1n normalizadas (una dimensi\u00f3n tiene m\u00e1s de una tabla asociada).</p> <ul> <li>Desventaja: La tabla de hechos no est\u00e1 relacionada directamente a todas las tablas que componen el modelo de datos, lo que requiere uniones (JOINs) encadenadas y m\u00e1s complejas. Su objetivo principal es disminuir el almacenamiento.</li> </ul>"},{"location":"multidimensional/multidimensional/#recursos-adicionales","title":"Recursos adicionales","text":"<p>Bases de datos: Dise\u00f1o de un cubo OLAP (Ejemplo 1)</p> <p>Bases de datos: Dise\u00f1o de un cubo OLAP (Ejemplo 2)</p>"},{"location":"multidimensional/multidimensional/#ejercicio-diseno-del-modelo-multidimensional-de-compras","title":"EJERCICIO. Dise\u00f1o del modelo Multidimensional de Compras","text":"<p>Para construir el Modelo Multidimensional de Compras, se deben seguir los pasos de la metodolog\u00eda anal\u00edtica, identificando el hecho, los indicadores y las dimensiones necesarias para responder a las preguntas gerenciales.</p> <p>Dado el siguiente modelo de datos transaccional:</p>"},{"location":"multidimensional/multidimensional/#1-identifica-las-tablas-de-hechos","title":"1. Identifica las tablas de Hechos","text":"<p>El hecho principal objeto de an\u00e1lisis en este contexto es:</p> <ul> <li>Hecho (Fact): </li> </ul>"},{"location":"multidimensional/multidimensional/#2-identifica-posibles-hechos-y-sus-posibles-calculos","title":"2. Identifica posibles Hechos y sus posibles calculos","text":"<p>Los c\u00e1lculos (o m\u00e9tricas) son los valores num\u00e9ricos que la gerencia desear\u00eda analizar en funci\u00f3n del evento \"Compra\".</p> Criterio Ejemplos de Indicadores (Medidas) Cuantificaci\u00f3n Financiero Contable"},{"location":"multidimensional/multidimensional/#3-identifica-las-dimensionescatalogos","title":"3. Identifica las Dimensiones/Catalogos","text":"<p>Las dimensiones definen las perspectivas de an\u00e1lisis para desagregar los indicadores de Compras (el contexto de qui\u00e9n, qu\u00e9, cu\u00e1ndo y d\u00f3nde).</p> Dimensi\u00f3n Atributos descriptivos <p>Nota: Identificar jerarqu\u00edas dentro de estas dimensiones es clave para el dise\u00f1o.</p>"},{"location":"multidimensional/multidimensional/#4-construir-el-modelo-multidimensional","title":"4. Construir el modelo multidimensional","text":"<p>El dise\u00f1o recomendado es el Esquema en Estrella, debido a su simplicidad para las consultas anal\u00edticas.</p> <p>Os podeis ayudar de la herramienta online DrawIO</p>"},{"location":"multidimensional/multidimensional/#5-describe-textualmente-el-modelo-y-que-contiene","title":"5. Describe textualmente el modelo y que contiene","text":"<ul> <li>Contenido: </li> <li>An\u00e1lisis:</li> <li>Beneficio:</li> </ul>"},{"location":"multidimensional/multidimensional/#6-analizar-otras-posibilidades-de-disenos","title":"6. Analizar otras posibilidades de dise\u00f1os","text":"<p>Adem\u00e1s del dise\u00f1o base, se pueden considerar alternativas para mejorar la eficiencia del an\u00e1lisis:</p> <ol> <li>Data Marts (DM): </li> <li>Capas Agregadas: </li> </ol>"},{"location":"multidimensional/multidimensional/#entregable","title":"ENTREGABLE","text":"<p>Hay que facilitar un documento con todos los puntos del ejercicio y el dise\u00f1o en la plataforma Aules.</p>"},{"location":"multidimensional/multidimensional_solucion/","title":"Modelo Multidimensional (OLAP)","text":""},{"location":"multidimensional/multidimensional_solucion/#conceptos-fundamentales","title":"Conceptos fundamentales","text":"<p>El modelo multidimensional es la base para el dise\u00f1o de Almacenes de Datos (Data Warehouses), cuyo objetivo es soportar el an\u00e1lisis de informaci\u00f3n para la toma de decisiones. A diferencia de los modelos transaccionales (OLTP), el modelo multidimensional busca optimizar la rapidez y simplicidad de las consultas.</p> <p>El dise\u00f1o de estos modelos se basa en transformar las preguntas del negocio en dos componentes esenciales: Hechos y Dimensiones.</p>"},{"location":"multidimensional/multidimensional_solucion/#componentes-clave","title":"Componentes clave","text":""},{"location":"multidimensional/multidimensional_solucion/#1-tabla-de-hechos-fact-table","title":"1. Tabla de Hechos (Fact Table)","text":"<p>La tabla de hechos es la tabla principal o central del modelo dimensional.</p> <ul> <li>Prop\u00f3sito: Registra los eventos del negocio que se desean analizar, como las compras o las ventas.</li> <li>M\u00e9tricas o Indicadores: Contiene los valores num\u00e9ricos, medibles y cuantitativos del evento que se van a analizar (por ejemplo, el financiamiento invertido o el total de pel\u00edculas producidas). Estos valores son generalmente agregables.</li> <li>Granularidad: Los hechos se registran a un alto nivel de atomicidad o m\u00ednima expresi\u00f3n, lo que se conoce como granularidad.</li> </ul>"},{"location":"multidimensional/multidimensional_solucion/#2-tablas-de-dimensiones-dimension-tables","title":"2. Tablas de Dimensiones (Dimension Tables)","text":"<p>Las dimensiones proporcionan el contexto o la perspectiva de an\u00e1lisis del hecho. Son atributos que describen los datos indicados en los hechos (metadatos).</p> <ul> <li>Contenido: Las dimensiones contienen la informaci\u00f3n descriptiva de los hechos. Aunque suelen tener menos registros que la tabla de hechos, cada registro puede tener un gran n\u00famero de atributos descriptivos.</li> <li>Ejemplos comunes: Los contextos t\u00edpicos incluyen el tiempo (fecha, mes, a\u00f1o), la geograf\u00eda (pa\u00eds, provincia, ciudad), los productos, o los proveedores.</li> <li>Jerarqu\u00edas: Las dimensiones a menudo contienen jerarqu\u00edas o relaciones internas. Por ejemplo, a partir de una dimensi\u00f3n base como la fecha de estreno, se pueden derivar el trimestre, el mes, el a\u00f1o y la d\u00e9cada.</li> </ul>"},{"location":"multidimensional/multidimensional_solucion/#esquemas-comunes","title":"Esquemas comunes","text":"<p>El modelo multidimensional se representa t\u00edpicamente mediante dos esquemas:</p>"},{"location":"multidimensional/multidimensional_solucion/#esquema-en-estrella-star-schema","title":"Esquema en Estrella (Star Schema)","text":"<p>El Esquema en Estrella es el modelo dimensional ideal por su simplicidad y velocidad para el an\u00e1lisis (OLAP).</p> <ul> <li>Estructura: Una tabla de hechos central est\u00e1 rodeada por tablas de dimensiones.</li> <li>Ventajas: Las consultas (JOINs) solo involucran a la tabla de hechos y a las de dimensiones, simplificando la consulta y mejorando el rendimiento. Se considera el esquema ideal para la visualizaci\u00f3n de datos.</li> </ul>"},{"location":"multidimensional/multidimensional_solucion/#esquema-en-copo-de-nieve-snowflake-schema","title":"Esquema en Copo de Nieve (Snowflake Schema)","text":"<p>Este esquema es m\u00e1s complejo y se presenta cuando las dimensiones est\u00e1n normalizadas (una dimensi\u00f3n tiene m\u00e1s de una tabla asociada).</p> <ul> <li>Desventaja: La tabla de hechos no est\u00e1 relacionada directamente a todas las tablas que componen el modelo de datos, lo que requiere uniones (JOINs) encadenadas y m\u00e1s complejas. Su objetivo principal es disminuir el almacenamiento.</li> </ul>"},{"location":"multidimensional/multidimensional_solucion/#ejercicio-diseno-del-modelo-multidimensional-de-compras","title":"EJERCICIO. Dise\u00f1o del modelo Multidimensional de Compras","text":"<p>Para construir el Modelo Multidimensional de Compras, se deben seguir los pasos de la metodolog\u00eda anal\u00edtica, identificando el hecho, los indicadores y las dimensiones necesarias para responder a las preguntas gerenciales.</p> <p>Dado el siguiente modelo de datos transaccional:</p>"},{"location":"multidimensional/multidimensional_solucion/#1-identifica-las-tablas-de-hechos","title":"1. Identifica las tablas de Hechos","text":"<p>El hecho principal objeto de an\u00e1lisis en este contexto es:</p> <ul> <li>Hecho (Fact): Compras (registrando la adquisici\u00f3n de bienes o servicios).</li> </ul>"},{"location":"multidimensional/multidimensional_solucion/#2-identifica-posibles-hechos-y-sus-posibles-calculos","title":"2. Identifica posibles Hechos y sus posibles calculos","text":"<p>Los c\u00e1lculos (o m\u00e9tricas) son los valores num\u00e9ricos que la gerencia desear\u00eda analizar en funci\u00f3n del evento \"Compra\".</p> Criterio Ejemplos de Indicadores (Medidas) Cuantificaci\u00f3n N\u00famero de unidades compradas Financiero Costo total de la compra, Costo promedio Contable Impuestos aplicados, Descuentos obtenidos"},{"location":"multidimensional/multidimensional_solucion/#3-identifica-las-dimensionescatalogos","title":"3. Identifica las Dimensiones/Catalogos","text":"<p>Las dimensiones definen las perspectivas de an\u00e1lisis para desagregar los indicadores de Compras (el contexto de qui\u00e9n, qu\u00e9, cu\u00e1ndo y d\u00f3nde).</p> Dimensi\u00f3n Com\u00fan Atributos Descriptivos T\u00edpicos Tiempo/Calendario Fecha de compra, Mes, Trimestre, A\u00f1o (Dimensi\u00f3n temporal base). Producto/\u00cdtem Nombre del producto, Categor\u00eda, Marca, Tipo de \u00edtem. Proveedor Nombre del proveedor, Pa\u00eds de origen, Tipo de proveedor. Geogr\u00e1fica/Almac\u00e9n Ubicaci\u00f3n de la recepci\u00f3n (Almac\u00e9n, Pa\u00eds, Ciudad, Provincia). Empleado/Comprador Nombre del empleado, Departamento. <p>Nota: Identificar jerarqu\u00edas dentro de estas dimensiones es clave para el dise\u00f1o.</p>"},{"location":"multidimensional/multidimensional_solucion/#4-construir-el-modelo-multidimensional","title":"4. Construir el modelo multidimensional","text":"<p>El dise\u00f1o recomendado es el Esquema en Estrella, debido a su simplicidad para las consultas anal\u00edticas.</p> <p>Estructura:</p> <p>La tabla central de Hechos (Compras) contendr\u00e1 los indicadores (Costo Total, Cantidad Comprada) y las claves for\u00e1neas que se conectan a cada una de las dimensiones identificadas (Dimensi\u00f3n Tiempo, Dimensi\u00f3n Proveedor, Dimensi\u00f3n Producto, etc.).</p> <ul> <li> <p>Tabla de Hechos (Fact_Compras):</p> <ul> <li><code>ID_Hecho\\_Compra</code> (Clave Sustituta)</li> <li><code>ID\\_Tiempo</code> (FK a Dim_Tiempo)</li> <li><code>ID\\_Proveedor</code> (FK a Dim_Proveedor)</li> <li><code>ID\\_Producto</code> (FK a Dim_Producto)</li> <li><code>Costo\\_Total</code> (M\u00e9trica)</li> <li><code>Cantidad\\_Comprada</code> (M\u00e9trica)</li> </ul> </li> <li> <p>Tablas de Dimensiones (Dim_Proveedor, Dim_Producto, Dim_Tiempo, etc.): Contienen la informaci\u00f3n descriptiva.</p> </li> </ul> <p>soluci\u00f3n diagrama</p>"},{"location":"multidimensional/multidimensional_solucion/#5-describe-textualmente-el-modelo-y-que-contiene","title":"5. Describe textualmente el modelo y que contiene","text":"<p>El modelo multidimensional de Compras est\u00e1 dise\u00f1ado como un Esquema en Estrella (o Copo de Nieve, si se normalizan las dimensiones). El prop\u00f3sito de este modelo es analizar los hechos de compra (la actividad central del negocio).</p> <ul> <li>Contenido: El modelo se centra en el hecho Compra, midiendo indicadores como el costo total y la cantidad de unidades adquiridas.</li> <li>An\u00e1lisis: Estos indicadores pueden ser analizados seg\u00fan diferentes perspectivas, como cu\u00e1ndo se compr\u00f3 (Dimensi\u00f3n Tiempo), qu\u00e9 se compr\u00f3 (Dimensi\u00f3n Producto) y a qui\u00e9n se le compr\u00f3 (Dimensi\u00f3n Proveedor).</li> <li>Beneficio: Permite a los gerentes generar informes din\u00e1micos y responder a preguntas como: \"\u00bfCu\u00e1nto gastamos en la Categor\u00eda X de productos durante el \u00faltimo trimestre, y a qu\u00e9 proveedores?\".</li> </ul>"},{"location":"multidimensional/multidimensional_solucion/#6-analizar-otras-posibilidades-de-disenos","title":"6. Analizar otras posibilidades de dise\u00f1os","text":"<p>Adem\u00e1s del dise\u00f1o base, se pueden considerar alternativas para mejorar la eficiencia del an\u00e1lisis:</p> <ol> <li>Esquema en Copo de Nieve (Snowflake): Si se requiere reducir la redundancia en las dimensiones, se podr\u00eda normalizar una dimensi\u00f3n. Por ejemplo, la Dimensi\u00f3n Proveedor podr\u00eda separarse en una tabla de Proveedores y una tabla de Ubicaciones Geogr\u00e1ficas del Proveedor.</li> <li>Data Marts (DM): Si este modelo de Compras es solo una parte de un Data Warehouse m\u00e1s grande (que incluye Ventas, Inventario, etc.), podr\u00eda definirse como un Data Mart espec\u00edfico para el departamento de Compras.</li> <li>Capas Agregadas: Se pueden incluir tablas con datos precalculados (agregados). Por ejemplo, una tabla agregada podr\u00eda contener el \"Costo Total Mensual por Categor\u00eda de Producto\" para acelerar los informes de resumen.</li> </ol>"},{"location":"multidimensional/multidimensional_solucion/#entregable","title":"ENTREGABLE","text":"<p>Hay que facilitar un documento con todos los puntos del ejercicio y el dise\u00f1o. </p>"},{"location":"nifi/nifiCasoUsoAPI/","title":"nifiCasoUsoAPI","text":""},{"location":"nifi/nifiCasoUsoAPI/#consulta-de-una-api-con-nifi","title":"Consulta de una API con NiFi","text":"<p>En la pr\u00e1ctica que hicimos sobre la API del INE, consultamos la encuesta de poblaci\u00f3n activa. Vamos a ampliar con esta pr\u00e1ctica leyendo y automatizando la lectura de los datos con NiFi.</p>"},{"location":"nifi/nifiCasoUsoAPI/#consulta-de-la-api-y-guardar-el-json-en-un-archivo","title":"Consulta de la API y guardar el JSON en un archivo","text":"<p>Processor a usar: <code>InvokeHTTP</code> Objetivo: Consultar la API del INE para obtener los datos en formato JSON y luego guardarlos en un archivo local.</p> <p>Configurar <code>InvokeHTTP</code>:</p> <pre><code>- **URL:** `http://servicios.ine.es/wstempus/js/ES/DATOS_SERIE/&lt;cod serie&gt;?nult=&lt;num dades&gt;`\n\n- **M\u00e9todo HTTP:** `GET`\n\n- **Propiedades adicionales:**  Aseg\u00farate de que se ha configurado correctamente el manejo de errores (por ejemplo, reintentos y tiempo de espera).\n</code></pre> <p>Guardar el JSON en un archivo con <code>PutFile</code>:</p> <pre><code>- **Directorio de destino:** Elige la carpeta donde deseas almacenar los archivos JSON.\n\n- **Nombre del archivo:** Puede ser din\u00e1mico o fijo (por ejemplo, `datos.json`).\n</code></pre> <p>Este paso recoger\u00e1 la respuesta de la API y la almacenar\u00e1 como un archivo JSON en el sistema de archivos.</p>"},{"location":"nifi/nifiCasoUsoAPI/#aplanar-el-json-y-guardarlo-como-csv","title":"Aplanar el JSON y guardarlo como CSV","text":"<p>Processor a usar: <code>JoltTransformJSON</code> y <code>ConvertRecord</code></p> <p>Aplanar el JSON con <code>JoltTransformJSON</code>:     - Objetivo: El JSON puede estar anidado y necesitamos aplanarlo para poder convertirlo a CSV.     - Jolt Specification: Deber\u00e1s crear una especificaci\u00f3n Jolt que transforme el JSON en una estructura plana. Un ejemplo b\u00e1sico de Jolt podr\u00eda ser:      <pre><code>[\n  {\n    \"operation\": \"shift\",\n    \"spec\": {\n      \"data\": {\n        \"*\": {\n          \"value\": \"data[].value\",\n          \"date\": \"data[].date\"\n        }\n      }\n    }\n  }\n]\n</code></pre>   '''note \"Nota\"     Extrae los datos necesarios del JSON y los coloca en un formato m\u00e1s adecuado para la conversi\u00f3n.</p> <p>Convertir el JSON a CSV con <code>ConvertRecord</code>:     - Record Reader: <code>JsonTreeReader</code> (configurado para leer JSON).     - Record Writer: <code>CSVRecordSetWriter</code> (configurado para generar archivos CSV).     - Este paso toma los datos JSON aplanados y los convierte en un formato CSV.</p> <p>Guardar el archivo CSV con <code>PutFile</code>:     - Similar al paso anterior, puedes guardar el archivo CSV en el sistema de archivos local.</p>"},{"location":"nifi/nifiCasoUsoAPI/#guardar-los-datos-en-una-base-de-datos-postgresql","title":"Guardar los datos en una base de datos PostgreSQL","text":"<p>Processor a usar: <code>PutDatabaseRecord</code></p> <p>Configurar <code>PutDatabaseRecord</code>:     - JDBC Connection Pooling Service: Debes configurar un servicio de conexi\u00f3n JDBC, como <code>DBCPConnectionPool</code>, para conectarte a tu base de datos PostgreSQL.     - Configuraci\u00f3n de la base de datos PostgreSQL:         - JDBC URL: <code>jdbc:postgresql://&lt;host&gt;:&lt;port&gt;/&lt;database&gt;</code>         - Usuario: El usuario para conectarse a la base de datos.         - Contrase\u00f1a: La contrase\u00f1a correspondiente.</p> <p>Especificar el esquema de la base de datos:     - Schema: Define un esquema adecuado para insertar los datos del archivo CSV.     - Tabla: Selecciona o crea una tabla en PostgreSQL para almacenar los datos. La estructura de la tabla debe coincidir con los datos CSV que estamos insertando.</p> <p>Configurar <code>PutDatabaseRecord</code>:     - Record Reader: <code>CSVReader</code> (configurado para leer los datos del archivo CSV).     - Record Writer: No es necesario, ya que los datos se insertan directamente en la base de datos.</p> <p>Este paso leer\u00e1 los datos CSV y los insertar\u00e1 en la base de datos PostgreSQL.</p>"},{"location":"nifi/nifiCasoUsoAPI/#entregable","title":"Entregable","text":"<p>Guardar la plantilla:</p> <pre><code>- Despu\u00e9s de configurar todos los processors, selecciona todos los processors y haz clic en el bot\u00f3n derecho para elegir la opci\u00f3n \"Create Template\".\n- Nombra tu plantilla (por ejemplo, \"API_to_PostgreSQL_Template\").\n- Guarda la plantilla y podr\u00e1s reutilizarla en futuros proyectos.\n</code></pre>"},{"location":"nifi/nifiCasoUsoAPI/#resumen-de-los-processors-a-utilizar","title":"Resumen de los processors a utilizar","text":"<ul> <li><code>InvokeHTTP</code>: Para consultar la API del INE y obtener el JSON.</li> <li><code>JoltTransformJSON</code>: Para aplanar el JSON.</li> <li><code>ConvertRecord</code>: Para convertir el JSON a CSV.</li> <li><code>PutFile</code>: Para guardar los archivos JSON y CSV en el sistema de archivos.</li> <li><code>PutDatabaseRecord</code>: Para insertar los datos en PostgreSQL.</li> <li><code>DBCPConnectionPool</code>: Para configurar la conexi\u00f3n JDBC a PostgreSQL.</li> </ul>"},{"location":"nifi/nifiCasoUsoAPI/#anexos","title":"Anexos","text":"<p>INE API</p>"},{"location":"nifi/nifiCasoUsoHadoop/","title":"nifiCasoUsoHadoop","text":""},{"location":"nifi/nifiCasoUsoHadoop/#traspaso-de-archivos-fs-y-volcado-a-hdfs","title":"Traspaso de archivos FS y volcado a HDFS","text":"<p>Debemos transferir una gran cantidad de archivos del sistema de la m\u00e1quina virtual Linux al sistema de archivos de Hadoop HDFS, tambi\u00e9n localizado en la misma m\u00e1quina virtual. Para evitar generar un script con muchas instrucciones como <code>hdfs dfs -put ...</code>, lo cual podr\u00eda ser tedioso, implementaremos un proceso en NiFi que automatice esta tarea. Una vez que los archivos est\u00e9n en el sistema HDFS, ser\u00e1 necesario implementar un proceso para devolver estos archivos desde el sistema de archivos HDFS al sistema de archivos local. Este proceso permitir\u00e1 automatizar esta tarea y realizarla peri\u00f3dicamente.</p>"},{"location":"nifi/nifiCasoUsoHadoop/#consideraciones","title":"Consideraciones","text":"<ul> <li> NiFi debe ubicarse en el directorio <code>/opt</code>, al igual que Hadoop. Como usuario root, copiarlo. Eliminar el archivo zip de NiFi para liberar espacio en el disco duro de la m\u00e1quina virtual. <pre><code>[root@nodo1 opt]# cp -R /home/hadoop/Descargas/nifi-1.23.2 nifi\n</code></pre></li> <li> Asegurar que el directorio de NiFi (<code>/opt/nifi</code>) tenga permisos de ejecuci\u00f3n para que NiFi y Hadoop puedan interactuar.</li> </ul> <pre><code>[root@nodo1 opt]# chown -R hadoop:hadoop nifi\n[root@nodo1 opt]# chmod -R 777 nifi\n</code></pre> <ul> <li> <p> Si el directorio en HDFS al que enviamos los archivos no existe, el proceso debe crearlo.</p> </li> <li> <p> NiFi debe tener permisos sobre HDFS.</p> </li> <li> <p> Los directorios en HDFS deben tener permisos para que NiFi pueda escribir en ellos.</p> </li> </ul>"},{"location":"nifi/nifiCasoUsoHadoop/#pasos-a-seguir","title":"Pasos a seguir","text":"<ol> <li>Crear un directorio llamado <code>input</code> en el sistema de archivos de la m\u00e1quina virtual Linux, donde se ubicar\u00e1n los archivos que queremos trasladar al sistema HDFS.</li> <li>Implementar y configurar el procesador correspondiente para leer los archivos de este directorio del sistema de archivos de Linux. Este procesador recoger\u00e1 todos los archivos del sistema de archivos de la m\u00e1quina virtual en el directorio que hemos creado y los trasladar\u00e1 al sistema HDFS de Hadoop.</li> </ol> <p>En la propiedad de Hadoop Configuration Resources hay que poner los siguientes ficheros de configuraci\u00f3n de Hadoop: <pre><code>/opt/hadoop/etc/hadoop/hdfs-site.xml,/opt/hadoop/etc/hadoop/core-site.xml\n</code></pre> 3. Implementar y configurar el procesador correspondiente para realizar un <code>put</code> de los archivos al sistema HDFS de Hadoop en un directorio llamado <code>output_hdfs</code>. Si el directorio no existe, el proceso deber\u00e1 crearlo con los permisos del usuario que ejecuta (hadoop).      </p> <ol> <li> <p>Validar mediante el comando: <pre><code>hdfs dfs -ls /\n</code></pre> En el sistema de archivos HDFS que el directorio se ha creado con los permisos correctos y que los archivos han llegado correctamente. Tambi\u00e9n se puede validar utilizando la p\u00e1gina web de HDFS: http://nodo1:9870/explorer.html.</p> </li> <li> <p>A\u00f1adir un nuevo procesador a NiFi que recoja los archivos del sistema HDFS del directorio \"output_hdfs\" y los traslade a un directorio \"output\" en el sistema de archivos Linux.      </p> </li> </ol> <p>Ayuda: NiFi Documentation (apache.org)</p>"},{"location":"nifi/nifiCasoUsoHadoop/#entregable","title":"Entregable","text":"<p>Un PDF con capturas de pantalla de todo lo implementado y de los resultados en los directorios, tanto en HDFS como en el sistema local.</p>"},{"location":"nifi/nifiCasoUsoMeteo/","title":"Caso de uso metereol\u00f3gico","text":""},{"location":"nifi/nifiCasoUsoMeteo/#enunciado","title":"Enunciado","text":"<p>La web el-tiempo.net tiene un API desde el la podemos consultar informaci\u00f3n meteorol\u00f3gica, tanto datos en tiempo real como previsiones.</p> <p>Por ejemplo, para acceder a la predicci\u00f3n meteorol\u00f3gica de X\u00e0tiva lo accedemos a trav\u00e9s de la URL: </p> <ul> <li>El tiempo</li> </ul> <p>En nuestro caso, nos piden que guardemos un hist\u00f3rico de varios datos en intervalos de X minutos (para que sea m\u00e1s vivo lo haremos de 1 minuto). </p> <p>Los datos que nos piden guardar son los siguientes:</p> <ul> <li> <p>Temperatura</p> </li> <li> <p>Humedad</p> </li> <li> <p>Precipitaci\u00f3n</p> </li> <li> <p>Viento</p> </li> </ul> <p>En concreto, nos piden que en cada minuto almacenemos valor m\u00ednimo y m\u00e1ximo de cada uno de estos valores salvo sobre precipitaciones que guardemos las precipitaciones del intervalo de tiempo y el total del d\u00eda (\u00faltimo valor recibido). Esto lo realizaremos recogiendo datos cada X tiempo, que en nuestro caso puede ser cada 5 segundos, de forma que al final nos encontramos en que vamos recibiendo datos y agrupamos varios de ellos para obtener datos a partir de operaciones como media, m\u00e1ximo y m\u00ednimo.</p> <p>As\u00ed pues, cada 5 segundos accederemos a la API, descargaremos los datos, y cada minuto (12 descargas) calcularemos valores medios, m\u00ednimos y m\u00e1ximos y los guardaremos en una tabla en PostgresSQL. </p> <p>Lo que pretendemos demostrar es que con Apache Nifi, podemos realizar c\u00e1lculos a partir de un flujo de datos constantes, para obtener un provecho concreto de alguno de los datos obtenidos.</p>"},{"location":"nifi/nifiCasoUsoMeteo/#ejercicio","title":"Ejercicio","text":"<p>Los pasos que vamos a seguir deber\u00edan ser mas o menos los siguientes</p> <ol> <li>Obtenemos los datos del API</li> <li>Nos quedamos con los 4 datos que nos interesa.</li> <li>Agrupamos estos datos las X veces preestablecidas</li> <li>Realizamos los c\u00e1lculos de los nuevos campos derivados planteados</li> <li>Almacenamos datos en la base de datos</li> </ol> <p>Para todo este proceso los Processors que vamos a necesitar son: </p> <ul> <li>InvokeHTTP: Este procesador realiza la petici\u00f3n HTTP a la URL de la API especificando el m\u00e9todo necesario para obtener los datos.</li> <li>JoltTransformJSON: Este procesador se utiliza para transformar un archivo JSON utilizando una especificaci\u00f3n Jolt.  Nos quedamos solo con los 4 campos indicados del JSONobtenido.</li> <li>MergeContent: Nos permite agrupar una cantidad determinada de elementos en uno solo, o sea X JSON en uno \u00fanico.</li> <li>ExecuteScript: Permite ejecuci\u00f3n de Scripts. Vamos a utilizar un Script para realizar los c\u00e1lculos.</li> <li>PutDatabaseRecord: Permite leer el JSON con los datos calculados e insertarlo directamente en PostgreSQL</li> </ul> <p>Por \u00faltimo, ya que estamos, metemos en una collection de MongoDB todos los datos que obtenemos de el-tiempo.net.</p>"},{"location":"nifi/nifiCasoUsoMeteo/#ayuda","title":"Ayuda","text":"<p>Para la creaci\u00f3n de la tabla en PostgreSQL</p> <pre><code>CREATE TABLE datos_meteo_xativa (\n    id SERIAL PRIMARY KEY,\n    fecha TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    media_temperatura DOUBLE PRECISION,\n    minima_temperatura DOUBLE PRECISION,\n    maxima_temperatura DOUBLE PRECISION,\n    cantidad_precipitaciones DOUBLE PRECISION,\n    acumulado_precipitaciones DOUBLE PRECISION,\n    media_humedad DOUBLE PRECISION,\n    minima_humedad DOUBLE PRECISION,\n    maxima_humedad DOUBLE PRECISION,\n    media_viento DOUBLE PRECISION,\n    minima_viento DOUBLE PRECISION,\n    maxima_viento DOUBLE PRECISION\n);\n</code></pre> <p>El c\u00f3digo del script a utilizar (Groovy) es el siguiente: </p> <p>Completa la parte centrar marcada e intenta comprender qu\u00e9 hace sl script.</p> <pre><code>import org.apache.commons.io.IOUtils\nimport java.nio.charset.StandardCharsets\nimport groovy.json.JsonSlurper\nimport groovy.json.JsonOutput\n\ndef flowFile = session.get()\nif (flowFile != null) {\n    flowFile = session.write(flowFile, { inputStream, outputStream -&gt;\n        def text = IOUtils.toString(inputStream, StandardCharsets.UTF_8)\n        def jsonSlurper = new JsonSlurper()\n\n        // Separar los objetos JSON concatenados\n        def data = text.split('(?&lt;=})\\\\s*(?=\\\\{)').collect { jsonSlurper.parseText(it) }\n\n        def temperaturas = data.collect { it.temperatura_actual.toDouble() }\n        def precipitaciones = data.collect { it.precipitacion.toDouble() }\n        def humedades = data.collect { it.humedad.toDouble() }\n        def vientos = data.collect { it.viento.toDouble() }\n\n        def resultado = [\n            media_temperatura: temperaturas.sum() / temperaturas.size(),\n            maxima_temperatura: temperaturas.max(),\n            minima_temperatura: temperaturas.min(),\n            cantidad_precipitaciones: precipitaciones.last() - precipitaciones.first(),\n            ------------------------------------------------\n            Completa el script aqu\u00ed\n            ------------------------------------------------  \n            minima_viento: vientos.min()\n        ]\n\n        outputStream.write(JsonOutput.toJson(resultado).getBytes(StandardCharsets.UTF_8))\n    } as StreamCallback)\n    session.transfer(flowFile, REL_SUCCESS)\n}\n</code></pre>"},{"location":"nifi/nifiCasoUsoMeteo/#entregable","title":"Entregable","text":"<p>Realiza un documento PDF con las capturas necesarias para demostrar que el sistema funciona correctamente y entr\u00e9galo junto con el Flow Definition.</p>"},{"location":"nifi/nifiCasoUsoMongoDB/","title":"nifiCasoUsoMongoDB","text":"<p>Por definir Similar al de la api pero acabant amb MongoDB</p>"},{"location":"nifi/nifiCasoUsoPostgreSQL/","title":"nifiCasoUsoPostgreSQL","text":""},{"location":"nifi/nifiCasoUsoPostgreSQL/#analisis-y-comparativa-de-temperaturas-y-acumulacion-de-lluvia-2022-2024","title":"An\u00e1lisis y comparativa de temperaturas y acumulaci\u00f3n de Lluvia (2022-2024)","text":""},{"location":"nifi/nifiCasoUsoPostgreSQL/#objetivo","title":"Objetivo","text":"<p>Realizar un an\u00e1lisis mensual de las temperaturas medias y la acumulaci\u00f3n de lluvia para los a\u00f1os 2022, 2023 y 2024, aprovechando los datos meteorol\u00f3gicos de Espa\u00f1a para obtener conclusiones a trav\u00e9s de una herramienta de reporting.</p> <p>Este caso de uso proporciona una soluci\u00f3n automatizada y eficiente para la integraci\u00f3n, procesamiento y an\u00e1lisis de datos meteorol\u00f3gicos, ayudando en la toma de decisiones y en la creaci\u00f3n de informes precisos y oportunos.</p>"},{"location":"nifi/nifiCasoUsoPostgreSQL/#flujo-de-datos","title":"Flujo de datos","text":"<ol> <li> <p>Obtenci\u00f3n de datos:</p> <ul> <li>Fuentes de Informaci\u00f3n:         - INE (Instituto Nacional de Estad\u00edstica): Relaci\u00f3n de provincias y municipios de Espa\u00f1a.         - AEMET (Agencia Espa\u00f1ola de Meteorolog\u00eda): Datos meteorol\u00f3gicos de todas las estaciones de Espa\u00f1a.</li> <li>Generaci\u00f3n de Cat\u00e1logo de Fechas: Se crea un cat\u00e1logo que permita tener las fechas de las mediciones de temperatura y lluvia a nivel mensual para cada estaci\u00f3n.</li> </ul> </li> <li> <p>Carga de datos a base de datos (PostgreSQL):    Los datos obtenidos de las fuentes (INE, AEMET y el cat\u00e1logo de fechas) se cargar\u00e1n en una base de datos en una capa de tablas Staging. Esta capa intermedia es utilizada para mantener los datos en su formato bruto y poder realizar la limpieza y validaci\u00f3n sin afectar a los sistemas fuente.</p> </li> <li> <p>Transformaci\u00f3n de los datos:    Se realizar\u00e1n los tratamientos necesarios en las tablas Staging, tales como:</p> <ul> <li>Limpieza de datos (por ejemplo, valores nulos, duplicados).</li> <li>C\u00e1lculos de agregados de temperatura media y acumulaci\u00f3n de lluvia a nivel mensual.</li> <li>Creaci\u00f3n de claves for\u00e1neas y relaciones necesarias para el modelo dimensional.</li> </ul> </li> <li> <p>Carga a capa final en la base de datos:    Una vez procesados los datos, se cargan en las capas finales del modelo multidimensional:</p> <ul> <li>Cat\u00e1logos/Dimensiones: Informaci\u00f3n jer\u00e1rquica y descriptiva (por ejemplo, provincias, municipios, estaciones).</li> <li>Modelo estrella (ODS): Factos y medidas agregadas (temperatura media, acumulaci\u00f3n de lluvia) por mes.</li> </ul> </li> <li> <p>C\u00e1lculo de KPIs:    Seg\u00fan los requisitos de tiempo, se tienen dos opciones para los KPIs:</p> <ul> <li>Opci\u00f3n 1: Agregado en la capa ODS: Se realizan los c\u00e1lculos de KPIs (por ejemplo, temperatura media anual, acumulaci\u00f3n de lluvia mensual) directamente sobre la capa ODS para facilitar y agilizar las consultas.</li> <li>Opci\u00f3n 2: C\u00e1lculos en la herramienta de reporting: Los KPIs se calculan directamente en la herramienta de reporting, lo que permite mayor flexibilidad, aunque a costa de tiempos de carga m\u00e1s largos.</li> </ul> </li> <li> <p>An\u00e1lisis de los datos (esta parte la haremos al final cuando veamos PowerBI):</p> <ul> <li>Herramienta de reporting: Utilizando una herramienta de reporting (como Power BI, Tableau, etc.), los usuarios podr\u00e1n generar informes, tablas y gr\u00e1ficos que permitir\u00e1n comparar los datos de temperatura y lluvia de los a\u00f1os 2022, 2023 y 2024.  </li> <li>A trav\u00e9s de los KPIs y visualizaciones, se podr\u00e1n sacar conclusiones y hacer predicciones o recomendaciones sobre el comportamiento del clima en las diferentes provincias y municipios de Espa\u00f1a.</li> </ul> </li> </ol>"},{"location":"nifi/nifiCasoUsoPostgreSQL/#tecnologias-utilizadas","title":"Tecnolog\u00edas utilizadas","text":"<ul> <li>Apache NiFi: Orquestaci\u00f3n de la integraci\u00f3n de datos desde las fuentes hasta la base de datos, garantizando un flujo controlado de los datos.</li> <li>PostgreSQL: Base de datos relacional para almacenamiento de datos procesados y generaci\u00f3n del modelo multidimensional.</li> <li>Herramienta de Reporting (ej. Power BI): Visualizaci\u00f3n y an\u00e1lisis de los KPIs generados.</li> </ul>"},{"location":"nifi/nifiCasoUsoPostgreSQL/#entregable","title":"Entregable","text":"<p>Guardar la plantilla:</p> <pre><code>- Despu\u00e9s de configurar todos los processors, selecciona todos los processors y haz clic en el bot\u00f3n derecho para elegir la opci\u00f3n \"Create Template\".\n- Nombra tu plantilla (por ejemplo, \"PostgreSQL_Template\").\n- Guarda la plantilla y podr\u00e1s reutilizarla en futuros proyectos.\n</code></pre>"},{"location":"nifi/nifiCasoUsoPostgreSQL/#anexo","title":"Anexo","text":"<p>Cat\u00e1logo de fechas</p> <ul> <li>Descargar fichero sql generaci\u00f3n cat\u00e1logo de fechas</li> </ul> <p>INE</p> <ul> <li>INE: Relaci\u00f3n Provincia-Municipio</li> </ul> <p>AEMET</p> <ul> <li> <p>\u00bfC\u00f3mo pedir una API Key?</p> </li> <li> <p>Centro de descargas de Aemet</p> </li> <li> <p>Valores climatol\u00f3gicos di\u00e1rios</p> </li> </ul>"},{"location":"nifi/nifiIntroduccion/","title":"NiFi","text":""},{"location":"nifi/nifiIntroduccion/#que-es-apache-nifi","title":"\u00bfQu\u00e9 es Apache NiFi?","text":"<p>Apache NiFi es un sistema distribuido dedicado a extraer, transformar y cargar datos (ETL). Es Open Source y est\u00e1 desarrollado y mantenido por la Apache Software Foundation.</p> <p>NiFi (o Ni-Fi) ha sido dise\u00f1ado para poder automatizar de una manera eficiente y visual los flujos de datos entre distintos sistemas: ingesta, enrutado y gesti\u00f3n. Para ello, cuenta con m\u00e1s de 300 conectores externos ya implementados y adem\u00e1s es posible a\u00f1adir conectores a medida.</p> <p>Uno de los puntos fuertes de NiFi es la capacidad para programar flujos de datos arrastrando y conectando los componentes necesarios sobre los canvas de la web de administraci\u00f3n. No es necesario por tanto tener conocimientos de programaci\u00f3n espec\u00edficos, sino entender y configurar correctamente cada uno de los componentes que se quieren usar.</p> <p>Aunque se pueda considerar una herramienta ETL, NiFi no est\u00e1 realmente optimizado para realizar transformaciones de datos complejas o pesadas. Es posible realizar transformaciones de datos ligeras pero no es un motor de transformaciones batch completo. A\u00fan as\u00ed es com\u00fan su uso integrado en sistemas Big Data, ya que ofrece muchas ventajas como herramienta de automatizaci\u00f3n de ingestas de datos y para realizar transformaciones y limpiezas sencillas.</p>"},{"location":"nifi/nifiIntroduccion/#componentes-principales","title":"Componentes principales","text":""},{"location":"nifi/nifiIntroduccion/#basicos","title":"B\u00e1sicos","text":"<ul> <li> <p>Flow: El workflow o topolog\u00eda es la definici\u00f3n del flujo de datos que se implementa en NiFi e indica la forma en la que se deben gestionar los datos.</p> </li> <li> <p>Flowfile: Es el paquete de datos que viaja por el flow entre los procesadores. Est\u00e1 compuesto por un puntero al propio dato \u00fatil o contenido (un array de bytes) y metadatos asociados llamados atributos. Los atributos pares clave-valor editables y NiFi los usa para enriquecer la informaci\u00f3n de provenance. Los metadatos m\u00e1s importantes son el identificador (uuid), el nombre del fichero (filename) y el path Para acelerar el rendimiento del sistema, el flowfile no contiene el propio dato, sino que apunta al dato en el almacenamiento local. Muchas de las operaciones que se realizan en NiFi no alteran el propio dato ni necesitan cargarlo en memoria. En concreto, el dato se encuentra en el llamado repositorio de contenido (Content Repository)</p> </li> </ul> <ul> <li>Processor: Los procesadores son los componentes principales de NiFi. Se encargan de ejecutar el proceso de extracci\u00f3n, transformaci\u00f3n o carga de datos. NiFi permite realizar operaciones diversas en los processors, as\u00ed como distribuir y programar su ejecuci\u00f3n. Estos componentes tambi\u00e9n proporcionan una interfaz para acceder a los flowfiles y sus propiedades. Se pueden implementar nuevos processors personalizados mediante una api de programaci\u00f3n en Java o bien usar los m\u00e1s de 280 processors existentes. Los processors permiten abstraer la complejidad de la programaci\u00f3n concurrente y pueden ejecutar en varios nodos de forma simult\u00e1nea o bien en el nodo primario del cl\u00faster. Adem\u00e1s, es posible programar su ejecuci\u00f3n mediante cron, tiempo predefinido o mediante eventos de entrada. Los processors tambi\u00e9n tienen relaciones de salida (connections) en funci\u00f3n de su comportamiento, por ejemplo \u00e9xito (success), fallo (failure) o reintento (retry). Llevan incorporado un validador de configuraci\u00f3n y gr\u00e1ficas con las estad\u00edsticas de uso e indicadores de trazabilidad.</li> </ul>"},{"location":"nifi/nifiIntroduccion/#avanzados","title":"Avanzados","text":"<ul> <li> <p>Connection: Son las tuber\u00edas de conexi\u00f3n entre dos Processors que les permiten interactuar. Es la encargada de transmitir los flowfiles entre los componentes y de gestionar las colas y su capacidad. Las conexiones act\u00faan como un buffer para los flowfiles, y tienen un sistema de backpressure en funci\u00f3n del n\u00famero de eventos o del tama\u00f1o en disco. Tambi\u00e9n es posible establecer la caducidad para los flowfiles o su prioridad. Mediante los funnels, NiFi permite agrupar varias conexiones en una.</p> </li> <li> <p>Process Group: Agrupaci\u00f3n de processors y connections para tratarlos como una unidad l\u00f3gica independiente dentro del flujo de procesamiento. Para interactuar con el resto de componentes tienen puertos de entrada y de salida que gestionan el env\u00edo de flowfiles. NiFi tambi\u00e9n incorpora los llamados Remote Process Groups (RPGs). Permiten tratar otra instancia o cl\u00faster externo de NiFi como un Process Group con el que interactuar. En vez de mover flowfiles entre diferentes process groups, se mueven entre distintos cl\u00fasters. Los puertos de entrada y de salida act\u00faan como puertas de entrada para los flowfiles.</p> </li> <li> <p>Controller Service: Los controller service o controladores se utilizan para compartir un recurso entre distintos processors. Por ejemplo puede ser una conexi\u00f3n a una base de datos, a S3 o a un contenedor de Azure.</p> </li> </ul> <p>Apache NiFi tambi\u00e9n nos permite crear plantillas (templates) con un flow almacenado. Las plantillas resultan muy \u00fatiles para a\u00f1adir de forma r\u00e1pida un nuevo conjunto de componentes est\u00e1ndar o mover sub-flujos entre distintos entornos de trabajo.</p>"},{"location":"nifi/nifiIntroduccion/#processor-mas-utilizados","title":"Processor m\u00e1s utilizados","text":""},{"location":"nifi/nifiIntroduccion/#ingesta-del-dato","title":"Ingesta del dato","text":"<pre><code>GenerateFlowFiles\u200b\nGetFile\u200b\nGetFTP\u200b\nGetSFTP\u200b\nGetJMSQueue\u200b\nGetJMSTopic\u200b\nGetHTTP\u200b\nListenHTTP\u200b\nListenUDP\u200b\ngetHDFS\u200b\nGetKafka\u200b\nQueryDatabaseTable\u200b\nGetMongo\u200b\nGetTwitter\u200b\nListHDFS / FetchHDFS\n</code></pre>"},{"location":"nifi/nifiIntroduccion/#transformacion-del-dato","title":"Transformaci\u00f3n del dato","text":"<pre><code>ConvertRecord\u200b\nUpdateRecord\u200b\nConvertJSONToSQL\u200b\nReplaceText\u200b\nCompressContent\u200b\nConvertCharacterSet\u200b\nEncryptContent\u200b\nTransformXml\u200b\nJoltTransformJSON\nSplitText\u200b\nSplitJson\u200b\nSplitXml\u200b\nSplitRecord\u200b\nSplitContent\u200b\nUnpackContent\u200b\nSegmentContent\u200b\nMergeContent\u200b\nQueryRecord\n</code></pre>"},{"location":"nifi/nifiIntroduccion/#envio-del-dato","title":"Env\u00edo del dato","text":"<pre><code>PutEmail\u200b\nPutFile\u200b\nPutFTP\u200b\nPutSFTP\u200b\nPutJMS\u200b\nPutSQL\u200b\nPutKafka\u200b\nPutMongo\u200b\nPutHDFS\n</code></pre>"},{"location":"nifi/nifiIntroduccion/#acceso-a-bases-de-datos","title":"Acceso a bases de datos","text":"<pre><code>ConvertJSONToSQL\u200b\nExecuteSQL\u200b\nPutSQL\u200b\nSelectHiveQL\u200b\nPutHiveQL\u200b\nListDatabaseTables\n</code></pre>"},{"location":"nifi/nifiIntroduccion/#extraccion-de-atributos","title":"Extracci\u00f3n de atributos","text":"<pre><code>EvaluateJsonPath\u200b\nEvaluateXPath\u200b\nEvaluateXQuery\u200b\nExtractText\u200b\nHashAttribute\u200b\nHashContent\u200b\nIdentifyMimeType\u200b\nUpdateAttribute\u200b\nLogAttribute\n</code></pre>"},{"location":"nifi/nifiIntroduccion/#arquitectura","title":"Arquitectura","text":"<p>Apache NiFi es una aplicaci\u00f3n Java que ejecuta en la JVM. En la imagen a continuaci\u00f3n podemos ver los componentes m\u00e1s importantes de su arquitectura.</p> <ul> <li> <p>El Flowfile Repository o repositorio de flowfiles almacena los atributos y del estado de cada flujo del sistema, junto a las referencias al contenido. Tambi\u00e9n almacena la cola en la que se encuentra en ese momento. Mantiene solamente el estado m\u00e1s actualizado del sistema mediante el Write-Ahead Log, lo que garantiza recuperar el estado m\u00e1s actualizado frente a una ca\u00eda del sistema.</p> </li> <li> <p>Los flowfiles se almacenan en un Hashmap en memoria. Cuando el n\u00famero de flowfiles en esta estructura excede el establecido en la propiedad nifi.queue.swap.threshold, NiFi los escribe a un fichero swap en disco seg\u00fan su prioridad.</p> </li> <li> <p>El Content Repository mantiene todo el contenido de los flowfiles. Cada vez que un dato se modifica se realiza una copia para no perder el original (copy on write). Pueden existir varios content repositories en un sistema, cada uno de ellos llamado contenedor y a su vez dividido en secciones.</p> </li> <li> <p>Por \u00faltimo el Provenance Repository se encarga de almacenar la informaci\u00f3n de la procedencia y el origen de cada flowfile mediante snapshots a partir de los que se podr\u00eda restaurar el ciclo de vida de cada uno. Este repositorio a\u00f1ade la dimensi\u00f3n del tiempo.</p> </li> </ul> <p>En los sistemas en los que NiFi tiene un volumen de datos muy alto, es posible que el content repository llene el disco, y en el caso de que el flowfile repository se encuentre en el mismo disco, podr\u00eda corromper su contenido, por lo que es algo a tener en cuenta al dise\u00f1ar la soluci\u00f3n.</p>"},{"location":"nifi/nifiIntroduccion/#streaming-en-nifi","title":"Streaming en NiFi","text":"<p>Para los casos de uso de Streaming, Apache NiFi es una tecnolog\u00eda con sus limitaciones. Por un lado, no est\u00e1 dise\u00f1ado para realizar joins sobre flowfiles de manera eficiente ni agregaciones de datos en ventanas de procesamiento.</p> <p>Una de las maneras de tratar los casos de uso de streaming es escribir los registros en un cl\u00faster de Apache Kafka. Una vez en Kafka, se podr\u00e1n procesar f\u00e1cilmente com Kafka Streams o Apache Flink.</p>"},{"location":"nifi/nifiIntroduccion/#ejemplo","title":"Ejemplo","text":"<p>En la imagen a continuaci\u00f3n podemos ver un ejemplo de un flujo en Apache NiFi. Este flujo se compone de dos processors: GetFile y PutFile.</p> <p>Tambi\u00e9n existen 2 conexiones, cada una de ellas se corresponde un un evento particular generado por el processor del que parte. Act\u00faan de cola para los flowfiles generados.</p> <p>El prop\u00f3sito del flujo es conseguir flowfiles en el primer processor de un origen (sftp, localhost,etc) y escribir los datos en el disco local mediante el processor PutFile.</p>"},{"location":"nifi/nifiIntroduccion/#ventajas","title":"Ventajas","text":"<ul> <li> <p> Facilidad de uso mediante UI</p> </li> <li> <p> Escalable horizontalmente</p> </li> <li> <p> Gran cantidad de componentes out-of-the-box (processors y conectores)</p> </li> <li> <p> Es posible implementar nuevos componentes y procesadores (programando con la API de Java)</p> </li> <li> <p> Se encuentra en constante evoluci\u00f3n y con una gran comunidad</p> </li> <li> <p> Incorpora auditor\u00eda del dato</p> </li> <li> <p> Tiene integrada la validaci\u00f3n de configuraciones</p> </li> <li> <p> Pol\u00edtica de Usuarios (LDAP)</p> </li> <li> <p> Software multiplataforma</p> </li> <li> <p> Linaje de datos integrada, de cara a cumplir regulaciones.</p> </li> <li> <p> Uso para enrutar mensajes a microservicios</p> </li> <li> <p> Integrado en Cloudera Data Platform (CDP) / Cloudera Flow Management (CDF)</p> </li> </ul>"},{"location":"nifi/nifiIntroduccion/#inconvenientes","title":"Inconvenientes","text":"<ul> <li> <p> Consumo de recursos de hardware muy elevado en funci\u00f3n de la carga de procesamiento</p> </li> <li> <p> Otras herramientas como Apache Flume son m\u00e1s ligeras y adecuadas para realizar transformaciones de datos simples</p> </li> </ul>"},{"location":"nifi/nifiIntroduccion/#alternativas","title":"Alternativas","text":"<p>Existen alternativas a Apache NiFi:     Como soluciones para gestionar dataflows, cada una con sus particularidades:</p> <pre><code>Streamsets\nAzure Data Factory\nAWS Data Pipeline\n</code></pre> <p>Mucha gente se pregunta el motivo de usar NiFi cuando ya se est\u00e1 usando Apache Kafka como punto de entrada al sistema de datos. Debemos tener en cuenta que Kafka est\u00e1 dise\u00f1ado para casos de uso de streaming y manejar ficheros de datos peque\u00f1os.</p> <p>NiFi puede manejar ficheros de datos mucho m\u00e1s grandes que Apache Kafka. Adem\u00e1s, NiFi proporciona una interfaz muy potente que permite controlar y administrar las operaciones de ingesta y de transformaci\u00f3n de forma sencilla y centralizada. Tambi\u00e9n debemos tener en cuenta la gran cantidad de conectores y procesadores que aporta NiFi, por lo que ambas tecnolog\u00edas son complementarias.</p>"},{"location":"nifi/nifiIntroduccion/#instalacion-nifi","title":"Instalaci\u00f3n NiFi","text":"<p>Nos descargamos en binario de la web oficial.\u200b Ocupa m\u00e1s de 1GB porque contiene todos los procesadores. Adem\u00e1s, necesitamos tener Java\u200b instalado.</p> <ul> <li>Descargamos NiFi:     NiFi 1.28.1</li> </ul> <pre><code>wget https://dlcdn.apache.org/nifi/1.28.1/nifi-1.28.1-bin.zip\n</code></pre> <ul> <li> <p>Descomprimimos el zip en el directorio /opt/</p> </li> <li> <p>Nos situamos en la carpeta /bin/</p> </li> <li> <p>Vamos a /conf/ y modificamos el nifi.propierties las siguientes propiedades:</p> </li> </ul> <pre><code>nifi.remote.input.secure=false\nnifi.web.http.host= ip de tu maquina\nnifi.web.http.host= puerto de tu maquina, p.e. 8443\n\nLes opcions en \"https...\" comentar-les:\n\nnifi.web.https.host=\nnifi.web.https.port=\n</code></pre> <ul> <li> <p>Iniciamos por primera vez NiFi. Ejecutamos: <pre><code>./bin/nifi.sh start (Unix)\n./bin/nifi.bat start (MSDOS)\n</code></pre></p> </li> <li> <p>Acceso. Utiliza por defecto el puerto 8443 (Linux)\u200b</p> <p>https://localhost:8443/nifi (Puede tardar)\u200b</p> </li> </ul> <p>Nota: Si no es el 8443 se puede hacer un grep al log buscando 127.0.0.1</p> <ul> <li>Para terminar. Obtenemos el usuario. El usuario se puede crear v\u00eda comandos por cmd, o se puede coger el que genera autom\u00e1ticamente en el log NiFi en el fichero nifi-app.log:  \u200b</li> </ul> <pre><code>Generated Username [80e91118-b222-4b47-8dab-63a8deb7905d]\u200b\nGenerated Password [zavwbGlRcYeky51Bxc0zbVN8hj2bE61u]\u200b\n</code></pre> <ul> <li>Si queremos personalizar el usuario:\u200b</li> </ul> <pre><code>/bin/nifi.sh set-single-user-credentials USERNAME PASSWORD\n</code></pre> <ul> <li>Instalar como servicio NiFi</li> </ul> <pre><code>bin/nifi.sh install dataflow.\n</code></pre> <ul> <li> <p>Una vez instalado, puedes hacer \"start\" o \"stop\" con los comandos apropiados. Tambi\u00e9n consultar el estado actual.</p> <p>sudo service nifi start</p> <p>sudo service nifi stop</p> <p>service nifi status</p> </li> </ul>"},{"location":"nifi/nifiPractica1/","title":"nifiPractica1","text":""},{"location":"nifi/nifiPractica1/#mover-un-fichero-de-un-origen-a-un-destino","title":"Mover un fichero de un origen a un destino","text":"<p>Iniciamos NiFi. Ejecutamos:</p> <pre><code>    ./bin/nifi.sh start (Unix)\n\n    ./bin/nifi.bat start (MSDOS)\n</code></pre> <p>Accedemos a la siguiente url que se levantar\u00e1 en el ordenador (o la que indica el  Readme, seg\u00fan la versi\u00f3n funcionara una u otra): </p> <p>https://localhost:8443/nifi/</p> <p>Pedir\u00e1 unas credenciales. Podemos utilizar las creadas de manera autom\u00e1tica  al arrancar por primera vez en el fichero de logs que podr\u00e9is encontrar en: logs/nifiapp.log. Lo encontrareis buscando la siguiente palabra clave: Username</p> <p>Una vez hayamos hecho login, nos aparecer\u00e1 la interfaz de trabajo.</p> <p>En la barra de iconos de herramientas, hacemos clic en \u201cProcessor\u201d y arrastramos  hacia el \u00e1rea de trabajo</p> <p>En la ventana de \u201cAdd Processor\u201d b\u00fascamos getFile. Y hacemos clic en \u201cAdd\u201d.</p> <p>Repetimos el mismo proceso para a\u00f1adir el processor \u201cputFile\u201d. Quedando algo similar  a la siguiente imagen.</p> <p>Ahora, vamos a configurar de d\u00f3nde vamos a coger el fichero origen y d\u00f3nde lo vamos a dejar en el destino. </p> <p>Hacemos clic bot\u00f3n derecho en \u201cGetFile\u201d en \u201cConfigure\u201d y vamos a la pesta\u00f1a  \u201cProperties\u201d. D\u00f3nde podemos ver los diferentes aspectos que son configurables.  Ahora mismo s\u00f3lo vamos a indicar el directorio de entrada \u201cInput Directory\u201d.</p> <p>Hacemos clic bot\u00f3n derecho en \u201cPutFile\u201d en \u201cConfigure\u201d y vamos a la pesta\u00f1a  \u201cProperties\u201d. D\u00f3nde podemos ver los diferentes aspectos que son configurables.  Ahora mismo s\u00f3lo vamos a indicar el directorio de entrada \u201cDirectory\u201d.</p> <p>Ya configurado el componente del origen de los ficheros y el destino, s\u00f3lo nos queda  unirlos. Si hacemos clic sobre el componente \u201cGetFile\u201d veremos una flecha que nos  permitir\u00e1 arrastrarla hasta el otro componente \u201cPutFile\u201d. Quedando como la siguiente  imagen:</p> <p>Ahora nos queda probar que traslada correctamente los ficheros, arrancando los dos  componentes con bot\u00f3n derecho y hacer clic en \u201cStart\u201d. En los dos componentes.  Primero el \u201cGetFile\u201d y luego el \u201cPutFile\u201d.</p> <p>Si al realizar la acci\u00f3n no sale la opci\u00f3n en el \u201cPutFile\u201d lo que hay que hacer es entrar  en este componente en propiedades e indicarle que acci\u00f3n debe hacer en el caso de  que falle</p> <p>Una vez todo configurado y en estado \u201cStart\u201d ver\u00e9is que empieza a mover el fichero  del origen configurado al destino. Marcado con verde como que se ha ejecutado  correctamente</p>"},{"location":"nifi/nifiPractica1/#entregable","title":"ENTREGABLE","text":"<p>Hay que exportar la practica con un Processor Group que genera un .json y entregarlo en Aules con el n\u00famero de la pr\u00e1ctica y el nombreApellidos. Por ejemplo:  P1_NomAlumnoApellidos.xml</p>"},{"location":"nifi/nifiPractica10/","title":"Transformando datos con NiFi","text":""},{"location":"nifi/nifiPractica10/#enunciado","title":"Enunciado","text":"<p>En la pr\u00e1ctica anterior de la API del INE consultamos la encuesta de poblaci\u00f3n activa. Vamos a continuar con esta pr\u00e1ctica leyendo los datos con NiFi.</p> <p>Recordemos que es necesario averiguar el c\u00f3digo para realizar esta consulta, y nos devolver\u00e1 un JSON:  </p> <p>Recordar</p> <pre><code>http://servicios.ine.es/wstempus/js/ES/DATOS_SERIE/&lt;c\u00f3digo serie&gt;?nult=&lt;n\u00famero datos&gt;\n</code></pre> <p>Ahora transferiremos estos datos para prepararlos para su uso futuro.</p> <p>Vamos a utilizar, como en la pr\u00e1ctica, dos carpetas: una donde dejaremos los archivos de origen y otra donde dejaremos la transformaci\u00f3n.</p>"},{"location":"nifi/nifiPractica10/#ejercicio","title":"Ejercicio","text":"<p>a) Haz que tome el JSON, lo transforme y lo guarde como CSV. b) Haz que tome el JSON y lo guarde en PostgreSQL. c) Haz que tome el JSON y lo guarde en MongoDB. d) Haz que tome el CSV aplanado del punto anterior y lo guarde en PostgreSQL. e) Haz que tome el CSV y lo guarde en MongoDB.  </p>"},{"location":"nifi/nifiPractica10/#notas","title":"Notas","text":"<ul> <li> <p>Como la carpeta de entrada es la misma, deber\u00e1s filtrar los procesadores GetFile por extensi\u00f3n. </p> </li> <li> <p>Aprovecha al m\u00e1ximo los componentes, por ejemplo, utilizando el n\u00famero m\u00ednimo posible de GetFile.  </p> </li> <li> <p>Utiliza el procesador LogAttribute para comprobar si ha habido errores o si todo ha funcionado correctamente, verificando las colas que se crean para acceder al componente.  </p> </li> <li> <p>Procesadores a utilizar: GetFile, ConvertJSONtoSQL, ConvertRecord, PutDatabaseRecord, PutSQL, PutFile, PutMongoRecord y LogAttribute*.  </p> </li> <li> <p>Aseg\u00farate de que cada archivo CSV tenga un nombre diferente para evitar sobrescribir cada transformaci\u00f3n. </p> </li> </ul> <p>La estructura de nuestro proyecto NiFi deber\u00e1 ser similar al siguiente esquema:</p>"},{"location":"nifi/nifiPractica10/#entregable","title":"Entregable","text":"<p>Realiza un documento PDF con las capturas necesarias para demostrar que el sistema funciona correctamente y entr\u00e9galo junto con el Flow Definition.</p>"},{"location":"nifi/nifiPractica2/","title":"nifiPractica2","text":""},{"location":"nifi/nifiPractica2/#atributos-y-contenido","title":"Atributos y contenido","text":"<p>Generaremos primero un fichero aleatorio o con contenido personalizado, le aplicaremos  alguna transformaci\u00f3n y luego alteraremos en este sus propiedades. Tambi\u00e9n veremos por los diferentes estados que pasa la ejecuci\u00f3n y c\u00f3mo ver en cada punto como va evolucionando el  proceso.</p> <p>Una vez hayamos hecho login, nos aparecer\u00e1 la interfaz de trabajo</p> <p>En la barra de iconos de herramientas, hacemos clic en \u201cProcessor\u201d y arrastramos  hacia el \u00e1rea de trabajo.</p> <p>En la ventana de \u201cAdd Processor\u201d b\u00fascamos \u201cGenerateFlowFile\u201d. Y hacemos clic en  \u201cAdd\u201d.</p> <p>Nota: Este processor crea FlowFiles con datos aleatorios o contenido personalizado</p> <p>Ahora, vamos a configurarlo con datos aleatorios conforme la siguiente imagen. Y  planificarlo.</p> <p>Ahora vamos a a\u00f1adir a la hoja de trabajo, otro processor llamado \u201cReplace Text  Processor\u201d</p> <p>Nota: bajo cada processor encontraremos una breve descripci\u00f3n de la funcionalidad</p> <p>Establecemos la conexi\u00f3n entre los dos processor antes de configurar el ReplaceText.</p> <p>Le dedicamos unos minutos a entender el processor \u201cReplaceText\u201d que propiedades  nos aporta.  Nota: Es recomendable revisar siempre antes de usar las propiedades disponibles y si  es necesario, realizar alguna peque\u00f1a prueba.</p> <p>Queremos remplazar en el contenido del fichero generado por unos valores separados  por comas. Y guardamos</p> <p>Para visualizar que es lo que pasa, vamos a a\u00f1adir un processor \u201clog attribute\u201d y  conectarlo sobre el processor \u201cReplaceText\u201d. En esta conexi\u00f3n, debemos activar la  casilla \u201csuccess\u201d.</p> <p>Vamos a probar la generaci\u00f3n del fichero y transformaci\u00f3n de este. Le damos a \u201cStart\u201d con bot\u00f3n derecho sobre el processor \u201cGenerateFlowFile\u201d. Veremos que genera un fichero en la cola. Una vez lo genere, vamos a pararlo de la misma manera con la acci\u00f3n \u201cStop\u201d. Ya que, si recordamos, lo hemos planificado para que se ejecute cada 5 segundos. Si no, estar\u00e1 generando cada 5 segundos un nuevo fichero.</p> <p>En la cola, podemos validar el contenido generado, con bot\u00f3n derecho, \u201cList queue\u201d. Si damos al primer bot\u00f3n (columna), podremos ver/descargar el fichero generado.</p> <p>Si el processor os pone una alerta y no os deja arrancarlo. Es debido a que necesita  que especifiquemos que hacer cuando falle. Cuando tenga \u00e9xito ira al processor  conectado, pero cuando falle, que acci\u00f3n debe hacer. Vamos a propiedades, a  relaciones y ah\u00ed lo podemos configurar</p> <p>Ahora ya le podemos dar la acci\u00f3n \u201cStart\u201d y ver que hace el \u201cReplaceText\u201d que le  hemos indicado al fichero de la cola. Podemos ir a la cola y validar que los ficheros se han convertido conforme lo esperado.</p> <p>Ahora vamos a a\u00f1adir el processor \u201cExtractText\u201d y en propiedades de este  configuramos una nueva propiedad \u201ccsv\u201d con una expresi\u00f3n regular \u201c(.+),(.+),(.+),(.+)\u201d de la siguiente manera:</p> <p>Ahora conectamos el \u201cReplaceText\u201d con \u201cExtractText\u201d eliminando la anterior relaci\u00f3n con \u201cLogAttribute\u201d. Y luego, conectamos \u201cExtractText\u201d con \u201cLogAttribute\u201d. En la definici\u00f3n de la relaci\u00f3n, se ha de marcar el check \u201cmatched\u201d. Como veis, el processor ExtractText tiene una alerta amarilla, quiere decir que no est\u00e1 del todo bien configurado. Pasa lo mismo que antes, es necesario a\u00f1adir cuando la acci\u00f3n contraria a \u201cmatch\u201d que tiene que hacer. Vamos a propiedades de \u201cExtractText\u201d, a \u201cRelationships\u201d y marcamos la acci\u00f3n \u201cterminate\u201d.</p> <p>Llegado a este punto, le damos a la acci\u00f3n \u201cstart\u201d del processor \u201cExtractText\u201d y vemos como procesa el fichero con una salida en la cola. Podemos ver el resultado en la cola de nuevo como ha generado nuevos atributos seg\u00fan la expresi\u00f3n regular que le hemos aplicado, ya que el contenido en este caso no se ha visto modificado.</p>"},{"location":"nifi/nifiPractica2/#entregable","title":"ENTREGABLE","text":"<p>Hay que exportar la practica con un Processor Group que genera un .json y entregarlo en Aules con el n\u00famero de la pr\u00e1ctica y el nombreApellidos. Por ejemplo:  P2_NomAlumnoApellidos.xml</p>"},{"location":"nifi/nifiPractica3/","title":"nifiPractica3","text":""},{"location":"nifi/nifiPractica3/#expression-language","title":"Expression Language","text":"<p>Partimos del resultado de la anterior practica 2, trabajando con atributos y contenido d\u00f3nde  generaremos primero un fichero aleatorio o con contenido personalizado, le aplicaremos  alguna transformaci\u00f3n y luego alteraremos en este sus propiedades. Tambi\u00e9n veremos por los  diferentes estados que pasa la ejecuci\u00f3n y c\u00f3mo ver en cada punto como va evolucionando el  proceso.</p> <p>El objetivo de esta pr\u00e1ctica es modificar la soluci\u00f3n de la practica 2, a\u00f1adiendo un paso m\u00e1s al final, d\u00f3nde utilizando Expression Language NiFi extraeremos informaci\u00f3n de los atributos de FlowFile que hemos creado para introducirlas en el contenido en formato JSON. Teniendo  como resultado algo similar:</p> <pre><code>{\n\"field1\": \"a\",\n\"field1\": \"b\",\n\"field1\": \"c\",\n\"field1\": \"d\"\n}\n</code></pre> <p>A\u00f1adimos un nuevo ReplaceText mediante el bot\u00f3n de Processor o duplicamos el que  ya ten\u00edamos configurado para tener la misma configuraci\u00f3n. Y lo conectamos entre el  Processor \u201cExtractText\u201d y terminando en el \u201cLogAttribute\u201d</p> <p>Una vez en este punto, nos queda configurar el \u201cReplaceText\u201d a\u00f1adido o duplicado,  con la misma configuraci\u00f3n que el primero, pero con la diferencia que en la propiedad  \u201cReplacementValue\u201d introducimos con Expression Language NiFi de d\u00f3nde va a  obtener los datos y que estructura va a tener para substituir el anterior contenido con el nuevo. El c\u00f3digo es el siguiente, d\u00f3nde referenciamos con el car\u00e1cter \u201c$\u201d que es una propiedad del FlowFile.</p> <pre><code>{\n\"field1\": \"${csv.1}\",\n\"field1\": \"${csv.2}\",\n\"field1\": \"${csv.3}\",\n\"field1\": \"${csv.4}\"\n}\n</code></pre> <p>Ejecutamos y podemos ver en la cola el resultado del contenido, si ha funcionado  correctamente debe salir algo similar a la siguiente imagen:</p> <p>Nota: El Expression Language NiFi es muy \u00fatil y existen muchas funciones que es posible utilizarlas. Para ello recomendamos visitar la siguiente documentaci\u00f3n y dedicarle unos minutos: Apache NiFi Expression Language Guide</p> <p>Ahora vamos a continuar con el dataflow que hemos creado.</p> <p>Si vamos a la cola final, y vemos los ficheros sus propiedades podemos ver que existen  las propiedades de UID y Filename, que son valores \u00fanicos. Estos identifican de forma  \u00fanica el fichero dentro del dataflow y es posible utilizarlos para referenciar el FlowFile.</p> <p>Por ejemplo, vamos a a\u00f1adir un Processor \u201cPutFile\u201d y lo conectaremos entre el \u00faltimo  replaceText y el log. Luego entraremos en las propiedades del PutFile y configuraremos el directorio d\u00f3nde quedamos que se guarde el fichero.</p> <p>Arrancamos todos los procesos para generar un fichero y que llegue hasta la cola del  log y veamos que pasa en el directorio. Confirmamos que el nombre del fichero lo coge  de la propiedad de Filename del FlowFile.</p> <p>Vamos a a\u00f1adir un Processor para actualizar el atributo y poner un nombre a los  ficheros de salida (en la propiedad). El Processor a a\u00f1adir es \u201cUpdateAttribute\u201d. Y  vamos a propiedades y a\u00f1adimos una nueva \u201cFilename\u201d. Y introducimos el valor  siguiente:</p> <pre><code>${filename}-${now():toNumber():format('dd-MM-yy')}.json\n</code></pre> <p>Ya solo quedar\u00eda conectar el Processor antes del PutFile para que le d\u00e9 tiempo a  cambiarle la propiedad. Y volvamos a revisar la salida a ver qu\u00e9 pasa.</p> <p>Hagamos como ejercicio, que el fichero de salida salga con el siguiente formato d\u00f3nde  lo siguiente a la fecha son horas, minutos y segundos:</p>"},{"location":"nifi/nifiPractica3/#entregable","title":"ENTREGABLE","text":"<p>Hay que exportar la practica con un Processor Group que genera un .json y entregarlo en Aules con el n\u00famero de la pr\u00e1ctica y el nombreApellidos. Por ejemplo:  P3_NomAlumnoApellidos.xml</p>"},{"location":"nifi/nifiPractica4/","title":"nifiPractica4","text":""},{"location":"nifi/nifiPractica4/#process-group-input-output-port","title":"Process group, Input-Output port","text":"<p>En NiFi uno o m\u00e1s processor son conectados en un process group. Podemos a\u00f1adir un  process group arrastrando el icono de la barra de herramientas . Partiendo de la practica 3.  Cogemos y a\u00f1adimos un nuevo process group y le ponemos como nombre \u201cCSV to JSON\u201d (es posible obviar este paso, si ya se ha puesto la anterior practica en un process group, duplicandolo y renombrandolo a CSV to JSON) Para decirle como usar los processor\u2019s, debemos seleccionar todos los processor (con  la tecla shift) y arrastrarlos sobre el process group</p> <p>Para ver los componentes de un Process Group, hacemos doble click y accedemos al  detalle. </p> <p>Para salir fuera del detalle, con bot\u00f3n derecho \u201cleave group\u201d saldremos de nuevo  fuera.</p> <p>Nota: Es recomendable utilizar los Process Group para l\u00f3gicas complejas. Es posible duplicar un process group heredando la misma configuraci\u00f3n.</p> <p>A veces, un Process Group puede generar una salida para utilizar en otros procesos.  Vamos a transferir informaci\u00f3n des de un Process Group a otro. Para ello vamos a  utilizar de la barra de herramientas el \u201cinput\u201d y el \u201coutput\u201d port . El actual flujo de datos, lo vamos a partir en dos Process Group de la siguiente manera: Creamos un nuevo Process Group que se llame \u201cWrite JSON to File System\u201d d\u00f3nde  vamos a introduir los processors des de el \u201cUpdateAttribute\u201d hasta el \u00faltimo  \u201cLogAttribute\u201d.</p> <p>C\u00f3mo ya tenemos un process group principal \u201cCSV to JSON\u201d, vamos a sacar el  process group \u201cWrite JSON to File System\u201d fuera para poder ejecutarlos  conjuntamente. Para ello hacemos bot\u00f3n derecho sobre \u201cWrite JSON to File  System\u201d y seleccionamos \u201cMove to parent group\u201d.</p> <p>Ahora el problema est\u00e1 en que los dos processor group es necesario que se  comuniquen para que funcionen. Para ello haremos uso del Input/Output port. Accedemos al process group \u201cCSV to JSON\u201d y a\u00f1adimos un \u201cOutput port\u201d.</p> <p>Salimos del grupo y accedemos ahora al Process Group \u201cWrite JSON to CSV\u201d y a\u00f1adimos un \u201cInput port\u201d al inicio.</p> <p>Salimos al flujo principal, d\u00f3nde ahora vamos a conectar los dos Process Group. Saldr\u00e1  el mapeo de entradas y salidas, hay que tenerlo en cuenta cuando tengamos m\u00e1s de  una entrada/salida que este bien asociado.</p> <p>Ahora solo queda arrancar con bot\u00f3n derecho \u201cstart\u201d y validar que todo funciona  correctamente.</p> <p>Nota: Dentro de un mismo Process Group no es posible conectar un \u201cOutput port\u201d a un \u201cInput port\u201d.</p>"},{"location":"nifi/nifiPractica4/#entregable","title":"ENTREGABLE","text":"<p>Hay que exportar la practica con un Processor Group que genera un .json y entregarlo en Aules con el n\u00famero de la pr\u00e1ctica y el nombreApellidos. Por ejemplo:  P4_NomAlumnoApellidos.xml</p>"},{"location":"nifi/nifiPractica5/","title":"nifiPractica5","text":""},{"location":"nifi/nifiPractica5/#templates","title":"Templates","text":"<pre><code>   &gt; Esta pr\u00e1ctica queda descartada si la haces con NiFi 2.0. Dado que en la versi\u00f3n M2 NiFi ya no se trabaja con los Templates si no con el Registry, exportando los trabajos a trav\u00e9s de un Processor Group, pero si en versiones anteriores. Tenedlo en cuenta dado que si trabajais en versiones anteriores es la forma de guardar y exportar el trabajo realizado.\n</code></pre> <p>Para guardar el trabajo realizado, exportarlo, etc usaremos los \u201cTemplates\u201d. Para ello debemos seleccionar todos los componentes a guardar en el template, y  buscar el icono crear template que encontrareis en la izquierda.</p> <p>Le pondremos por ejemplo \u201cCSV to JSON\u201d y lo creamos.</p> <p>Hacemos click en \u201cCreate\u201d.</p> <p>Este template se almacena en la instancia local. Si queremos ver todos los templates que tenemos creados en nuestra instancia local,  tenemos que ir al bot\u00f3n de la derecha arriba y hacer clic en \u201cTemplates\u201d d\u00f3nde  saldr\u00e1 un listado. D\u00f3nde a la derecha aparece los botones para descargarlos para  una futura importaci\u00f3n o copia de seguridad.</p> <p>*Nota: Existen templates de ejemplo que podemos descargar e importar a nuestra instancia local para practicar o utilizar para un desarrollo. Lo pod\u00e9is encontrar en el siguiente enlace: </p> <p>cwiki</p> <p>Vamos a acceder a la anterior p\u00e1gina web y descargar un template. Por ejemplo, el  \u201cRetry Count Loop\u201d.</p> <p>Nos quedar\u00eda ahora importarlo a nuestro entorno de desarrollo. Accedemos al bot\u00f3n  \u201cUpload Template\u201d y seleccionamos el fichero descargado \u201cRetry_Count_Loop.xml\u201d a  trav\u00e9s del icono .</p> <p>Ahora para utilizarlo o revisar, s\u00f3lo debemos ir a la barra de herramientas y  seleccionar \u201cAdd Template\u201d y arrastrar al espacio de trabajo. Seleccionamos el  template descargado y ver\u00e9is que aparece un nuevo Process Group.</p> <p>El cual, si hacemos doble clic, nos aparecer\u00e1n todos los processor que tiene este  Process Group. </p>"},{"location":"nifi/nifiPractica5/#entregable","title":"ENTREGABLE","text":"<p>Hay que exportar la practica con un Processor Group que genera un .json y entregarlo en Aules con el n\u00famero de la pr\u00e1ctica y el nombreApellidos. Por ejemplo:  P5_NomAlumnoApellidos.xml</p>"},{"location":"nifi/nifiPractica6/","title":"nifiPractica6","text":""},{"location":"nifi/nifiPractica6/#funnel","title":"Funnel","text":"<ul> <li> <p>Combinar datos de varias conexiones en una sola.</p> </li> <li> <p>Vamos a revisarlo de forma r\u00e1pida, se trata de como coger varias fuentes de datos y pasarlas por un solo componente para llegar a otro componente.</p> </li> <li> <p>A\u00f1adimos dos processor \u201cGenerateFlowFile\u201d a una nueva hoja de trabajo.</p> </li> <li> <p>A\u00f1adimos un \u201cFunnel\u201d des de la barra de herramientas.</p> </li> <li> <p>A\u00f1adimos un processor \u201cLogAttribute\u201d.</p> </li> <li> <p>Enlazamos los dos processor \u201cGenerateFlowFile\u201d al \u201cFunnel\u201d.</p> </li> <li> <p>Y el \u201cFunnel\u201d lo enlazamos con el \u201dLogAttribute\u201d.</p> </li> </ul>"},{"location":"nifi/nifiPractica6/#entregable","title":"ENTREGABLE","text":"<p>Hay que exportar la practica con un Processor Group que genera un .json y entregarlo en Aules con el n\u00famero de la pr\u00e1ctica y el nombreApellidos. Por ejemplo:  P6_NomAlumnoApellidos.xml</p>"},{"location":"nifi/nifiPractica7/","title":"nifiPractica7","text":""},{"location":"nifi/nifiPractica7/#controller-services","title":"Controller Services","text":"<p>Los Controller Services son servicios compartidos que pueden ser usados con los processor o con otros controller  services.</p> <p>Vamos a realizar el siguiente caso de uso para validar esta funcionalidad.</p> <p>NOTA: Para esta pr\u00e1ctica es necesario tener una base de datos PostgreSQL disponible</p> <p>A\u00f1adimos un nuevo processor \u201cGenerateFlowFile\u201d. Lo configuramos para que genere  un fichero cada 10 segundos y en propiedades en el CustomText le ponemos lo  siguiente:</p> <pre><code>{\n\"title\": \"mr\",\n\"first\": \"John ${random():mod(10):plus(1)}\",\n\"last\": \"Doe ${random():mod(10):plus(1)}\",\n\"email\": \"johndoe${random():mod(10):plus(1)}nail.com\",\n\"created_on\": \"${now():toNumber()}\"\n}\n</code></pre> <p>A\u00f1adimos un processor \u201cLogAttribute\u201d y conectamos el FlowFileGenerator al  \u201cLogAttribute\u201d.</p> <p>Ahora vamos a ejecutar el proceso y validar que funciona correctamente viendo el  contenido del fichero generado</p> <p>A\u00f1adimos un processor \u201cPutSQL\u201d para mandar este contenido generado a una  tabla de base de datos.</p> <p>Nota: Necesitamos tener una base de datos simple para hacer esta pr\u00e1ctica ya sea en local o  remota, por ejemplo, un PostgreSQL.</p> <p>Para llegar a tener en formato SQL el contenido del JSON. Antes necesitamos  convertirlo de formato. Para ello usaremos el processor ConvertJSONToSQL.</p> <p>Vamos a configurar el processor ConvertJSONToSQL. Primero hay que configurar un  nuevo Controller Service. De la siguiente manera:         o Table Name: tbl_users         o Statment Type: INSERT         o JDBC Connection Pool: </p> <p>Aqu\u00ed es necesario crear un nuevo Controller Services seleccionando  \u201cCreate Controller Services\u201d d\u00f3nde en este caso vamos a seleccionar  \u201cDBCPConnectionPool\u201d.</p> <p>Ahora faltar\u00eda configurar las propiedades de este Controller Service que hemos creado para introducir los datos de la conexi\u00f3n a la base de  datos. Haciendo clic en la flecha que sale a la derecha.</p> <p>Hacemos clic en el bot\u00f3n de configuraci\u00f3n y cuando salga la ventana vamos a  propiedades y rellenamos las siguientes propiedades:</p> <pre><code>    o Connection URL: jdbc:postgresql://127.0.0.1:5432/postgres\n    o Driver class name: org.postgresql.Driver\n    o Driver Location: d\u00f3nde tengamos el postgresql-42.2.25.jar \n    o Database User: xxxxxxxxxxx\n    o Password: xxxxxxxxx\n</code></pre> <p>Aplicamos cambios y activamos la conexi\u00f3n</p> <p>La estructura de la tabla destino es la siguiente</p> <pre><code>CREATE TABLE tbl_users (\n    title VARCHAR(255),\n    first VARCHAR(255),\n    last VARCHAR(255),\n    email VARCHAR(255),\n    created_on VARCHAR(255)\n);\n</code></pre> <p>Volvemos al espacio de trabajo y conectamos el processor \u201cConverJSONtoSQL\u201d al  \u201cPutSQL\u201d y configuramos el tipo de relaci\u00f3n que tendr\u00e1n, en este caso \u201csql\u201d</p> <p>Pero tambi\u00e9n queremos mantener el fichero convertido, por tanto, a\u00f1adiremos un  nuevo proceso \u201cLogAttribute\u201d que conectaremos el \u201cConvertJSONtoSQL\u201d a este y  seleccionaremos la relaci\u00f3n \u201coriginal\u201d. Esto nos valdr\u00e1 de traza. Tambi\u00e9n en el  processor \u201cConverJSONToSQL\u201d es necesario configurar la relaci\u00f3n que cuando falle  termine en ese punto.</p> <p>Y ejecutamos para ver qu\u00e9 es lo que hace en las dos colas. En una podemos ver el  fichero original y en otra la traducci\u00f3n a una sentencia INSERT sql. Como pod\u00e9is ver es  necesario especificar los valores del insert a trav\u00e9s de los atributos, como pod\u00e9is ver  en las siguientes im\u00e1genes ya que los valores del FlowFile generado (JSON) los deja en  los atributos para que puedan ser usados</p> <p>Nos queda ir al processor \u201cPutSQL\u201d y configurar que tenga bien puesto el jdbc  (controller services) que hemos creado anteriormente. Unir el processor \u201cPutSQL\u201d con el \u201cLogAttribute\u201d cuando tenga \u00e9xito. Sobre el mismo connector \u201cPutSQL\u201d establecer una relaci\u00f3n de que vuelva a intentarlo  si falla \u201cRetry\u201d.</p> <p>Activamos todo el workflow y validamos en las colas que todo funciona. Como \u00faltima  comprobaci\u00f3n, revisamos la tabla de la base de datos y veamos que los datos se est\u00e9n insertando</p>"},{"location":"nifi/nifiPractica7/#driver-jdbc-postgresql","title":"Driver JDBC PostgreSQL","text":"<ul> <li>Descargar postgresql-42.7.4</li> </ul>"},{"location":"nifi/nifiPractica7/#entregable","title":"ENTREGABLE","text":"<p>Hay que exportar la practica con un Processor Group que genera un .json y entregarlo en Aules con el n\u00famero de la pr\u00e1ctica y el nombreApellidos. Por ejemplo:  P7_NomAlumnoApellidos.xml</p>"},{"location":"nifi/nifiPractica8/","title":"nifiPractica8","text":""},{"location":"nifi/nifiPractica8/#variables","title":"Variables","text":"<p>Las variables pueden ser usadas de dos formas: En la ventana de la interfa de NiFi En un fichero de configuraci\u00f3n el cual referimos a trav\u00e9s de \u201cnifi.propierties\u201d. Vamos a crear un fichero de configuraci\u00f3n llamado \u201cdb.propierties\u201d y a\u00f1adiremos  todas las propiedades que en la practica 7 hemos usado para la configuraci\u00f3n del  Controller Service con la base de datos.</p> <pre><code>    o postgres.uri=jdbc:postgresql://127.0.0.1:5432/postgres\n    o postgres.driverclassname=org.postgresql.Driver\n    o postgres.driverpath= D:/NiFi/Practicas/practica7/postgresql-42.2.25.jar\n    o postgres.username=postgres\n    o postgres.password=nifi\n</code></pre> <p>Ahora buscamos el fichero de configuraci\u00f3n de NiFi llamado \u201cnifi.properties\u201d en el  directorio \u201cconf\u201d y lo editamos. B\u00fascamos \u201cregistry\u201d y configuramos la variable que estar\u00e1 vac\u00eda en la ubicaci\u00f3n del fichero que hemos creado \u201cdb.properties\u201d que lo  vamos a ubicar en el mismo directorio conf.</p> <p>Nota: Si tenemos m\u00e1s de un fichero de configuraci\u00f3n, lo pondr\u00edamos separado por \u201c,\u201d.</p> <p>Ahora para aplicar los cambios, es necesario reiniciar la instancia de NiFi. Control + c para cerrar la ventana de lanzamiento del proceso. Si se trata de sistema operativo </p> <pre><code>Linux /nifi.sh stop , /nifi.sh start\n</code></pre> <p>Una vez hemos vuelto a arrancar NiFi, nos vamos al processor ConvertJSONtoSQL, a  propiedades y vamos con la flecha a la configuraci\u00f3n del JDBC Connection Pool d\u00f3nde  debemos deshabilitar el Controller Service</p> <p>Ya deshabilitado, podemos ir a la configuraci\u00f3n del Controller Service y vamos a usar  Expression Language en las propiedades para especificar los par\u00e1metros definidos en  el fichero que hemos cargado en el arranque de NiFi. Son los siguientes:</p> <pre><code>    o Database Connection URL: ${postgres.uri}\n    o Database Driver Class Name: ${postgres.driverclassname}\n    o Database Driver Location: ${postgres.driverpath}\n    o Database User: ${postgres.username}\n    o Database Password: ${postgres.password}\n</code></pre> <p>Aplicamos los cambios y volvemos a activar el Contoller Service para validar que  funciona todo correctamente.</p> <p>Ejecutamos y comprobamos que todos los pasos funcionan seg\u00fan lo esperado y que  llegamos a insertar en la base de datos, por ejemplo, revisando la fecha de creaci\u00f3n. C\u00f3mo hemos comentado hay dos opciones de utilizar variables. Ya hemos visto la primera que es utilizando ficheros de propiedades que se cargan cuando arranca NiFi. Ahora vamos a ver la otra opci\u00f3n.</p> <p>B\u00e1sicamente se hace en la hoja de trabajo, con bot\u00f3n derecho aparece la opci\u00f3n  \u201cVariables\u201d. Hacemos clic y nos aparece una ventana para definirlas. Estas variables  son definidas directamente.</p> <p>Se referencian de la misma manera que las que se cargan con NiFi. </p> <p>Nota: Es m\u00e1s ventajoso usar las variables definidas directamente ya que no necesitan  reiniciar NiFi, adem\u00e1s que ayuda a corregir posibles errores de manera r\u00e1pida y tambi\u00e9n evitarlos.</p>"},{"location":"nifi/nifiPractica8/#entregable","title":"ENTREGABLE","text":"<p>Hay que exportar la practica con un Processor Group que genera un .json y entregarlo en Aules con el n\u00famero de la pr\u00e1ctica y el nombreApellidos. Por ejemplo:  P8_NomAlumnoApellidos.xml</p>"},{"location":"nifi/nifiPracticas_full/","title":"NiFi Pr\u00e1cticas","text":""},{"location":"nifi/nifiPracticas_full/#practica-1-mover-un-fichero-de-un-origen-a-un-destino","title":"Pr\u00e1ctica 1: Mover un fichero de un origen a un destino","text":"<p>Iniciamos NiFi. Ejecutamos:</p> <pre><code>    ./bin/nifi.sh start (Unix)\n    ./bin/nifi.bat start (MSDOS)\n</code></pre> <p>Accedemos a la siguiente url que se levantar\u00e1 en el ordenador (o la que indica el  Readme, seg\u00fan la versi\u00f3n funcionara una u otra): </p> <p>https://localhost:8443/nifi/</p> <p> </p> <p>Pedir\u00e1 unas credenciales. Podemos utilizar las creadas de manera autom\u00e1tica  al arrancar por primera vez en el fichero de logs que podr\u00e9is encontrar en: logs/nifiapp.log. Lo encontrareis buscando la siguiente palabra clave: Username</p> <p>Una vez hayamos hecho login, nos aparecer\u00e1 la interfaz de trabajo.</p> <p> </p> <p>En la barra de iconos de herramientas, hacemos clic en \u201cProcessor\u201d y arrastramos  hacia el \u00e1rea de trabajo</p> <p> </p> <p>En la ventana de \u201cAdd Processor\u201d b\u00fascamos getFile. Y hacemos clic en \u201cAdd\u201d.</p> <p> </p> <p>Repetimos el mismo proceso para a\u00f1adir el processor \u201cputFile\u201d. Quedando algo similar  a la siguiente imagen.</p> <p> </p> <p>Ahora, vamos a configurar de d\u00f3nde vamos a coger el fichero origen y d\u00f3nde lo vamos  a dejar en el destino. </p> <p>Hacemos clic bot\u00f3n derecho en \u201cGetFile\u201d en \u201cConfigure\u201d y vamos a la pesta\u00f1a  \u201cProperties\u201d. D\u00f3nde podemos ver los diferentes aspectos que son configurables.  Ahora mismo s\u00f3lo vamos a indicar el directorio de entrada \u201cInput Directory\u201d.</p> <p> </p> <p>Hacemos clic bot\u00f3n derecho en \u201cPutFile\u201d en \u201cConfigure\u201d y vamos a la pesta\u00f1a  \u201cProperties\u201d. D\u00f3nde podemos ver los diferentes aspectos que son configurables.  Ahora mismo s\u00f3lo vamos a indicar el directorio de entrada \u201cDirectory\u201d.</p> <p> </p> <p>Ya configurado el componente del origen de los ficheros y el destino, s\u00f3lo nos queda  unirlos. Si hacemos clic sobre el componente \u201cGetFile\u201d veremos una flecha que nos  permitir\u00e1 arrastrarla hasta el otro componente \u201cPutFile\u201d. Quedando como la siguiente  imagen:</p> <p> </p> <p>Ahora nos queda probar que traslada correctamente los ficheros, arrancando los dos  componentes con bot\u00f3n derecho y hacer clic en \u201cStart\u201d. En los dos componentes.  Primero el \u201cGetFile\u201d y luego el \u201cPutFile\u201d.</p> <p> </p> <p>Si al realizar la acci\u00f3n no sale la opci\u00f3n en el \u201cPutFile\u201d lo que hay que hacer es entrar  en este componente en propiedades e indicarle que acci\u00f3n debe hacer en el caso de  que falle</p> <p> </p> <p>Una vez todo configurado y en estado \u201cStart\u201d ver\u00e9is que empieza a mover el fichero  del origen configurado al destino. Marcado con verde como que se ha ejecutado  correctamente</p> <p> </p> <p>ENTREGABLE: Hay que exportar el template del proyecto y entregarlo en Aules con el n\u00famero de la pr\u00e1ctica y el nombreApellidos. Por ejemplo:  P1_NomAlumnoApellidos.xml</p>"},{"location":"nifi/nifiPracticas_full/#practica-2-atributos-y-contenido","title":"Pr\u00e1ctica 2: Atributos y contenido","text":"<p>Generaremos primero un fichero aleatorio o con contenido personalizado, le aplicaremos  alguna transformaci\u00f3n y luego alteraremos en este sus propiedades. Tambi\u00e9n veremos por los  diferentes estados que pasa la ejecuci\u00f3n y c\u00f3mo ver en cada punto como va evolucionando el  proceso.</p> <p>Una vez hayamos hecho login, nos aparecer\u00e1 la interfaz de trabajo</p> <p> </p> <p>En la barra de iconos de herramientas, hacemos clic en \u201cProcessor\u201d y arrastramos  hacia el \u00e1rea de trabajo.</p> <p> </p> <p>En la ventana de \u201cAdd Processor\u201d b\u00fascamos \u201cGenerateFlowFile\u201d. Y hacemos clic en  \u201cAdd\u201d.</p> <p>Nota: Este processor crea FlowFiles con datos aleatorios o contenido personalizado</p> <p> </p> <p>Ahora, vamos a configurarlo con datos aleatorios conforme la siguiente imagen. Y  planificarlo.</p> <p> </p> <p> </p> <p>Ahora vamos a a\u00f1adir a la hoja de trabajo, otro processor llamado \u201cReplace Text  Processor\u201d</p> <p>Nota: bajo cada processor encontraremos una breve descripci\u00f3n de la funcionalidad</p> <p> </p> <p>Establecemos la conexi\u00f3n entre los dos processor antes de configurar el ReplaceText.</p> <p> </p> <p>Le dedicamos unos minutos a entender el processor \u201cReplaceText\u201d que propiedades  nos aporta.  Nota: Es recomendable revisar siempre antes de usar las propiedades disponibles y si  es necesario, realizar alguna peque\u00f1a prueba.</p> <p> </p> <p>Queremos remplazar en el contenido del fichero generado por unos valores separados  por comas. Y guardamos</p> <p> </p> <p>Para visualizar que es lo que pasa, vamos a a\u00f1adir un processor \u201clog attribute\u201d y  conectarlo sobre el processor \u201cReplaceText\u201d. En esta conexi\u00f3n, debemos activar la  casilla \u201csuccess\u201d.</p> <p> </p> <p>Vamos a probar la generaci\u00f3n del fichero y transformaci\u00f3n de este. Le damos a \u201cStart\u201d  con bot\u00f3n derecho sobre el processor \u201cGenerateFlowFile\u201d. Veremos que genera un  fichero en la cola. Una vez lo genere, vamos a pararlo de la misma manera con la  acci\u00f3n \u201cStop\u201d. Ya que, si recordamos, lo hemos planificado para que se ejecute cada 5  segundos. Si no, estar\u00e1 generando cada 5 segundos un nuevo fichero.</p> <p> </p> <p>En la cola, podemos validar el contenido generado, con bot\u00f3n derecho, \u201cList queue\u201d. Si  damos al primer bot\u00f3n (columna), podremos ver/descargar el fichero generado.</p> <p> </p> <p>Si el processor os pone una alerta y no os deja arrancarlo. Es debido a que necesita  que especifiquemos que hacer cuando falle. Cuando tenga \u00e9xito ira al processor  conectado, pero cuando falle, que acci\u00f3n debe hacer. Vamos a propiedades, a  relaciones y ah\u00ed lo podemos configurar</p> <p> </p> <p>Ahora ya le podemos dar la acci\u00f3n \u201cStart\u201d y ver que hace el \u201cReplaceText\u201d que le  hemos indicado al fichero de la cola. Podemos ir a la cola y validar que los ficheros se  han convertido conforme lo esperado.</p> <p> </p> <p> </p> <p>Ahora vamos a a\u00f1adir el processor \u201cExtractText\u201d y en propiedades de este  configuramos una nueva propiedad \u201ccsv\u201d con una expresi\u00f3n regular \u201c(.+),(.+),(.+),(.+)\u201d  de la siguiente manera:</p> <p> </p> <p>Ahora conectamos el \u201cReplaceText\u201d con \u201cExtractText\u201d eliminando la anterior relaci\u00f3n  con \u201cLogAttribute\u201d. Y luego, conectamos \u201cExtractText\u201d con \u201cLogAttribute\u201d. En la  definici\u00f3n de la relaci\u00f3n, se ha de marcar el check \u201cmatched\u201d. Como veis, el processor  ExtractText tiene una alerta amarilla, quiere decir que no est\u00e1 del todo bien  configurado. Pasa lo mismo que antes, es necesario a\u00f1adir cuando la acci\u00f3n contraria a  \u201cmatch\u201d que tiene que hacer.  Vamos a propiedades de \u201cExtractText\u201d, a \u201cRelationships\u201d y marcamos la acci\u00f3n  \u201cterminate\u201d.</p> <p> </p> <p> </p> <p>Llegado a este punto, le damos a la acci\u00f3n \u201cstart\u201d del processor \u201cExtractText\u201d y vemos  como procesa el fichero con una salida en la cola. Podemos ver el resultado en la cola  de nuevo como ha generado nuevos atributos seg\u00fan la expresi\u00f3n regular que le hemos  aplicado, ya que el contenido en este caso no se ha visto modificado.</p> <p> </p> <p>ENTREGABLE: Hay que exportar el template del proyecto y entregarlo en Aules con el n\u00famero de la pr\u00e1ctica y el nombreApellidos. Por ejemplo:  P2_NomAlumnoApellidos.xml</p>"},{"location":"nifi/nifiPracticas_full/#practica-3-expression-language","title":"Pr\u00e1ctica 3: Expression Language","text":"<p>Partimos del resultado de la anterior practica 2, trabajando con atributos y contenido d\u00f3nde  generaremos primero un fichero aleatorio o con contenido personalizado, le aplicaremos  alguna transformaci\u00f3n y luego alteraremos en este sus propiedades. Tambi\u00e9n veremos por los  diferentes estados que pasa la ejecuci\u00f3n y c\u00f3mo ver en cada punto como va evolucionando el  proceso.</p> <p>El objetivo de esta pr\u00e1ctica es modificar la soluci\u00f3n de la practica 2, a\u00f1adiendo un paso m\u00e1s al  final, d\u00f3nde utilizando Expression Language NiFi extraeremos informaci\u00f3n de los atributos de  FlowFile que hemos creado para introducirlas en el contenido en formato JSON. Teniendo  como resultado algo similar:</p> <pre><code>    {\n    \"field1\": \"a\",\n    \"field1\": \"b\",\n    \"field1\": \"c\",\n    \"field1\": \"d\"\n    }\n</code></pre> <p> </p> <p>A\u00f1adimos un nuevo ReplaceText mediante el bot\u00f3n de Processor o duplicamos el que  ya ten\u00edamos configurado para tener la misma configuraci\u00f3n. Y lo conectamos entre el  Processor \u201cExtractText\u201d y terminando en el \u201cLogAttribute\u201d</p> <p> </p> <p>Una vez en este punto, nos queda configurar el \u201cReplaceText\u201d a\u00f1adido o duplicado,  con la misma configuraci\u00f3n que el primero, pero con la diferencia que en la propiedad  \u201cReplacementValue\u201d introducimos con Expression Language NiFi de d\u00f3nde va a  obtener los datos y que estructura va a tener para substituir el anterior contenido con  el nuevo. El c\u00f3digo es el siguiente, d\u00f3nde referenciamos con el car\u00e1cter \u201c$\u201d que es una  propiedad del FlowFile.</p> <pre><code>    {\n    \"field1\": \"${csv.1}\",\n    \"field1\": \"${csv.2}\",\n    \"field1\": \"${csv.3}\",\n    \"field1\": \"${csv.4}\"\n    }\n</code></pre> <p> </p> <p>Ejecutamos y podemos ver en la cola el resultado del contenido, si ha funcionado  correctamente debe salir algo similar a la siguiente imagen:</p> <p> </p> <p>Nota: El Expression Language NiFi es muy \u00fatil y existen muchas funciones que es posible  utilizarlas. Para ello recomendamos visitar la siguiente documentaci\u00f3n y dedicarle unos  minutos: Apache NiFi Expression Language Guide</p> <p>Ahora vamos a continuar con el dataflow que hemos creado. Si vamos a la cola final, y vemos los ficheros sus propiedades podemos ver que existen  las propiedades de UID y Filename, que son valores \u00fanicos. Estos identifican de forma  \u00fanica el fichero dentro del dataflow y es posible utilizarlos para referenciar el FlowFile. Por ejemplo, vamos a a\u00f1adir un Processor \u201cPutFile\u201d y lo conectaremos entre el \u00faltimo  replaceText y el log. Luego entraremos en las propiedades del PutFile y  configuraremos el directorio d\u00f3nde quedamos que se guarde el fichero.</p> <p> </p> <p> </p> <p>Arrancamos todos los procesos para generar un fichero y que llegue hasta la cola del  log y veamos que pasa en el directorio. Confirmamos que el nombre del fichero lo coge  de la propiedad de Filename del FlowFile.</p> <p> </p> <p>Vamos a a\u00f1adir un Processor para actualizar el atributo y poner un nombre a los  ficheros de salida (en la propiedad). El Processor a a\u00f1adir es \u201cUpdateAttribute\u201d. Y  vamos a propiedades y a\u00f1adimos una nueva \u201cFilename\u201d. Y introducimos el valor  siguiente:</p> <pre><code>    ${filename}-${now():toNumber():format('dd-MM-yy')}.json\n</code></pre> <p> </p> <p>Ya solo quedar\u00eda conectar el Processor antes del PutFile para que le d\u00e9 tiempo a  cambiarle la propiedad. Y volvamos a revisar la salida a ver qu\u00e9 pasa.</p> <p> </p> <p>Hagamos como ejercicio, que el fichero de salida salga con el siguiente formato d\u00f3nde  lo siguiente a la fecha son horas, minutos y segundos:</p> <p></p> <p>ENTREGABLE: Hay que exportar la practica con un Processor Group que genera un .json y entregarlo en Aules con el n\u00famero de la pr\u00e1ctica y el nombreApellidos. Por ejemplo:  P3_NomAlumnoApellidos.xml</p>"},{"location":"nifi/nifiPracticas_full/#practica-4-process-group-input-output-port","title":"Pr\u00e1ctica 4: Process group, Input-Output port","text":"<p>En NiFi uno o m\u00e1s processor son conectados en un process group. Podemos a\u00f1adir un  process group arrastrando el icono de la barra de herramientas . Partiendo de la practica 3.  Cogemos y a\u00f1adimos un nuevo process group y le ponemos como nombre \u201cCSV to JSON\u201d (es posible obviar este paso, si ya se ha puesto la anterior practica en un process group, duplicandolo y renombrandolo a CSV to JSON) Para decirle como usar los processor\u2019s, debemos seleccionar todos los processor (con  la tecla shift) y arrastrarlos sobre el process group</p> <p> </p> <p>Para ver los componentes de un Process Group, hacemos doble click y accedemos al  detalle. </p> <p> </p> <p>Para salir fuera del detalle, con bot\u00f3n derecho \u201cleave group\u201d saldremos de nuevo  fuera.</p> <p>Nota: Es recomendable utilizar los Process Group para l\u00f3gicas complejas. Es posible duplicar un  process group heredando la misma configuraci\u00f3n.</p> <p> </p> <p>A veces, un Process Group puede generar una salida para utilizar en otros procesos.  Vamos a transferir informaci\u00f3n des de un Process Group a otro. Para ello vamos a  utilizar de la barra de herramientas el \u201cinput\u201d y el \u201coutput\u201d port . El actual flujo de datos, lo vamos a partir en dos Process Group de la siguiente manera: Creamos un nuevo Process Group que se llame \u201cWrite JSON to File System\u201d d\u00f3nde  vamos a introduir los processors des de el \u201cUpdateAttribute\u201d hasta el \u00faltimo  \u201cLogAttribute\u201d.</p> <p> </p> <p>C\u00f3mo ya tenemos un process group principal \u201cCSV to JSON\u201d, vamos a sacar el  process group \u201cWrite JSON to File System\u201d fuera para poder ejecutarlos  conjuntamente. Para ello hacemos bot\u00f3n derecho sobre \u201cWrite JSON to File  System\u201d y seleccionamos \u201cMove to parent group\u201d.</p> <p> </p> <p>Ahora el problema est\u00e1 en que los dos processor group es necesario que se  comuniquen para que funcionen. Para ello haremos uso del Input/Output port. Accedemos al process group \u201cCSV to JSON\u201d y a\u00f1adimos un \u201cOutput port\u201d.</p> <p> </p> <p>Salimos del grupo y accedemos ahora al Process Group \u201cWrite JSON to CSV\u201d y  a\u00f1adimos un \u201cInput port\u201d al inicio</p> <p> </p> <p>Salimos al flujo principal, d\u00f3nde ahora vamos a conectar los dos Process Group. Saldr\u00e1  el mapeo de entradas y salidas, hay que tenerlo en cuenta cuando tengamos m\u00e1s de  una entrada/salida que este bien asociado. Ahora solo queda arrancar con bot\u00f3n derecho \u201cstart\u201d y validar que todo funciona  correctamente.</p> <p>Nota: Dentro de un mismo Process Group no es posible conectar un \u201cOutput port\u201d a un \u201cInput  port\u201d.</p> <p>ENTREGABLE: Hay que exportar la practica con un Processor Group que genera un .json y entregarlo en Aules con el n\u00famero de la pr\u00e1ctica y el nombreApellidos. Por ejemplo:  P4_NomAlumnoApellidos.xml</p>"},{"location":"nifi/nifiPracticas_full/#practica-5-templates","title":"Pr\u00e1ctica 5: Templates","text":"<pre><code>            *Esta pr\u00e1ctica queda descartada. Dado que en la versi\u00f3n M2 NiFi ya no se trabaja con los Templates si no con el Registry, exportando los trabajos a trav\u00e9s de un Processor Group, pero si en versiones anteriores. Tenedlo en cuenta dado que si trabajais en versiones anteriores es la forma de guardar y exportar el trabajo realizado.*\n</code></pre> <p>Para guardar el trabajo realizado, exportarlo, etc usaremos los \u201cTemplates\u201d. Para ello debemos seleccionar todos los componentes a guardar en el template, y  buscar el icono crear template que encontrareis en la izquierda.</p> <p> </p> <p>Le pondremos por ejemplo \u201cCSV to JSON\u201d y lo creamos.</p> <p> </p> <p>Hacemos click en \u201cCreate\u201d.</p> <p> </p> <p>Este template se almacena en la instancia local. Si queremos ver todos los templates que tenemos creados en nuestra instancia local,  tenemos que ir al bot\u00f3n de la derecha arriba y hacer clic en \u201cTemplates\u201d d\u00f3nde  saldr\u00e1 un listado. D\u00f3nde a la derecha aparece los botones para descargarlos para  una futura importaci\u00f3n o copia de seguridad.</p> <p> </p> <p>Nota: Existen templates de ejemplo que podemos descargar e importar a nuestra instancia local para practicar o utilizar para un desarrollo. Lo pod\u00e9is encontrar en el siguiente enlace:  https://cwiki.apache.org/confluence/display/nifi/example+dataflow+templates</p> <p>Vamos a acceder a la anterior p\u00e1gina web y descargar un template. Por ejemplo, el  \u201cRetry Count Loop\u201d. Nos quedar\u00eda ahora importarlo a nuestro entorno de desarrollo. Accedemos al bot\u00f3n  \u201cUpload Template\u201d y seleccionamos el fichero descargado \u201cRetry_Count_Loop.xml\u201d a  trav\u00e9s del icono . Ahora para utilizarlo o revisar, s\u00f3lo debemos ir a la barra de herramientas y  seleccionar \u201cAdd Template\u201d y arrastrar al espacio de trabajo. Seleccionamos el  template descargado y ver\u00e9is que aparece un nuevo Process Group.</p> <p> </p> <p>El cual, si hacemos doble clic, nos aparecer\u00e1n todos los processor que tiene este  Process Group. </p> <p>ENTREGABLE: Hay que exportar la practica con un Processor Group que genera un .json y entregarlo en Aules con el n\u00famero de la pr\u00e1ctica y el nombreApellidos. Por ejemplo:  P5_NomAlumnoApellidos.xml</p> <p> </p>"},{"location":"nifi/nifiPracticas_full/#practica-6-funnel","title":"Pr\u00e1ctica 6: Funnel","text":"<p>Combinar datos de varias conexiones en una sola Vamos a revisarlo de forma r\u00e1pida, se trata de como coger varias fuentes de datos y  pasarlas por un solo componente para llegar a otro componente. A\u00f1adimos dos processor \u201cGenerateFlowFile\u201d a una nueva hoja de trabajo. A\u00f1adimos un \u201cFunnel\u201d des de la barra de herramientas A\u00f1adimos un processor \u201cLogAttribute\u201d Enlazamos los dos processor \u201cGenerateFlowFile\u201d al \u201cFunnel\u201d Y el \u201cFunnel\u201d lo enlazamos con el \u201dLogAttribute\u201d</p> <p> </p> <p>ENTREGABLE: Hay que exportar la practica con un Processor Group que genera un .json y entregarlo en Aules con el n\u00famero de la pr\u00e1ctica y el nombreApellidos. Por ejemplo:  P6_NomAlumnoApellidos.xml</p>"},{"location":"nifi/nifiPracticas_full/#practica-7-controller-services","title":"Pr\u00e1ctica 7: Controller Services","text":"<p>Los Controller Services son servicios compartidos que pueden ser usados con los processor o con otros controller  services.</p> <p>Vamos a realizar el siguiente caso de uso para validar esta funcionalidad.</p> <p>A\u00f1adimos un nuevo processor \u201cFlowFileGenerator\u201d. Lo configuramos para que genere  un fichero cada 10 segundos y en propiedades en el CustomText le ponemos lo  siguiente:</p> <pre><code>    {\n    \"title\": \"mr\",\n    \"first\": \"John ${random():mod(10):plus(1)}\",\n    \"last\": \"Doe ${random():mod(10):plus(1)}\",\n    \"email\": \"johndoe${random():mod(10):plus(1)}nail.com\",\n    \"created_on\": \"${now():toNumber()}\"\n    }\n</code></pre> <p> </p> <p> </p> <p>A\u00f1adimos un processor \u201cLogAttribute\u201d y conectamos el \u201cFlowFileGenerator\u201d al  \u201cLogAttribute\u201d.</p> <p> </p> <p>Ahora vamos a ejecutar el proceso y validar que funciona correctamente viendo el  contenido del fichero generado</p> <p> </p> <p>A\u00f1adimos un processor \u201cPutSQL\u201d para mandar este contenido generado a una  tabla de base de datos.</p> <p>Nota: Necesitamos tener una base de datos simple para hacer esta pr\u00e1ctica ya sea en local o  remota, por ejemplo, un PostgreSQL.</p> <p>Para llegar a tener en formato SQL el contenido del JSON. Antes necesitamos  convertirlo de formato. Para ello usaremos el processor \u201cConvertJSONToSQL\u201d.</p> <p> </p> <p>Vamos a configurar el processor \u201cConvertJSONToSQL\u201d. Primero hay que configurar un  nuevo \u201cController Service\u201d. De la siguiente manera:         o Table Name: tbl_users         o Statment Type: INSERT         o JDBC Connection Pool: </p> <p>Aqu\u00ed es necesario crear un nuevo \u201cController Services\u201d seleccionando  \u201cCreate Controller Services\u201d d\u00f3nde en este caso vamos a seleccionar  \u201cDBCPConnectionPool\u201d.</p> <p> </p> <p>Ahora faltar\u00eda configurar las propiedades de este \u201cController Service\u201d  que hemos creado para introducir los datos de la conexi\u00f3n a la base de  datos. Haciendo clic en la flecha que sale a la derecha.</p> <p> </p> <p> </p> <p>Hacemos clic en el bot\u00f3n de configuraci\u00f3n y cuando salga la ventana vamos a  propiedades y rellenamos las siguientes:</p> <pre><code>    o Connection URL: jdbc:postgresql://127.0.0.1:5432/postgres\n    o Driver class name: org.postgresql.Driver\n    o Driver Location: d\u00f3nde tengamos el postgresql-42.2.25.jar \n    o Database User: xxxxxxxxxxx\n    o Password: xxxxxxxxx\n</code></pre> <p> </p> <p>Aplicamos cambios y activamos la conexi\u00f3n</p> <p> </p> <p> </p> <p>La estructura de la tabla destino es la siguiente</p> <p> </p> <p>Volvemos al espacio de trabajo y conectamos el processor \u201cConverJSONtoSQL\u201d al  \u201cPutSQL\u201d y configuramos el tipo de relaci\u00f3n que tendr\u00e1n, en este caso \u201csql\u201d</p> <p> </p> <p>Pero tambi\u00e9n queremos mantener el fichero convertido, por tanto, a\u00f1adiremos un  nuevo proceso \u201cLogAttribute\u201d que conectaremos el \u201cConvertJSONtoSQL\u201d a este y  seleccionaremos la relaci\u00f3n \u201coriginal\u201d. Esto nos valdr\u00e1 de traza. Tambi\u00e9n en el  processor \u201cConverJSONToSQL\u201d es necesario configurar la relaci\u00f3n que cuando falle  termine en ese punto.</p> <p> </p> <p> </p> <p>Y ejecutamos para ver qu\u00e9 es lo que hace en las dos colas. En una podemos ver el  fichero original y en otra la traducci\u00f3n a una sentencia INSERT sql. Como pod\u00e9is ver es  necesario especificar los valores del insert a trav\u00e9s de los atributos, como pod\u00e9is ver  en las siguientes im\u00e1genes ya que los valores del FlowFile generado (JSON) los deja en  los atributos para que puedan ser usados</p> <p> </p> <p> </p> <p>Nos queda ir al processor \u201cPutSQL\u201d y configurar que tenga bien puesto el jdbc  (controller services) que hemos creado anteriormente. Unir el processor \u201cPutSQL\u201d con el \u201cLogAttribute\u201d cuando tenga \u00e9xito. Sobre el mismo connector \u201cPutSQL\u201d establecer una relaci\u00f3n de que vuelva a intentarlo  si falla \u201cRetry\u201d.</p> <p> </p> <p>Activamos todo el workflow y validamos en las colas que todo funciona. Como \u00faltima  comprobaci\u00f3n, revisamos la tabla de la base de datos y veamos que los datos se est\u00e9n insertando</p> <p> </p> <p>ENTREGABLE: Hay que exportar el template del proyecto y entregarlo en Aules con el n\u00famero de la pr\u00e1ctica y el nombreApellidos. Por ejemplo:  P7_NomAlumnoApellidos.xml</p>"},{"location":"nifi/nifiPracticas_full/#practica-8-variables","title":"Pr\u00e1ctica 8: Variables","text":"<p>Las variables pueden ser usadas de dos formas: En la ventana de la interfa de NiFi En un fichero de configuraci\u00f3n el cual referimos a trav\u00e9s de \u201cnifi.propierties\u201d. Vamos a crear un fichero de configuraci\u00f3n llamado \u201cdb.propierties\u201d y a\u00f1adiremos  todas las propiedades que en la practica 7 hemos usado para la configuraci\u00f3n del  Controller Service con la base de datos.</p> <pre><code>    o postgres.uri=jdbc:postgresql://127.0.0.1:5432/postgres\n    o postgres.driverclassname=org.postgresql.Driver\n    o postgres.driverpath= D:/NiFi/Practicas/practica7/postgresql-42.2.25.jar\n    o postgres.username=postgres\n    o postgres.password=nifi\n</code></pre> <p> </p> <p>Ahora buscamos el fichero de configuraci\u00f3n de NiFi llamado \u201cnifi.properties\u201d en el  directorio \u201cconf\u201d y lo editamos. B\u00fascamos \u201cregistry\u201d y configuramos la variable que  estar\u00e1 vac\u00eda en la ubicaci\u00f3n del fichero que hemos creado \u201cdb.properties\u201d que lo  vamos a ubicar en el mismo directorio conf.</p> <p> </p> <p>Nota: Si tenemos m\u00e1s de un fichero de configuraci\u00f3n, lo pondr\u00edamos separado por \u201c,\u201d.</p> <p>Ahora para aplicar los cambios, es necesario reiniciar la instancia de NiFi. Control + c  para cerrar la ventana de lanzamiento del proceso. Si se trata de sistema operativo  Linux /nifi.sh stop , /nifi.sh start</p> <p> </p> <p>Una vez hemos vuelto a arrancar NiFi, nos vamos al processor ConvertJSONtoSQL, a  propiedades y vamos con la flecha a la configuraci\u00f3n del JDBC Connection Pool d\u00f3nde  debemos deshabilitar el Controller Service</p> <p> </p> <p>Ya deshabilitado, podemos ir a la configuraci\u00f3n del Controller Service y vamos a usar  Expression Language en las propiedades para especificar los par\u00e1metros definidos en  el fichero que hemos cargado en el arranque de NiFi. Son los siguientes:</p> <pre><code>    o Database Connection URL: ${postgres.uri}\n    o Database Driver Class Name: ${postgres.driverclassname}\n    o Database Driver Location: ${postgres.driverpath}\n    o Database User: ${postgres.username}\n    o Database Password: ${postgres.password}\n</code></pre> <p>Aplicamos los cambios y volvemos a activar el Contoller Service para validar que  funciona todo correctamente.</p> <p> </p> <p>Ejecutamos y comprobamos que todos los pasos funcionan seg\u00fan lo esperado y que  llegamos a insertar en la base de datos, por ejemplo, revisando la fecha de creaci\u00f3n. C\u00f3mo hemos comentado hay dos opciones de utilizar variables. Ya hemos visto la primera que  es utilizando ficheros de propiedades que se cargan cuando arranca NiFi. Ahora vamos a ver la  otra opci\u00f3n.</p> <p> </p> <p>B\u00e1sicamente se hace en la hoja de trabajo, con bot\u00f3n derecho aparece la opci\u00f3n  \u201cVariables\u201d. Hacemos clic y nos aparece una ventana para definirlas. Estas variables  son definidas directamente.</p> <p></p> <p>Se referencian de la misma manera que las que se cargan con NiFi. </p> <p>Nota: Es m\u00e1s ventajoso usar las variables definidas directamente ya que no necesitan  reiniciar NiFi, adem\u00e1s que ayuda a corregir posibles errores de manera r\u00e1pida y tambi\u00e9n  evitarlos.</p> <p>ENTREGABLE: Hay que exportar el template del proyecto y entregarlo en Aules con el n\u00famero de la pr\u00e1ctica y el nombreApellidos. Por ejemplo:  P8_NomAlumnoApellidos.xml</p>"},{"location":"nifi/nifiPracticas_full/#caso-de-uso-con-api","title":"Caso de uso con API","text":"<p>Consulta de una API con NiFi</p> <p>En la pr\u00e1ctica anterior sobre la API del INE, consultamos la encuesta de poblaci\u00f3n activa. Vamos a continuar con esta pr\u00e1ctica leyendo los datos con NiFi.</p> <p>Recordemos que es necesario averiguar el c\u00f3digo para realizar esta consulta, que nos devolver\u00e1 un JSON:  </p> <p>http://servicios.ine.es/wstempus/js/ES/DATOS_SERIE/?nult= <p>Ejercicio:</p> <ol> <li> <p>Crea un pipeline que tome el JSON y lo guarde en un archivo en el sistema.</p> </li> <li> <p>Haz que el pipeline aplane el JSON y lo guarde como un archivo CSV.</p> </li> <li> <p>Configura el pipeline para tomar el JSON aplanado y guardarlo en una base de datos PostgreSQL.</p> </li> <li> <p>Guarda este pipeline como una plantilla (template) para utilizarlo en futuros proyectos.</p> </li> </ol>"},{"location":"nifi/nifiPracticas_full/#caso-de-uso-con-postgresql","title":"Caso de uso con PostgreSQL","text":"<p>Dadas las intensas temperaturas que se est\u00e1n dando estos \u00faltimos meses de 2022,  necesitamos analizar y comparar con los a\u00f1os 2020 y 2021 de manera mensual la temperatura  media y la acumulaci\u00f3n de lluvia. Para ello vamos a necesitar obtener como fuentes de  informaci\u00f3n del INE la relaci\u00f3n de provincias y municipios de Espa\u00f1a, del AEMET la relaci\u00f3n de  datos meteorol\u00f3gicos de todas las estaciones de Espa\u00f1a y generar un cat\u00e1logo de fechas. Los datos que obtenemos de las diferentes fuentes de datos, los vamos a volcar en una base  de datos \u2018Definir qu\u00e9 base de datos, p.e. PostgreSQL\u2019 en una capa de tablas Staging, para  descargar los sistemas origen. </p> <p>Luego realizaremos los tratamientos necesarios y los volcaremos  a las capas finales en la base de datos, a los cat\u00e1logos/dimensiones y estrella (ods). Ya preparado el modelo multidimensional, tenemos dos opciones seg\u00fan tiempo: generar un agregado sobre la capa ods con c\u00e1lculos (kpi\u2019s) ya hechos para agilizar la lectura de la  aplicaci\u00f3n de reporting o directamente pasar a la herramienta de reporting a  pintar/analizar/calcular kpis. Sacar conclusiones a trav\u00e9s de la herramienta de reporting con tablas, informes y gr\u00e1ficos.</p> <p>Anexo Fechas A\u00f1adir fuente/consulta sql para a\u00f1adir datos del cat\u00e1logo fecha.</p> <p>INE: Relaci\u00f3n Provincia-Municipio</p> <p>AEMET Es necesario pedir una Api Key https://opendata.aemet.es/centrodedescargas/docs/FAQs170621.pdf</p> <p>https://opendata.aemet.es/centrodedescargas/inicio</p> <p>https://opendata.aemet.es/dist/index.html?#!/valoresclimatologicos/Climatolog%C3%ADas_diarias</p> <p>Herramientas para desarrolladores  Servicio web datos diarios estaci\u00f3n meteorol\u00f3gica de todas las estaciones, entre ellas, Xativa.</p> <p> </p> <p>Nos devuelve un JSON.</p>"},{"location":"nifi/nifiPracticas_full/#caso-de-uso-con-mongodb","title":"Caso de uso con MongoDB","text":"<p>Por definir</p>"},{"location":"nifi/nifiPracticas_full/#caso-de-uso-con-hadoop","title":"Caso de uso con Hadoop","text":"<p>Traspaso de archivos FS y volcado a HDFS</p> <p>Debemos transferir una gran cantidad de archivos del sistema de la m\u00e1quina virtual Linux al sistema de archivos de Hadoop HDFS, tambi\u00e9n localizado en la misma m\u00e1quina virtual. Para evitar generar un script con muchas instrucciones como <code>hdfs dfs -put ...</code>, lo cual podr\u00eda ser tedioso, implementaremos un proceso en NiFi que automatice esta tarea. Una vez que los archivos est\u00e9n en el sistema HDFS, ser\u00e1 necesario implementar un proceso para devolver estos archivos desde el sistema de archivos HDFS al sistema de archivos local. Este proceso permitir\u00e1 automatizar esta tarea y realizarla peri\u00f3dicamente.</p> <p>Consideraciones:</p> <ul> <li> <p>NiFi debe ubicarse en el directorio <code>/opt</code>, al igual que Hadoop. Como usuario root, copiarlo. Eliminar el archivo zip de NiFi para liberar espacio en el disco duro de la m\u00e1quina virtual.</p> <pre><code>[root@nodo1 opt]# cp -R /home/hadoop/Descargas/nifi-1.23.2 nifi\n</code></pre> </li> <li> <p>Asegurar que el directorio de NiFi (<code>/opt/nifi</code>) tenga permisos de ejecuci\u00f3n para que NiFi y Hadoop puedan interactuar.</p> <pre><code>[root@nodo1 opt]# chown -R hadoop:hadoop nifi\n[root@nodo1 opt]# chmod -R 777 nifi\n</code></pre> </li> <li> <p>Si el directorio en HDFS al que enviamos los archivos no existe, el proceso debe crearlo.</p> </li> <li> <p>NiFi debe tener permisos sobre HDFS.</p> </li> <li> <p>Los directorios en HDFS deben tener permisos para que NiFi pueda escribir en ellos.</p> </li> </ul> <p>Pasos a seguir:</p> <ol> <li> <p>Crear un directorio llamado <code>input</code> en el sistema de archivos de la m\u00e1quina virtual Linux, donde se ubicar\u00e1n los archivos que queremos trasladar al sistema HDFS.</p> </li> <li> <p>Implementar y configurar el procesador correspondiente para leer los archivos de este directorio del sistema de archivos de Linux.</p> </li> <li> <p>Este procesador recoger\u00e1 todos los archivos del sistema de archivos de la m\u00e1quina virtual en el directorio que hemos creado y los trasladar\u00e1 al sistema HDFS de Hadoop.</p> </li> <li> <p>Implementar y configurar el procesador correspondiente para realizar un <code>put</code> de los archivos al sistema HDFS de Hadoop en un directorio llamado <code>output_hdfs</code>. Si el directorio no existe, el proceso deber\u00e1 crearlo con los permisos del usuario que ejecuta (hadoop).</p> </li> <li> <p>Validar mediante el comando:</p> <pre><code>hdfs dfs -ls /\n</code></pre> <p>en el sistema de archivos HDFS que el directorio se ha creado con los permisos correctos y que los archivos han llegado correctamente. Tambi\u00e9n se puede validar utilizando la p\u00e1gina web de HDFS: http://nodo1:9870/explorer.html.</p> </li> <li> <p>A\u00f1adir un nuevo procesador a NiFi que recoja los archivos del sistema HDFS del directorio <code>output_hdfs</code> y los traslade a un directorio <code>output</code> en el sistema de archivos Linux.</p> </li> </ol> <p>ENTREGA:: Un PDF con capturas de pantalla de todo lo implementado y de los resultados en los directorios, tanto en HDFS como en el sistema local.</p> <p>Ayuda: NiFi Documentation (apache.org)</p>"},{"location":"odi/odiCasoDeUso/","title":"Case de uso COVID","text":""},{"location":"odi/odiCasoDeUso/#enunciado","title":"Enunciado","text":"<p>El Centro Nacional de Epidemiolog\u00eda ha habilitado una secci\u00f3n espec\u00edfica en su p\u00e1gina web para proporcionar informaci\u00f3n detallada sobre la evoluci\u00f3n de la epidemia de la Covid-19 desde 2020. Esta plataforma no solo presenta informes y conclusiones internas, sino tambi\u00e9n acceso a las fuentes originales de datos que los respaldan. La documentaci\u00f3n y los conjuntos de datos est\u00e1n disponibles en: CNE Covid.</p> <p>En aproximadamente 4 sesiones, trabajaremos en nuestro caso de uso utilizando esta informaci\u00f3n. El proceso incluir\u00e1 un proceso ETL con Oracle Data Integrator (ODI), seguido de la creaci\u00f3n de un modelo multidimensional para almacenar los datos de manera eficiente en una base de datos anal\u00edtica. Finalmente, realizaremos un an\u00e1lisis detallado con herramientas de generaci\u00f3n de informes para explorar y explotar los datos analizados.</p> <p>La visualizaci\u00f3n se realizar\u00e1 en etapas posteriores, con el objetivo de crear un dise\u00f1o similar al de PowerBI presentado en la misma p\u00e1gina web, para facilitar la comprensi\u00f3n e interpretaci\u00f3n de los datos sobre la Covid-19.</p>"},{"location":"odi/odiCasoDeUso/#requisitos","title":"Requisitos","text":"<p>Se deben crear las siguientes dimensiones:</p> <ul> <li> <p>Sexo Esta dimensi\u00f3n se crear\u00e1 a partir de los datos proporcionados en el archivo fuente hist\u00f3rico del Covid (cogiendo los diferentes tipos de valores para el c\u00f3digo Sexo). La variable \"Sexo\" representar\u00e1 el g\u00e9nero de las personas afectadas por la epidemia de la Covid-19, permitiendo segmentar los casos, hospitalizaciones, ingresos en UCI y defunciones seg\u00fan este atributo.</p> </li> <li> <p>Grupo de Edad La dimensi\u00f3n \"Grupo de Edad\" se crear\u00e1 utilizando los datos del archivo fuente hist\u00f3rico del Covid (cogiendo los diferentes tipos de valores para el c\u00f3digo Grupo Edad). Los grupos de edad se dividir\u00e1n en categor\u00edas relevantes, como pueden ser menores de 18 a\u00f1os, 18-40 a\u00f1os, 41-60 a\u00f1os, 61-80 a\u00f1os, y mayores de 80 a\u00f1os. Esto permitir\u00e1 analizar la distribuci\u00f3n de los casos y otros eventos en funci\u00f3n de la edad de los afectados.</p> </li> <li> <p>Provincia Esta dimensi\u00f3n se generar\u00e1 a partir del archivo de provincias, que contiene los c\u00f3digos y nombres de las provincias. La dimensi\u00f3n de Provincia permitir\u00e1 segmentar los datos seg\u00fan la ubicaci\u00f3n geogr\u00e1fica de los afectados, facilitando el an\u00e1lisis regional de la epidemia.</p> </li> <li> <p>Fecha La dimensi\u00f3n fecha se establecer\u00e1 mediante un procedimiento SQL, donde se representar\u00e1n los eventos en funci\u00f3n del tiempo. Esta dimensi\u00f3n se utilizar\u00e1 para organizar y analizar los datos epidemiol\u00f3gicos en funci\u00f3n de fechas espec\u00edficas, lo que facilitar\u00e1 el an\u00e1lisis a lo largo del tiempo.</p> </li> <li> <p>Estado de Alarma Adem\u00e1s de las dimensiones anteriores, es necesario generar un cat\u00e1logo para analizar la cantidad de personas que estuvieron afectadas o no por la Covid-19 durante los per\u00edodos de estado de alarma. Este cat\u00e1logo incluir\u00e1 los siguientes estados:</p> <ol> <li> <p>Declaraci\u00f3n del estado de alarma desde el 14 de marzo: Representa el inicio del estado de alarma en Espa\u00f1a debido a la pandemia.</p> </li> <li> <p>Pr\u00f3rroga del estado de alarma desde el 27 de marzo: Refleja la extensi\u00f3n del estado de alarma inicial.</p> </li> <li> <p>Pr\u00f3rroga del estado de alarma desde el 10 de abril: Otra extensi\u00f3n del estado de alarma.</p> </li> <li> <p>Pr\u00f3rroga del estado de alarma desde el 24 de abril: Continuaci\u00f3n del estado de alarma.</p> </li> <li> <p>Pr\u00f3rroga del estado de alarma desde el 8 de mayo: Nueva extensi\u00f3n del estado de alarma.</p> </li> <li> <p>Pr\u00f3rroga del estado de alarma desde el 22 de mayo: Continuaci\u00f3n del periodo de alarma.</p> </li> <li> <p>Pr\u00f3rroga del estado de alarma desde el 5 de junio: \u00daltima pr\u00f3rroga del estado de alarma antes de su fin.</p> </li> <li> <p>Fuera del estado de alarma: Indica los casos fuera de los per\u00edodos de estado de alarma.</p> </li> </ol> </li> <li> <p>El n\u00famero de casos, el n\u00famero de hospitalizaciones, el n\u00famero de ingresos en UCI y el n\u00famero de defunciones de manera diaria.</p> </li> <li>Determinar si las personas han sido afectadas en alguno de los per\u00edodos de estado de alarma mencionados anteriormente, lo que permitir\u00e1 hacer un an\u00e1lisis detallado sobre el impacto de las diferentes fases del estado de alarma en los casos de Covid-19.</li> </ul>"},{"location":"odi/odiCasoDeUso/#objetivo","title":"Objetivo","text":"<p>El objetivo final de este proceso es que respecto a la informaci\u00f3n recibida se pueda interpretar y analizar:</p>"},{"location":"odi/odiCasoDeUso/#recursos","title":"Recursos","text":""},{"location":"odi/odiCasoDeUso/#ficheros","title":"Ficheros","text":"<ul> <li> <p>Hist\u00f3rico Covid. El archivo contiene informaci\u00f3n detallada sobre la evoluci\u00f3n de la pandemia de Covid-19 hasta el 28 de marzo de 2022. El archivo incluye datos sobre el n\u00famero de casos confirmados, hospitalizaciones, ingresos en unidades de cuidados intensivos (UCI) y defunciones, segmentados por sexo, edad y provincia de residencia. Los datos abarcan todas las edades desde el inicio de la pandemia.</p> <ul> <li>Descarga hist\u00f3rico COVID-19</li> </ul> </li> <li> <p>Provincias: C\u00f3digos de las provincias en Espa\u00f1a. Al descargar los c\u00f3digos en formato CSV, se ha encontrado un problema de codificaci\u00f3n de caracteres que debe resolverse durante el proceso de ETL.</p> <ul> <li>Descargar ISO 3166</li> </ul> </li> <li> <p>Fechas: Consulta SQL para generar de forma din\u00e1mica el cat\u00e1logo de fechas. </p> <ul> <li>Descargar SQL</li> </ul> </li> <li> <p>Tramos estado alarma: Representa los diferentes tramos por los que se pasa en el estado de alarma para poder clasificarlos y agruparlos.</p> <ul> <li>Descargar tramos</li> </ul> </li> </ul>"},{"location":"odi/odiCasoDeUso/#diagramas","title":"Diagramas","text":""},{"location":"odi/odiCasoDeUso/#catalogos","title":"Catalogos","text":"<p>Se representa un ejemplo de como cargar el fichero Covid por las diferentes capas des de que es un fichero (fuente del dato)  hasta que llega a la tabla final, de d\u00f3nde se obtiene la tabla de catalogo de Sexo y Grupo Edad.</p> <p>La fecha es un PLSQL que se carga con un procedimiento de ODI directamente a la tabla final que es el catalogo de fecha.</p> <p>Las provincias se cargan a trav\u00e9s del fichero ISO, se realizan las transformaciones pertinentes y luego se carga a la tabla final de Provincia. </p>"},{"location":"odi/odiCasoDeUso/#multidimensional","title":"Multidimensional","text":"<p>Se representan la estrella como eje central y luego alrededor los diferentes cat\u00e1logos que representan y detallan la informaci\u00f3n de an\u00e1lisis. </p>"},{"location":"odi/odiCasoDeUso/#modelos","title":"Modelos","text":"<p>Un ejemplo de como se quedar\u00e1 el modelo de datos l\u00f3gico de ODI en vuestro proyecto. D\u00f3nde vemos el modeo FILE_COVID es d\u00f3nde se encuentran los ficheros definidos del COVID y el resto son las tablas Oracle. Se clasifican los modelos por tecnolog\u00eda. </p>"},{"location":"odi/odiCasoDeUso/#obtener-y-generar-modelo-multidimensional","title":"Obtener y generar modelo multidimensional","text":"<p>Se prescinde de la creaci\u00f3n de un documento formal de An\u00e1lisis Funcional y Dise\u00f1o. Sin embargo, se considera esencial establecer una visi\u00f3n clara sobre la construcci\u00f3n del modelo de datos. Este modelo se basar\u00e1 en las necesidades espec\u00edficas del informe final y en los conocimientos adquiridos a lo largo del curso. A continuaci\u00f3n, se detallan los requisitos esenciales para esta etapa:</p>"},{"location":"odi/odiCasoDeUso/#1-modelo-de-datos","title":"1. Modelo de datos","text":"<p>Es necesario identificar las entidades clave relacionadas con la epidemia de la Covid-19. Estas entidades podr\u00edan incluir casos, hospitalizaciones, ingresos en UCI, defunciones, sexo, edad, provincia, entre otras.</p> <p>Una vez identificadas las entidades, se debe definir c\u00f3mo se relacionan entre s\u00ed para asegurar que la base de datos tenga una estructura l\u00f3gica y eficiente.</p>"},{"location":"odi/odiCasoDeUso/#2-variables-y-atributos","title":"2. Variables y atributos","text":"<p>Es importante definir claramente las variables y los atributos necesarios para capturar toda la informaci\u00f3n relevante relacionada con la epidemia. Esto podr\u00eda incluir variables como el n\u00famero de casos, el n\u00famero de hospitalizaciones, la distribuci\u00f3n por edades y sexos, etc.</p> <p>Cada atributo debe estar bien etiquetado (claridad en la etiquetaci\u00f3n de atributos) y debe ser representativo de la informaci\u00f3n. Esto garantiza que los datos sean comprensibles y \u00fatiles para su posterior an\u00e1lisis.</p>"},{"location":"odi/odiCasoDeUso/#3-jerarquias-temporales","title":"3. Jerarqu\u00edas temporales","text":"<p>Es necesario crear jerarqu\u00edas temporales para representar correctamente la secuencia de los eventos relacionados con la epidemia. Estas jerarqu\u00edas pueden incluir, por ejemplo, a\u00f1os, meses, semanas y d\u00edas, permitiendo as\u00ed un an\u00e1lisis detallado de la evoluci\u00f3n de la pandemia a lo largo del tiempo.</p>"},{"location":"odi/odiCasoDeUso/#4-normativas-y-restricciones","title":"4. Normativas y restricciones","text":"<p>Se deben tener en cuenta todas las normativas o restricciones espec\u00edficas asociadas con la recopilaci\u00f3n y el uso de las datos epidemiol\u00f3gicos. Esto puede incluir regulaciones sobre la protecci\u00f3n de datos personales, el acceso a la informaci\u00f3n, y las pol\u00edticas de confidencialidad y privacidad.</p>"},{"location":"odi/odiCasoDeUso/#5-metodologia-etl","title":"5. Metodolog\u00eda ETL","text":"<p>Es fundamental planificar y describir detalladamente la metodolog\u00eda ETL que se seguir\u00e1 para integrar los datos desde su origen hasta la base de datos anal\u00edtica. Este proceso garantizar\u00e1 que los datos se extraigan correctamente de las fuentes originales, se transformen seg\u00fan las necesidades del an\u00e1lisis y se carguen adecuadamente en la base de datos final.</p>"},{"location":"odi/odiCasoDeUso/#proceso-etl","title":"Proceso ETL","text":"<p>Se describe a continuaci\u00f3n las diferentes capas del modelo multidimensional en el cual se van a cargar los ficheros de datos que recibimos de la fuente de datos, las transformamos, las modelamos en un modelo anal\u00edtico y preparamos estos datos para que puedan ser analizados y consultados por herramientas de reporting.</p>"},{"location":"odi/odiCasoDeUso/#capa-source-src","title":"Capa SOURCE: (SRC)","text":"<ul> <li>Cargar fichero COVID</li> <li>Cargar ficheros propios para generar los cat\u00e1logos</li> <li>Generar proceso / paquete de carga SRC</li> </ul> <p>Pasos en ODI</p> <ol> <li> <p>Hacer ingenieria inversa de los ficheros para obtener su estructura.</p> <ul> <li>Covid</li> <li>Estado alarma</li> <li>Provincia</li> </ul> </li> <li> <p>Hacer ingenieria inversa de las tablas SRC creadas.</p> </li> <li> <p>Desarrollar las ETL para cargar los ficheros a las tabla creadas previamente SRC.</p> </li> <li> <p>Generar paquete con la carga de todas las ETL y procedimiento en SRC. WF_SRC_XXX</p> </li> </ol> <p>Tablas SRC a crear</p> <ul> <li>SRC_COVID</li> <li>SRC_PROVINCIA</li> <li>SRC_ESTAT_ALARMA</li> </ul> <p>Note</p> <p>Todos los IKM deben configurarse como IKM Control Append, con la opci\u00f3n TRUNCATE = TRUE y FLOW_CONTROL = FALSE. Para que en cada ejecuci\u00f3n la tabla final se trunque y no valide las restricciones. </p>"},{"location":"odi/odiCasoDeUso/#capa-catalogos-lkp","title":"Capa CAT\u00c1LOGOS: (LKP)","text":"<ul> <li>Extraer de la tabla SRC COVID los cat\u00e1logos necesarios.</li> <li>Generar procedimiento ODI para el cat\u00e1logo de fecha</li> <li>Cargar del resto de tablas SRC los cat\u00e1logos LKP.</li> <li>Generar proceso / paquete de carga LKP.</li> </ul> <p>Pasos a ODI</p> <ol> <li> <p>Crear tablas de los cat\u00e1logos (LKP). Realizar ingenier\u00eda inversa de estas tablas.</p> </li> <li> <p>Desarrollar las ETLs para cargar de las tablas SRC a las tablas de cat\u00e1logo LKP.</p> </li> <li> <p>Generar paquete con la carga de todas las ETL para cat\u00e1logos LKP (LKP). WF_LKP_XXX</p> </li> </ol> <p>Tablas LKP a crear</p> <ul> <li>LKP_SEXO</li> <li>LKP_GRUPO_EDAD</li> <li>LKP_PROVINCIA</li> <li>LKP_ESTAT_ALARMA</li> <li>LKP_FECHA</li> </ul> <p>Filtros LKP Provincias</p> <ul> <li> <p>SRC_PROVINCIA.CAT_SUB = 'province'</p> </li> <li> <p>SUBSTR(SRC_PROVINCIA.CODIGO,4,LENGTH(SRC_PROVINCIA.CODIGO))</p> </li> </ul> <p>Note</p> <p>Todos los IKM deben configurarse como IKM Control Append, con la opci\u00f3n TRUNCATE = TRUE y FLOW_CONTROL = FALSE. Para que en cada ejecuci\u00f3n la tabla final se trunque y no valide las restricciones. </p>"},{"location":"odi/odiCasoDeUso/#capa-estrella-ods","title":"Capa ESTRELLA: (ODS)","text":"<ul> <li>Extraer de la tabla SRC COVID los datos hist\u00f3ricos/transaccionales.</li> <li>Extraer solo los c\u00f3digos, no las descripciones.</li> <li>Generar lookups en el mapeo de ODS.</li> <li>Generar proceso/paquete de carga ODS.</li> </ul> <p>Pasos a ODI</p> <ol> <li> <p>Crear tabla hist\u00f3rica/transaccional (ODS). Realizar ingenier\u00eda inversa de esta tabla.</p> </li> <li> <p>Desarrollar la ETL para cargar de las tablas SRC a ODS.</p> </li> <li> <p>Utilizar los lookups para validar y cargar la informaci\u00f3n de los cat\u00e1logos.</p> </li> <li> <p>Generar paquete con la carga del (ODS). WF_ODS_XXX</p> </li> </ol> <p>Tabla ODS a crear</p> <ul> <li>ODS_COVID</li> </ul>"},{"location":"odi/odiCasoDeUso/#final","title":"Final","text":"<p>Pasos a ODI</p> <ol> <li> <p>Generar paquete que cargue de una vez todas las capas: SRC, LKP y ODS. WF_TOTAL_XXX</p> </li> <li> <p>Validamos que no haya errores.</p> </li> <li> <p>Validamos que las tablas dades de taules:</p> </li> <li> <p>Que tinguin dades i siguin correctes</p> </li> <li> <p>Que apleguen totes les dades que hi ha als fitxers: conteig al fitxer, src i ods.</p> </li> <li> <p>Que estan amb el format que es vol representar</p> </li> </ol> <p>Note</p> <p>Todos los IKM deben configurarse como IKM Control Append, con la opci\u00f3n TRUNCATE = TRUE y FLOW_CONTROL = FALSE. Para que en cada ejecuci\u00f3n la tabla final se trunque y no valide las restricciones. </p>"},{"location":"odi/odiCasoDeUso/#visualizacion-y-respresentacion-con-power-bi","title":"Visualizaci\u00f3n y respresentaci\u00f3n con Power BI","text":"<ol> <li> <p>Conexi\u00f3n directa con el modelo multidimensional Power BI permite conectarse directamente a bases de datos como Oracle, SQL Server, entre otras. Si la conexi\u00f3n con la m\u00e1quina virtual es exitosa, se podr\u00e1 importar directamente el modelo multidimensional desde la base de datos, lo que facilitar\u00e1 la carga de datos en Power BI. En este caso, los datos estar\u00e1n organizados en tablas de hechos y dimensiones que pueden ser f\u00e1cilmente visualizadas y analizadas.</p> </li> <li> <p>En caso de no conseguir la conectividad Si existen problemas t\u00e9cnicos para conectar Power BI con la base de datos, la alternativa ser\u00e1 exportar el modelo multidimensional a archivos de texto (por ejemplo, archivos CSV o Excel). Estos archivos deben contener toda la informaci\u00f3n relevante, organizados por las diferentes dimensiones y hechos definidos en el modelo.</p> </li> </ol> <p>Luego, se importar\u00e1n estos archivos en Power BI, donde se pueden definir relaciones entre las diferentes tablas de datos (por ejemplo, entre las tablas de hechos y dimensiones) para crear el modelo de datos en Power BI. Posteriormente, se podr\u00e1n crear visualizaciones y an\u00e1lisis basados en esos datos.</p> <ol> <li>Generaci\u00f3n del informe final Con el modelo multidimensional le\u00eddo y cargado en Power BI (ya sea de manera directa o importando los archivos), se proceder\u00e1 a crear las visualizaciones que respondan a las necesidades del informe final. Esto incluir\u00e1 gr\u00e1ficos, tablas y otras representaciones visuales que permitan explorar las diferentes m\u00e9tricas y dimensiones de los datos (por ejemplo, n\u00famero de casos, hospitalizaciones, ingresos en UCI, defunciones, etc.).</li> </ol> <p>Tambi\u00e9n se podr\u00e1n implementar filtros y segmentaciones basadas en dimensiones como la edad, el sexo, la provincia, etc., para que los usuarios del informe puedan analizar los datos de manera m\u00e1s flexible y detallada.</p>"},{"location":"odi/odiCasoDeUso/#resumen","title":"Resumen","text":"<p>Una vez descargada la m\u00e1quina virtual con todo el entorno preparado, se deben aplicar los conocimientos adquiridos durante el curso y desarrollar el an\u00e1lisis y dise\u00f1o realizado en los pasos anteriores. El objetivo es crear un modelo de base de datos multidimensional por capas utilizando la base de datos Oracle en la m\u00e1quina virtual proporcionada. El proceso se desglosa en los siguientes pasos:</p> <ol> <li> <p>Crear tablas que carguen los archivos al BI (SRC_XXX)</p> <ul> <li>Se deben crear las tablas de origen que cargar\u00e1n los archivos de datos en el sistema de Business Intelligence (BI). Estas tablas se denominan SRC_XXX. Su funci\u00f3n es almacenar los datos procedentes de diversas fuentes antes de que sean procesados y transformados. Asegurar la correcta carga y organizaci\u00f3n de los datos es esencial para que el sistema de BI pueda realizar los an\u00e1lisis de manera eficiente y precisa.</li> </ul> </li> <li> <p>Crear tablas de cat\u00e1logos que carguen los archivos de la capa SRC_XXX a la capa LKP_XXX</p> <ul> <li> <p>A continuaci\u00f3n, se deben crear las tablas de cat\u00e1logos que gestionan el proceso de transformaci\u00f3n de los datos. Estas tablas cargan los datos de la capa SRC_XXX a la capa LKP_XXX (capa de b\u00fasqueda o lookup). En esta capa, los datos provenientes de los archivos se transforman para ser normalizados o enriquecidos con valores adicionales, como claves de referencia, descripciones o c\u00f3digos. Esto garantiza la consistencia y la integridad de los datos a lo largo de las diferentes etapas del proceso.</p> </li> <li> <p>Se debe incluir un diagrama multidimensional de la extracci\u00f3n como anexo, que representar\u00e1 visualmente c\u00f3mo se mueven y transforman los datos entre las diferentes capas y c\u00f3mo se estructuran las tablas dentro de cada capa.</p> </li> </ul> </li> <li> <p>Crear tabla referente a la estrella que carga de las tablas SRC_XXX a la capa ODS_XXX</p> <ul> <li> <p>El siguiente paso implica la creaci\u00f3n de una tabla estrella que cargue los datos desde las tablas SRC_XXX a la capa ODS_XXX (almac\u00e9n de datos operacionales). En esta capa, los datos se consolidan y se almacenan para facilitar su an\u00e1lisis. En el modelo estrella, las tablas de hechos y dimensiones est\u00e1n organizadas de manera que facilitan las consultas r\u00e1pidas y eficientes. La tabla estrella es crucial porque organiza la informaci\u00f3n de manera que permite un an\u00e1lisis multidimensional, ofreciendo una visi\u00f3n completa de los datos seg\u00fan varias dimensiones.</p> </li> <li> <p>En este paso, se debe incluir un diagrama multidimensional de la estrella como anexo, que mostrar\u00e1 c\u00f3mo las tablas de hechos y dimensiones se relacionan dentro del modelo estrella.</p> </li> </ul> </li> <li> <p>Crear la ingesta de datos con ETL usando ODI seg\u00fan la soluci\u00f3n analizada</p> <ul> <li> <p>Se debe crear el proceso de ingesta de datos usando ETL. Utilizando Oracle Data Integrator (ODI), extraemos los datos de las fuentes originales y los almacenamos en tablas con las transformaciones pertinentes SRC_XXX, luego se obtienen tambi\u00e9n los cat\u00e1logos LKP_XXX y definitivamente generamos la tabla estrella que contiene todo el hist\u00f3rico ODS_XXX. </p> </li> <li> <p>El proceso ETL es crucial para garantizar que los datos se carguen correctamente en el sistema de datos multidimensional. ODI facilitar\u00e1 la automatizaci\u00f3n de este proceso, permitiendo transformar los datos de manera eficiente y almacenarlos de forma estructurada.</p> </li> </ul> </li> <li> <p>WF_XXX</p> <ul> <li>Finalmente, se hace referencia a la creaci\u00f3n de WF_XXX asociado al proceso de integraci\u00f3n de datos. Este flujo de trabajo es un conjunto de tareas o pasos definidos que se ejecutan en un orden espec\u00edfico para asegurar que el proceso ETL se complete correctamente. El workflow puede incluir tareas como validaci\u00f3n de datos, ejecuci\u00f3n de transformaciones y carga final de los datos. Con lo que a\u00f1adiremos de forma secuencial todos los objetos creados. </li> </ul> </li> </ol>"},{"location":"odi/odiDemo/","title":"ODI DEMO","text":"<p>Tenemos que realizar los siguientes puntos del documento pdf Started Guide que esta en el escritorio de la m\u00e1quina virtual descargada. ODI_12-KnowledgeModuleDeveloperGuide.pdf</p>"},{"location":"odi/odiDemo/#puntos-a-seguir","title":"Puntos a seguir","text":"<pre><code>            2.      Working with the ETL Project\n\n                    2.1 The Example Environment\n\n                    2.2 The Data Models\n\n                            2.2.1 Orders Application\n\n                            2.2.2 Parameters\n\n                            2.2.3 Sales Administration \u2013 Oracle\n\n                    2.3 Integration Challenges\n\n\n            3.      Introduction to Using Oracle Data Integrator\n\n                    3.1 Using the ODI Studio Navigators\n\n                            3.1.1 Starting Oracle Data Integrator Studio\n\n                    3.2 Designer Navigator\n\n                    3.3 Operator Navigator\n\n\n            4.      Working with Mappings\n\n                    4.1 Load TRG_CUSTOMER Mapping Example\n\n                            4.1.1 Purpose and Integration Requirements\n\n                            4.1.2 Mapping Definition\n\n                            4.1.3 Creating the Mapping\n\n                    4.2 Load TRG_SALES Mapping Example\n\n                            4.2.1 Purpose and Integration Requirements\n\n                            4.2.2 Mapping Definition\n\n                            4.2.3 Creating the Mapping\n\n            6.      Working with Packages\n\n                    6.1 Introduction\n\n                            6.1.1 Automating Data Integration Flows\n\n                            6.1.2 Packages\n\n                    6.2 Load Sales Administration Package Example\n\n                            6.2.1 Purpose\n\n                            6.2.2 Mappings Provided with Oracle Data Integrator\n\n                            6.2.3 Problem Analysis\n\n                            6.2.4 Creating the Package\n\n            7.      Executing Your Developments and Reviewing the Results\n\n                    7.1 Executing the Load Sales Administration Package\n\n                            7.1.1 Run the Package\n\n                            7.1.2 Follow the Execution of the Package in Operator Navigator\n\n                            7.1.3 Interpreting the Results of the Load TRG_CUSTOMER Session Step\n\n            8.      Deploying Integrated Applications\n\n                    8.1 Introduction\n\n                    8.2 Scenario Creation\n\n                    8.3 Run the Scenario\n\n                            8.3.1 Executing a Scenario from ODI Studio\n\n                    8.4 Follow the Execution of the Scenario\n</code></pre>"},{"location":"odi/odiIntroduccion/","title":"ODI","text":""},{"location":"odi/odiIntroduccion/#que-es-oracle-data-integrator-odi","title":"\u00bfQu\u00e9 es Oracle Data Integrator (ODI)?","text":"<p>Es una plataforma de integraci\u00f3n de datos que facilita la transferencia, transformaci\u00f3n y carga de datos (ETL: Extract, Transform, Load) entre diferentes fuentes de datos. Est\u00e1 dise\u00f1ada para manejar grandes vol\u00famenes de datos y facilitar la integraci\u00f3n en entornos de m\u00faltiples bases de datos, aplicaciones y sistemas.</p>"},{"location":"odi/odiIntroduccion/#caracteristicas-principales","title":"Caracter\u00edsticas principales","text":"<ul> <li> <p> Integraci\u00f3n de datos en tiempo real y batch:    ODI permite realizar integraciones tanto en tiempo real como por lotes (batch). Esta flexibilidad es clave en escenarios donde los datos deben procesarse en tiempo real para an\u00e1lisis inmediatos o cuando los procesos pueden esperar un procesamiento m\u00e1s programado.</p> </li> <li> <p> Transformaci\u00f3n de datos:    ODI ofrece un motor de transformaci\u00f3n de datos de alta performance, que permite realizar transformaciones complejas de los datos durante el proceso de integraci\u00f3n. Esto incluye operaciones como agregaciones, uniones, filtrado y m\u00e1s.</p> </li> <li> <p> Conectividad con m\u00faltiples fuentes y destinos:    ODI soporta una amplia variedad de sistemas de bases de datos, aplicaciones y plataformas. Esto incluye Oracle Database, SQL Server, MySQL, y muchos otros sistemas tanto on-premise como en la nube. Tambi\u00e9n puede integrarse con plataformas de Big Data como Hadoop, y soluciones en la nube como Oracle Cloud.</p> </li> <li> <p> Dise\u00f1o gr\u00e1fico y desarrollo f\u00e1cil:    El desarrollo de procesos de integraci\u00f3n en ODI es muy visual, con una interfaz gr\u00e1fica de usuario (GUI) que permite dise\u00f1ar flujos de datos sin necesidad de escribir c\u00f3digo. Se utilizan objetos visuales como mappings, procedimientos y otros componentes que facilitan la construcci\u00f3n de los procesos de integraci\u00f3n.</p> </li> <li> <p> Data Quality y validaci\u00f3n:    ODI incluye capacidades de validaci\u00f3n de datos, asegurando que los datos que se est\u00e1n integrando sean precisos, completos y est\u00e9n en el formato adecuado. Esta es una caracter\u00edstica importante en procesos de integraci\u00f3n de datos en entornos empresariales donde la calidad de los datos es cr\u00edtica.</p> </li> <li> <p> Automatizaci\u00f3n y gesti\u00f3n de procesos:    ODI permite la automatizaci\u00f3n de procesos de integraci\u00f3n de datos mediante la planificaci\u00f3n y ejecuci\u00f3n de trabajos (jobs). Tambi\u00e9n ofrece funcionalidades de monitoreo y auditor\u00eda para garantizar que los trabajos de integraci\u00f3n se ejecuten de manera eficiente y sin errores.</p> </li> <li> <p> Integraci\u00f3n con Oracle Ecosystem:    ODI se integra de manera nativa con el ecosistema de Oracle, lo que significa que se puede utilizar de forma eficiente con otras soluciones de Oracle como Oracle Exadata, Oracle Warehouse Builder y Oracle Business Intelligence.</p> </li> </ul>"},{"location":"odi/odiIntroduccion/#arquitectura","title":"Arquitectura","text":"<p>Arquitectura de repositorios</p> <p>ODI almacena todo su escosistema en metadatos en la base de datos. Se basa en repositorios llamados \u201cMaster repository\u201d y \u201cWork repository\u201d. </p> <p>Existe un solo master repository compartido y como regla general un work repository por departamentos.</p> <p>Los repositorios Work Repository existen de dos tipos: Development y Execution. En entornos productivos, ser\u00e1 siempre del tipo \"Execution\" dado que s\u00f3lo se despliega c\u00f3digo compilado.</p> <p>El Master Repository comparte con todos los Work Repository la : seguridad, conexiones (topolog\u00eda), versionado, etc\u2026</p> <p>Cada Work Repository tiene su c\u00f3digo y no podr\u00e1 ver el c\u00f3digo de otro work pero si su seguridad y conexiones.</p> <p>Componentes del sistema</p> <ul> <li> <p>Consola ODI: componente web que permite realizar el seguimiento de ejecuci\u00f3n y parametrizaci\u00f3n de determinados objetos.</p> </li> <li> <p>Agente de ODI: componente que s\u2019encarga de ejecutar los procesos de carga i transformaci\u00f3n de datos. Los agentes est\u00e1n ubicados en el weblogic.</p> </li> <li> <p>ODI Studio: componente que proporciona la herramienta para el desarrollo y gestion. </p> </li> </ul> <p>Tipos de despliegues</p> <ul> <li> <p>Smart Export/Import: despliegue que exporta todo objeto y lo m\u00e1s importante, sus dependencias. Cuando se realiza este tipo de despliegue, hay que tener especial cuidado con que objetos se lleva porque puedes machacar configuraci\u00f3n de entornos Productivos como el agente, conexiones, etc..</p> </li> <li> <p>Export/Import: exporta objetos de manera individual, con lo que te da el control total de que exportas e importas.</p> </li> <li> <p>Export Topology, Secutiry, Work Repository, etc: hay muchos tipos m\u00e1s de exports/imports, pero en el d\u00eda a d\u00eda no se usar\u00e1n. Son m\u00e1s para momentos puntuales a nivel administrativo / arquitectura. SIEMPRE se ha de desplegar c\u00f3digo compilado, es decir \u201cScenarios\u201d, \u201cLoad Plans\u201d, Objetos de \u201cTopology\u201d e instrucciones para realizar programaciones en entornos Productivos.</p> </li> </ul>"},{"location":"odi/odiIntroduccion/#usos-comunes","title":"Usos comunes","text":"<ul> <li>ETL en proyectos de Data Warehousing: ODI se utiliza ampliamente en la integraci\u00f3n de grandes vol\u00famenes de datos desde diversas fuentes hacia un Data Warehouse.</li> <li>Migraci\u00f3n de datos: Es ideal para mover datos entre plataformas diferentes, ya sea para la actualizaci\u00f3n de sistemas o la migraci\u00f3n a la nube.</li> <li>Integraci\u00f3n de Big Data: Puede integrarse con tecnolog\u00edas como Hadoop para procesar grandes vol\u00famenes de datos no estructurados o semi-estructurados.</li> <li>Integraci\u00f3n de aplicaciones: Permite la integraci\u00f3n de diversas aplicaciones empresariales a trav\u00e9s de la consolidaci\u00f3n de datos.</li> </ul> <p>Oracle Data Integrator es una herramienta potente para la integraci\u00f3n de datos en empresas, dise\u00f1ada para realizar ETL de manera eficiente, manejar grandes vol\u00famenes de datos, y ofrecer flexibilidad tanto en procesamiento batch como en tiempo real.</p>"},{"location":"odi/odiIntroduccion/#descarga-maquina-virtual-oracle","title":"Descarga m\u00e1quina virtual Oracle","text":"<p>Descargar la m\u00e1quina virtual que contiene todo el entorno para hacer el desarrollo. </p> <p>Pre-built Virtual Machine for Oracle Data Integrator 12c Getting Started Guide.</p> <p>Contiene:</p> <ul> <li> <p>Oracle Enterprise Linux (64-bit) 2.6.32-300.39.5</p> </li> <li> <p>Oracle EE Database 11.2.0.4</p> </li> <li> <p>Oracle Data Integrator 12.2.1.3.1</p> </li> <li> <p>Oracle GoldenGate 12.1.2.0.0</p> </li> <li> <p>Java Platform (JDK) 1.8.0_60</p> </li> </ul>"},{"location":"odi/odiIntroduccion/#overview","title":"Overview","text":""},{"location":"odi/odiIntroduccion/#entorno","title":"Entorno","text":"<ul> <li>ODI Studio 12c</li> <li>Mappings</li> <li>Topologia logica y f\u00edsica</li> <li>Contexto</li> <li>Operador (agents)</li> <li>Modelos</li> <li>Planes de carga</li> <li>Paquetes</li> <li>Escenarios</li> <li>Interpretaci\u00f3n de logs</li> <li>Load plans</li> <li>Modulos de conocimiento:<ul> <li>What is a Knowledge Module?<ul> <li>Reverse-Engineering Knowledge Modules (RKM)</li> <li>Check Knowledge Modules (CKM)</li> <li>Loading Knowledge Modules (LKM)</li> <li>Integration Knowledge Modules (IKM)</li> <li>Journalizing Knowledge Modules (JKM)</li> <li>Service Knowledge Modules (SKM)</li> </ul> </li> </ul> </li> </ul>"}]}